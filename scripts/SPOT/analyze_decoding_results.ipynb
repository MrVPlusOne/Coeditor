{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# first, load the trained model\n",
    "import os\n",
    "import torch\n",
    "from typing import *\n",
    "\n",
    "from spot.model import ModelWrapper\n",
    "from spot.utils import get_model_dir, proj_root\n",
    "\n",
    "os.chdir(proj_root())\n",
    "\n",
    "# gpu_id = 1\n",
    "# modeldir = get_model_dir()\n",
    "\n",
    "# model_name=\"model-v6--TrainingConfig(drop_env_types=False)\"\n",
    "model_name=\"model-v7--TrainingConfig(drop_env_types=False, add_implicit_rel_imports=True)\"\n",
    "# dataset_name = \"ManyTypes4Py\"\n",
    "dataset_name = \"InferTypes4Py\"\n",
    "# dataset_name = \"SPOT-src\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading test projects: 100%|██████████| 3/3 [00:11<00:00,  3.86s/it]\n"
     ]
    }
   ],
   "source": [
    "from spot.utils import *\n",
    "from spot.function_dataset import data_project_from_dir\n",
    "\n",
    "# load test projects\n",
    "repos_dir = get_dataset_dir(dataset_name) / \"repos\" / \"test\"\n",
    "test_repo_paths = [f for f in repos_dir.iterdir() if f.is_dir()]\n",
    "test_projects = pmap(\n",
    "    data_project_from_dir,\n",
    "    test_repo_paths,\n",
    "    desc=\"Loading test projects\",\n",
    ")\n",
    "assert len(test_projects) > 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading evaluation results: 100%|██████████| 6/6 [00:01<00:00,  4.00it/s]\n"
     ]
    }
   ],
   "source": [
    "from spot.function_decoding import DecodingOrders, EvalResult\n",
    "import re\n",
    "\n",
    "results_dir = get_eval_dir(dataset_name, \"(implicit_imports) \" + model_name)\n",
    "\n",
    "decode_orders = [m.group(1) for f in results_dir.iterdir() if (m:=re.match(r\"(.+)-EvalResult\\.pkl\", f.name)) is not None]\n",
    "decode_orders.sort()\n",
    "\n",
    "evals = dict[str, EvalResult]()\n",
    "\n",
    "for oname in tqdm(decode_orders, desc=\"Loading evaluation results\"):\n",
    "    evals[oname] = pickle_load(results_dir / f\"{oname}-EvalResult.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌──────────────────┬───────────┬────────┬──────────┐\n",
      "│            order │ plain_acc │    acc │ base_acc │\n",
      "├──────────────────┼───────────┼────────┼──────────┤\n",
      "│    callee2caller │    0.7118 │ 0.7164 │   0.7891 │\n",
      "│    caller2callee │    0.6754 │ 0.6870 │   0.7630 │\n",
      "│ double-traversal │    0.7122 │ 0.7168 │   0.7869 │\n",
      "│     no-neighbors │    0.6529 │ 0.6576 │   0.7274 │\n",
      "│         non-incr │    0.6838 │ 0.6907 │   0.7682 │\n",
      "│           random │    0.6896 │ 0.6980 │   0.7663 │\n",
      "└──────────────────┴───────────┴────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "import prettytable as pt\n",
    "from spot.type_env import AccuracyMetric\n",
    "\n",
    "common_type_names = ModelWrapper.load_common_type_names(get_model_dir() / model_name)\n",
    "metrics = AccuracyMetric.default_metrics(common_type_names)\n",
    "results_table = PrettyTable()\n",
    "results_table.field_names = [\"order\", *(m.name for m in metrics)]\n",
    "results_table.align = \"r\"\n",
    "results_table.set_style(pt.SINGLE_BORDER)\n",
    "results_table.float_format = \".4\"\n",
    "\n",
    "for oname in decode_orders:\n",
    "    accs = [evals[oname].error_analysis(None, metric).accuracies[metric.name].acc for metric in metrics]\n",
    "    results_table.add_row([oname, *accs])\n",
    "\n",
    "print(results_table)\n",
    "# write_file(results_dir / \"comparison.txt\", results_table.get_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project</th>\n",
       "      <th>non-incr</th>\n",
       "      <th>random</th>\n",
       "      <th>double-traversal</th>\n",
       "      <th>label_size</th>\n",
       "      <th>label_rate</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SPOT</td>\n",
       "      <td>0.609185</td>\n",
       "      <td>0.622184</td>\n",
       "      <td>0.636915</td>\n",
       "      <td>1.540728</td>\n",
       "      <td>0.682436</td>\n",
       "      <td>1154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>typilus</td>\n",
       "      <td>0.757455</td>\n",
       "      <td>0.760437</td>\n",
       "      <td>0.817097</td>\n",
       "      <td>1.883698</td>\n",
       "      <td>0.478820</td>\n",
       "      <td>1006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>type4py</td>\n",
       "      <td>0.738434</td>\n",
       "      <td>0.741993</td>\n",
       "      <td>0.701068</td>\n",
       "      <td>1.224199</td>\n",
       "      <td>0.313441</td>\n",
       "      <td>562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   project  non-incr    random  double-traversal  label_size  label_rate  \\\n",
       "0     SPOT  0.609185  0.622184          0.636915    1.540728    0.682436   \n",
       "1  typilus  0.757455  0.760437          0.817097    1.883698    0.478820   \n",
       "2  type4py  0.738434  0.741993          0.701068    1.224199    0.313441   \n",
       "\n",
       "   labels  \n",
       "0    1154  \n",
       "1    1006  \n",
       "2     562  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "projects = [p.root_dir.name for p in test_projects]\n",
    "acc_metric = AccuracyMetric(common_type_names)\n",
    "\n",
    "strategies_to_show = [\"non-incr\", \"random\", \"double-traversal\"]\n",
    "strategy2accs = {\n",
    "    s: [\n",
    "        evals[s].error_analysis(pname, acc_metric).accuracies[acc_metric.name]\n",
    "        for pname in projects\n",
    "    ]\n",
    "    for s in strategies_to_show\n",
    "}\n",
    "\n",
    "n_annots = [\n",
    "    sum(e.get_signature().n_annots() for e in p.all_elems()) for p in test_projects\n",
    "]\n",
    "# n_labels = [\n",
    "#     sum(e.get_signature().n_annotated() for e in p.all_elems()) for p in test_projects\n",
    "# ]\n",
    "n_labels = [x.n_total for x in strategy2accs[strategies_to_show[0]]]\n",
    "label_sizes = [\n",
    "    evals[strategies_to_show[0]]\n",
    "    .error_analysis(pname, acc_metric)\n",
    "    .accuracies[f\"{acc_metric.name}_label_size\"]\n",
    "    for pname in projects\n",
    "]\n",
    "\n",
    "label_rates = [a / b for a, b in zip(n_labels, n_annots)]\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"project\": projects,\n",
    "        **{n: [x.acc for x in xs] for n, xs in strategy2accs.items()},\n",
    "        \"label_size\": label_sizes,\n",
    "        \"label_rate\": label_rates,\n",
    "        \"labels\": n_labels,\n",
    "    }\n",
    ")\n",
    "df_sorted = df.sort_values(by=[\"labels\"], ascending=False)\n",
    "display(df_sorted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted.to_csv(proj_root() / \"data\" / \"accs_by_project.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project location: /mnt/data0/jiayi/SPOT/datasets/InferTypes4Py/repos/test/type4py\n",
      "Number of errors: 168\n"
     ]
    }
   ],
   "source": [
    "ex_proj = test_projects[2]\n",
    "print(\"Project location:\", ex_proj.root_dir)\n",
    "\n",
    "evalr = evals[\"double-traversal\"]\n",
    "pid = evalr.project_roots.index(ex_proj.root_dir)\n",
    "ex_analysis = evalr.error_analysis(pid, acc_metric)\n",
    "ex_errors = ex_analysis.errors[ex_proj.root_dir.name]\n",
    "ex_rollout = evalr.predictions[pid]\n",
    "\n",
    "print(\"Number of errors:\", len(ex_errors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under-predicted types:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(ty'pd.DataFrame', 18),\n",
       " (ty'np.array', 14),\n",
       " (ty'Dict', 10),\n",
       " (ty'List', 7),\n",
       " (ty'Set', 7),\n",
       " (ty'cst.Name', 6),\n",
       " (ty'str', 6),\n",
       " (ty'cst.ClassDef', 5),\n",
       " (ty'cst.FunctionDef', 5),\n",
       " (ty'List[List]', 4)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over-predicted types:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(ty'Dict', 20),\n",
       " (ty'str', 15),\n",
       " (ty'List[str]', 15),\n",
       " (ty'cst.CSTNode', 14),\n",
       " (ty'Union[cst.Module, str]', 11),\n",
       " (ty'Path', 8),\n",
       " (ty'np.ndarray', 7),\n",
       " (ty'Dict[str, str]', 6),\n",
       " (ty'List', 6),\n",
       " (ty'Union[cst.Expr, str]', 3)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# under predicted types sorted by frequency\n",
    "under_pred_types = Counter()\n",
    "over_predicted_types = Counter()\n",
    "for pred in ex_errors:\n",
    "    under_pred_types[acc_metric.process_type(pred.expected)] += 1\n",
    "    over_predicted_types[acc_metric.process_type(pred.predicted)] += 1\n",
    "\n",
    "print(\"Under-predicted types:\")\n",
    "display(under_pred_types.most_common(10))\n",
    "print(\"Over-predicted types:\")\n",
    "display(over_predicted_types.most_common(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n",
      "Elem Path: libsa4py.cst_transformers/TypeApplier.__get_cls_vars\n",
      "Type Index: 1\n",
      "Expected:  MethodSig((var_name: str) -> dict)\n",
      "Predicted: MethodSig((var_name: str) -> str)\n",
      "Code:\n",
      "from collections import Counter\n",
      "from libsa4py import PY_TYPING_MOD, PY_COLLECTION_MOD\n",
      "from itertools import chain\n",
      "from libsa4py.nl_preprocessing import NLPreprocessor\n",
      "import libcst as cst\n",
      "import libcst.matchers as match\n",
      "import re\n",
      "import regex\n",
      "from typing import Union, Dict, Tuple, List, Optional\n",
      "class CommentAndDocStringRemover(cst.CSTTransformer):\n",
      "   ...\n",
      "class StringRemover(cst.CSTTransformer):\n",
      "   ...\n",
      "class NumberRemover(cst.CSTTransformer):\n",
      "   ...\n",
      "class TypeAnnotationRemover(cst.CSTTransformer):\n",
      "   ...\n",
      "class TypeAdder(cst.CSTTransformer):\n",
      "   ...\n",
      "class SpaceAdder(cst.CSTTransformer):\n",
      "   ...\n",
      "class TypeQualifierResolver(cst.CSTTransformer):\n",
      "   ...\n",
      "class ParametricTypeDepthReducer(cst.CSTTransformer):\n",
      "   ...\n",
      "class TypeApplier(cst.CSTTransformer):\n",
      "   ...\n",
      "# libsa4py.cst_transformers\n",
      "class TypeApplier(cst.CSTTransformer):\n",
      "    cls_visited: List\n",
      "    fn_visited: List\n",
      "    last_visited_assign_t_count: int\n",
      "    nlp_p: Callable\n",
      "    def __get_fn_vars(self, var_name: str) -> str:...\n",
      "    \n",
      "    def __get_mod_vars(self) -> str:...\n",
      "    \n",
      "\n",
      "# Used above\n",
      "# libsa4py.cst_transformers\n",
      "class TypeApplier(cst.CSTTransformer):\n",
      "    def __get_cls_vars(self, var_name: <extra_id_0>) -> <extra_id_1>:\n",
      "        if var_name in self.cls_visited[-1][0]['variables']:\n",
      "            return self.cls_visited[-1][0]['variables'][var_name]\n",
      "    \n",
      "\n",
      "# Users below\n",
      "# libsa4py.cst_transformers\n",
      "class TypeApplier(cst.CSTTransformer):\n",
      "    def __get_var_type_assign_t(self, var_name: str) -> Optional[str]:\n",
      "        t = None\n",
      "        if len(self.cls_visited)!= 0:\n",
      "            if len(self.fn_visited)!= 0:\n",
      "                if self.fn_visited[-1][1][var_name] == self.last_visited_assign_t_count:\n",
      "                    t = self.__get_fn_vars(self.nlp_p(var_name))\n",
      "            else:\n",
      "                if self.cls_visited[-1][1][var_name] == self.last_visited_assign_t_count:\n",
      "                    t = self.__get_cls_vars(self.nlp_p(var_name))\n",
      "        elif len(self.fn_visited)!= 0:\n",
      "            if self.fn_visited[-1][1][var_name] == self.last_visited_assign_t_count:\n",
      "                t = self.__get_fn_vars(self.nlp_p(var_name))\n",
      "        else:\n",
      "            t = self.__get_mod_vars()[self.nlp_p(var_name)]\n",
      "        return t\n",
      "    \n",
      "    def __get_var_type_an_assign(self, var_name: str) -> Optional[str]:\n",
      "        if len(self.cls_visited)!= 0:\n",
      "            if len(self.fn_visited)!= 0:\n",
      "                t = self.__get_fn_vars(self.nlp_p(var_name))\n",
      "            else:\n",
      "                t = self.__get_cls_vars(self.nlp_p(var_name))\n",
      "        elif len(self.fn_visited)!= 0:\n",
      "            t = self.__get_fn_vars(self.nlp_p(var_name))\n",
      "        else:\n",
      "            t = self.__get_mod_vars()[self.nlp_p(var_name)]\n",
      "        return t\n",
      "    \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from spot.static_analysis import ProjectPath\n",
    "\n",
    "error_id = 35\n",
    "pred = ex_errors[error_id]\n",
    "print(\"----------------------\")\n",
    "print(\"Elem Path:\", pred.path)\n",
    "print(\"Type Index:\", pred.index)\n",
    "evalr.inspect_elem(pid, pred.path)\n",
    "# evalr.inspect_elem(pid, ProjectPath.from_str(\"tests.factories/AllScanCommandsAttemptsFactory.create\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.venv': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6ffc72953da4dd16b2e00785be9c4013ef131f465a8658f3921b6634d4eeec8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
