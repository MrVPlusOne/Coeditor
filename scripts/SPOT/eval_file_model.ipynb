{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiayi/Projects/SPOT/.venv/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import asyncio\n",
    "import os\n",
    "from typing import *\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "from spot.data import get_tk_dataset_name\n",
    "from spot.function_dataset import data_project_from_dir\n",
    "from spot.model import ModelWrapper\n",
    "from spot.train import TrainingConfig, PreprocessArgs\n",
    "from spot.type_env import AccuracyMetric\n",
    "from spot.utils import (\n",
    "    PickleCache,\n",
    "    assert_eq,\n",
    "    get_dataroot,\n",
    "    get_dataset_dir,\n",
    "    get_eval_dir,\n",
    "    get_gpu_id,\n",
    "    get_model_dir,\n",
    "    pickle_dump,\n",
    "    pmap,\n",
    "    pretty_print_dict,\n",
    "    pretty_show_dict,\n",
    "    proj_root,\n",
    "    run_long_task,\n",
    "    write_file,\n",
    ")\n",
    "from spot.visualization import string_to_html\n",
    "from termcolor import colored\n",
    "\n",
    "os.chdir(proj_root())\n",
    "\n",
    "\n",
    "def wandb_string(s: str):\n",
    "    return wandb.Html(string_to_html(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU_ID not set, using: 2\n",
      "\u001b[32mUse GPU: 2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# experiment configurations\n",
    "quicktest = False\n",
    "\n",
    "gpu_id = get_gpu_id(2)\n",
    "# model_name = \"model-v6--TrainingConfig(func_only=False, left_margin=2048, preamble_size=800, right_margin=1536)\"\n",
    "model_name = \"model-v6--TrainingConfig(func_only=False, imports_in_preamble=False, stub_in_preamble=False, left_margin=2048, right_margin=1536)\"\n",
    "pre_args = PreprocessArgs(imports_in_preamble=False, stub_in_preamble=False)\n",
    "dataset_name = \"ManyTypes4Py\"\n",
    "# dataset_name = \"InferTypes4Py\"\n",
    "# dataset_name = \"SPOT-src\"\n",
    "experiment_name = dataset_name + \": \" + model_name\n",
    "\n",
    "print(colored(f\"Use GPU: {gpu_id}\", \"green\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TokenizedSrcSets:  /mnt/nas/jiayi/SPOT/TokenizedSrcSets/ManyTypes4Py-v5-PreprocessArgs(imports_in_preamble=False, stub_in_preamble=False)\n",
      "254M\t/mnt/nas/jiayi/SPOT/TokenizedSrcSets/ManyTypes4Py-v5-PreprocessArgs(imports_in_preamble=False, stub_in_preamble=False)\n"
     ]
    }
   ],
   "source": [
    "# load test data\n",
    "from spot.data import load_tokenized_srcsets, create_tokenized_srcsets\n",
    "\n",
    "sdata_name = get_tk_dataset_name(dataset_name, pre_args, func_only=False)\n",
    "sdata_path = get_dataroot() / \"TokenizedSrcSets\" / sdata_name\n",
    "recreate=False\n",
    "if recreate or not sdata_path.exists():\n",
    "    create_tokenized_srcsets(\n",
    "        dataset_name,\n",
    "        sdata_path,\n",
    "        func_only=False,\n",
    "        pre_args=pre_args,\n",
    "    )\n",
    "tk_dataset = load_tokenized_srcsets(\n",
    "    sdata_path,\n",
    "    quicktest=quicktest,\n",
    "    sets_to_load=[\"test\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluation\n",
    "\n",
    "from spot.function_decoding import (\n",
    "    DecodingOrders,\n",
    "    EvalResult,\n",
    "    PreprocessArgs,\n",
    "    RolloutCtx,\n",
    ")\n",
    "from spot.function_dataset import sigmap_from_file_predictions\n",
    "from spot.static_analysis import SignatureErrorAnalysis\n",
    "\n",
    "# load model\n",
    "model = ModelWrapper.from_pretrained(get_model_dir() / model_name)\n",
    "device = torch.device(f\"cuda:{gpu_id}\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "ctx_args = model.args.ctx_args\n",
    "model.args.sampling_max_tokens = ctx_args.ctx_size\n",
    "model.args.do_sample = False\n",
    "model.args.num_beams = 10\n",
    "model.args.tokens_per_type = 16\n",
    "\n",
    "eval_cache = PickleCache(get_eval_dir(dataset_name, model_name) / f\"{pre_args}\")\n",
    "# eval_cache.clear()\n",
    "pre_r = eval_cache.cached(\n",
    "    \"DatasetPredResult.pkl\",\n",
    "    lambda: model.eval_on_dataset(tk_dataset[\"test\"]),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading test projects: 100%|██████████| 50/50 [00:26<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies on all types:\n",
      "header:  ['full.all', 'calibrated.simple', 'calibrated.complex', 'calibrated.all', 'base.all']\n",
      "67.07 & 72.12 & 44.05 & 67.47 & 73.44\n",
      "Accuracies on common types:\n",
      "header:  ['full.all', 'calibrated.simple', 'calibrated.complex', 'calibrated.all', 'base.all']\n",
      "76.74 & 82.43 & 53.03 & 78.04 & 82.44\n"
     ]
    }
   ],
   "source": [
    "repos_dir = get_dataset_dir(dataset_name) / \"repos\" / \"test\"\n",
    "test_repo_paths = [f for f in repos_dir.iterdir() if f.is_dir()]\n",
    "test_projects = pmap(\n",
    "    data_project_from_dir,\n",
    "    test_repo_paths,\n",
    "    desc=\"Loading test projects\",\n",
    ")\n",
    "assert len(test_projects) > 0\n",
    "\n",
    "common_names = ModelWrapper.load_common_type_names(get_model_dir() / model_name)\n",
    "pred_map, label_map = sigmap_from_file_predictions(pre_r, test_projects, repos_dir)\n",
    "accs = {\n",
    "    m.name: SignatureErrorAnalysis(pred_map, label_map, m).accuracies\n",
    "    for m in AccuracyMetric.default_metrics(common_names)\n",
    "}\n",
    "\n",
    "from spot.experiments.typet5 import accs_as_table_row\n",
    "accs_as_table_row(accs)\n",
    "pretty_print_dict(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exporting: 100%|██████████| 1851/1851 [00:19<00:00, 96.51it/s] \n",
      "Computing accuracies: 100%|██████████| 1851/1851 [00:00<00:00, 11808.94it/s]\n"
     ]
    }
   ],
   "source": [
    "from spot.utils import decode_tokens, Path\n",
    "from spot.visualization import export_preds_on_code\n",
    "\n",
    "export_to = Path(f\"caches/model_predictions/eval_file_model/{dataset_name}\")\n",
    "export_preds_on_code(pre_r.chunks, pre_r.predictions, export_to, AccuracyMetric(common_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.venv': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6ffc72953da4dd16b2e00785be9c4013ef131f465a8658f3921b6634d4eeec8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
