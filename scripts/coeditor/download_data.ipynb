{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from coeditor.common import *\n",
    "import os\n",
    "\n",
    "import requests\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "os.chdir(proj_root())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import dateparser\n",
    "from spot.data import GitRepo\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "\n",
    "def request_page(page: int, n_items: int = 10):\n",
    "    return requests.get(\n",
    "        f\"https://api.github.com/search/repositories?q=NOT+interview+NOT+reference+NOT+course+NOT+cheatsheet+created%3A>2018-01-01+stars%3A>1000+size%3A<20000+language%3APython&sort=stars&order=desc&per_page={n_items}&page={page}\"\n",
    "    ).json()\n",
    "\n",
    "\n",
    "def fetch_top_python_repos(n_repos: int):\n",
    "    repos = dict[str, GitRepo]()\n",
    "    i = 1\n",
    "    with tqdm(total=n_repos) as pbar:\n",
    "        while len(repos) < n_repos:\n",
    "            page = request_page(i, n_items=100)\n",
    "            time.sleep(0.5)\n",
    "            if (msg := page.get(\"message\", \"\")) and msg.startswith(\"API rate limit exceeded\"):\n",
    "                print(\"API rate limit exceeded, now wait for 1 min\")\n",
    "                time.sleep(60)\n",
    "                continue\n",
    "            if not page.get(\"items\"):\n",
    "                print(\"Fetching page failed:\")\n",
    "                print(page)\n",
    "                break\n",
    "            for item in page[\"items\"]:\n",
    "                r = GitRepo(\n",
    "                    author=item[\"owner\"][\"login\"],\n",
    "                    name=item[\"name\"],\n",
    "                    url=item[\"html_url\"],\n",
    "                    description=item[\"description\"],\n",
    "                    stars=item[\"stargazers_count\"],\n",
    "                    forks=item[\"forks_count\"],\n",
    "                    archived=item[\"archived\"],\n",
    "                    last_update=dateparser.parse(item[\"pushed_at\"]).replace(tzinfo=None),\n",
    "                )\n",
    "                if not r.archived:\n",
    "                    if r.authorname() in repos:\n",
    "                        warnings.warn(f\"{r.authorname()} already in repos\", UserWarning)\n",
    "                    repos[r.authorname()] = r\n",
    "            pbar.update(len(page[\"items\"]))\n",
    "            i += 1\n",
    "    return [repos[k] for k in list(repos)[:n_repos]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 997/1000 [00:37<00:00, 30.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API rate limit exceeded, now wait for 2 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [02:38<00:00,  6.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page failed:\n",
      "{'message': 'Only the first 1000 search results are available', 'documentation_url': 'https://docs.github.com/v3/search/'}\n",
      "Repos: 978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_repos = fetch_top_python_repos(1000)\n",
    "print(\"Repos:\", len(all_repos))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "downloading repos: 100%|██████████| 978/978 [10:31<00:00,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: 978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"xl\"\n",
    "repos_dir = get_dataset_dir(dataset_name)\n",
    "(repos_dir / \"downloading\").mkdir(exist_ok=True, parents=True)\n",
    "(repos_dir / \"downloaded\").mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "downloaded = pmap(\n",
    "    GitRepo.download,\n",
    "    all_repos,\n",
    "    [repos_dir] * len(all_repos),\n",
    "    [True] * len(all_repos),\n",
    "    desc=\"downloading repos\",\n",
    "    max_workers=4,\n",
    ")\n",
    "\n",
    "print(\"Successfully downloaded:\", sum(downloaded))\n",
    "downloaded_repos = [r for r, d in zip(all_repos, downloaded) if d]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "978"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downloaded_repos = [r for rs in split.values() for r in rs]\n",
    "len(downloaded_repos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "getting repo signatures:  16%|█▌        | 157/978 [00:20<01:45,  7.77it/s]warning: inexact rename detection was skipped due to too many files.\n",
      "warning: you may want to set your diff.renameLimit variable to at least 2368 and retry the command.\n",
      "getting repo signatures:  57%|█████▋    | 553/978 [01:30<01:09,  6.08it/s]warning: inexact rename detection was skipped due to too many files.\n",
      "warning: you may want to set your diff.renameLimit variable to at least 649 and retry the command.\n",
      "getting repo signatures:  57%|█████▋    | 559/978 [01:32<01:09,  6.02it/s]warning: inexact rename detection was skipped due to too many files.\n",
      "warning: you may want to set your diff.renameLimit variable to at least 617 and retry the command.\n",
      "getting repo signatures: 100%|██████████| 978/978 [03:53<00:00,  4.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 repos have the same signature ('add tensorboard logger', 'README modify heading', 'README add acknowlegement', 'check before release', 'update README', 'update README', 'updata README', 'update README', 'update README', 'clean others', 'clean data loader', 'add wgan-gp and other matters', 'add extract subimages based on var', 'add discriminator arch', 'SRGAN modify loss action and dataset ref', 'change dataset opt', 'add LRHRRef dataloader', 'srgan initial', 'format logger', 'update discriminator and vggfeatureextractor network', 'merge generator, discriminator and perceptual network to architecture', 'modify GANloss', 'add perceptual network(VGG feature extractor)', 'README', 'update README', 'add GAN loss', 'json with leading comma', 'add Degradation Net', 'first commit', 'Initial commit'):\n",
      "  XPixelGroup?BasicSR\n",
      "  xinntao?EDVR\n",
      "2 repos have the same signature ('Update README.md', 'Update README.md', 'Update README.md', 'Update README.md', 'Update README.md', 'Update README.md', 'Update README.md', 'Update README.md', 'Update README.md', 'Update README.md', 'Update README.md', 'Update README.md', 'Update README.md', 'Update README.md', 'Update README.md', 'Update README.md', 'Update README.md', 'Update README.md', 'Update README.md', 'Update README.md', 'Update README.md', 'Update README.md', 'Update README.md', 'Update README.md', 'Update README.md', 'Update README.md', 'Update README.md', 'Update README.md', 'Update README.md', 'Create README.md'):\n",
      "  Embedding?Chinese-Word-Vectors\n",
      "  KingOfBugbounty?KingOfBugBountyTips\n",
      "2 repos have the same signature ('Inference using either TensorFlow or ONNX_TF', 'Merge pull request #34 from timmh/master', 'add docker usage instructions', 'implement Dockerfile', 'Expose default transform in hubconf', \"Merge branch 'master' of https://github.com/intel-isl/MiDaS\", 'Update paths to avoid conflicts in Hub', 'Update link to weights', 'Fix unnecessary download of weights', 'Fixed link to weight', 'Missing dependency', 'typo', 'Adding hubconf.py', 'Update README.md', 'Update readme', 'Adding link to original release', 'Remove old model, update README', 'update model.pt link', 'Update readme', 'Update readme', 'Update to latest version of MiDaS', 'Merge pull request #3 from barnjamin/master', 'adding configurable depth bits', 'Remove debug statement', 'Minor', 'Update README', 'Update README, remove spurious dependencies', 'Update README.md', 'initial commit', 'Initial commit'):\n",
      "  isl-org?DPT\n",
      "  isl-org?MiDaS\n",
      "Totoal duplicates: 3\n",
      "Remaining repos: 975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from coeditor.dataset import get_repo_signature\n",
    "repo_paths = [repos_dir / \"downloaded\" / r.authorname() for r in downloaded_repos]\n",
    "sigs = pmap(get_repo_signature, repo_paths, desc=\"getting repo signatures\")\n",
    "sig_groups = groupby(enumerate(sigs), lambda x: x[1])\n",
    "\n",
    "duplicates = set[str]()\n",
    "for sig, group in sig_groups.items():\n",
    "    if len(group) > 1:\n",
    "        print(f\"{len(group)} repos have the same signature {sig}:\")\n",
    "        for i, _ in group:\n",
    "            print(f\"  {downloaded_repos[i].authorname()}\")\n",
    "        for i, _ in group[1:]:\n",
    "            duplicates.add(downloaded_repos[i].authorname())\n",
    "            \n",
    "print(\"Totoal duplicates:\", len(duplicates))\n",
    "downloaded_repos = [r for r in downloaded_repos if r.authorname() not in duplicates]\n",
    "print(\"Remaining repos:\", len(downloaded_repos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_test=50, n_valid=50, n_train=875\n"
     ]
    }
   ],
   "source": [
    "n_test = 50\n",
    "n_valid = 50\n",
    "n_train = len(downloaded_repos) - n_test - n_valid\n",
    "print(f\"n_test={n_test}, n_valid={n_valid}, n_train={n_train}\")\n",
    "\n",
    "random.seed(42)\n",
    "downloaded_repos.sort(key=lambda r: r.authorname())\n",
    "random.shuffle(downloaded_repos)\n",
    "\n",
    "split = {\n",
    "    \"train\": downloaded_repos[n_test + n_valid :][:n_train],\n",
    "    \"test\": downloaded_repos[:n_test],\n",
    "    \"valid\": downloaded_repos[n_test : n_test + n_valid],\n",
    "}\n",
    "\n",
    "pickle_dump(repos_dir / \"repos_split.pkl\", split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "moving train: 100%|██████████| 875/875 [00:01<00:00, 805.73it/s]\n",
      "moving test: 100%|██████████| 50/50 [00:00<00:00, 802.17it/s]\n",
      "moving valid: 100%|██████████| 50/50 [00:00<00:00, 797.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# move downloaded repos to their split group\n",
    "for group, rs in split.items():\n",
    "    for repo in tqdm(rs, desc=f\"moving {group}\"):\n",
    "        dest = repos_dir / \"repos\" / group\n",
    "        dest.mkdir(exist_ok=True, parents=True)\n",
    "        shutil.move(repos_dir / \"downloaded\" / repo.authorname(), dest)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.venv': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0 | packaged by conda-forge | (main, Oct 25 2022, 06:24:40) [GCC 10.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6ffc72953da4dd16b2e00785be9c4013ef131f465a8658f3921b6634d4eeec8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
