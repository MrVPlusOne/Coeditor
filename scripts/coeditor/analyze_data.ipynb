{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from coeditor.common import *\n",
    "from coeditor.dataset import *\n",
    "from coeditor.encoding import *\n",
    "from coeditor.encoders import QueryRefEditEncoder\n",
    "\n",
    "from spot.utils import pretty_print_dict\n",
    "\n",
    "os.chdir(proj_root())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting commit histories: 100%|██████████| 1/1 [00:01<00:00,  1.73s/repo]\n",
      "Create tokenized edits: 100%|██████████| 4/4 [02:09<00:00, 32.41s/chunk]\n"
     ]
    }
   ],
   "source": [
    "# test_data_name = \"medium\"\n",
    "# test_data_name = \"SPOT\"\n",
    "# encoder = AnalysisBasedEditEncoder(extra_ctx_names=(\"usees\", \"post-usees\"), add_truncate_bos=False)\n",
    "# encoder = CstBasedEditEncoder(500, add_truncate_bos=False)\n",
    "encoder = QueryRefEditEncoder(ast_mask_prob=0.1)\n",
    "data = dataset_from_projects([proj_root()], encoder, [True])#, max_history_per_repo=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_projects: 1\n",
      "n_edits: 353\n",
      "n_additions: 0\n",
      "input_tks:\n",
      "   mean: 309.14\n",
      "   median: 287\n",
      "   min: 29\n",
      "   max: 512\n",
      "output_tks:\n",
      "   mean: 85.348\n",
      "   median: 72\n",
      "   min: 3\n",
      "   max: 256\n",
      "prev_chunks:\n",
      "   mean: 0.33144\n",
      "   median: 0\n",
      "   min: 0\n",
      "   max: 5\n",
      "is_rename_update:\n",
      "   mean: 0\n",
      "   median: 0\n",
      "   min: 0\n",
      "   max: 0\n",
      "n_references:\n",
      "   mean: 30.62\n",
      "   median: 26\n",
      "   min: 1\n",
      "   max: 122\n",
      "ref_size_max:\n",
      "   mean: 490.27\n",
      "   median: 512\n",
      "   min: 190\n",
      "   max: 512\n",
      "ref_size_sum:\n",
      "   mean: 6721.6\n",
      "   median: 6747\n",
      "   min: 265\n",
      "   max: 13402\n"
     ]
    }
   ],
   "source": [
    "pretty_print_dict(data.overall_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "renamed updates: 14\n",
      "n_references: 55\n",
      "n_ref_blocks: 64\n",
      "========Ground Truth========\n",
      " <3>:<add>         load_tokenizer_spot(),\n",
      "     <del>         get_spot_tokenizer(),\n",
      "\n",
      "========Main Code========\n",
      "# edit: tests.test_type_env/test_mypy_checking\n",
      "def test_mypy_checking():\n",
      " <0>    simple_dataset = SrcDataset.from_repos(\n",
      " <1>        proj_root() / \"data\",\n",
      " <2>        [proj_root() / \"data/code\"],\n",
      " <3>        get_spot_tokenizer(),\n",
      " <4>        drop_comments=True,\n",
      " <5>        max_workers=10,\n",
      " <6>        label_ratio=1.0,\n",
      " <7>    )\n",
      " <8>\n",
      " <9>    src_to_check = simple_dataset.get_src_by_file(Path(\"bad_code_2.py\"))\n",
      "<10>    result_1 = type_check_src(src_to_check, {0: \"int\"})\n",
      "<11>    assert len(result_1.feedbacks) == 0\n",
      "<12>\n",
      "<13>    src_to_check = simple_dataset.get_src_by_file(Path(\"bad_code_2.py\"))\n",
      "<14>    temp_dir = proj_root() / \"mypy_temp/test_dir\"\n",
      "<15>    shutil.rmtree(temp_dir, ignore_errors=True)\n",
      "<16>\n",
      "<17>    result_2 = type_check_src_in_project(\n",
      "<18>        src_to_check,\n",
      "<19>        {0: \"int\"},\n",
      "<20>        project_root=(proj_root() / \"data/code\"),\n",
      "<21>    )\n",
      "<22>    assert isinstance(result_2.feedbacks, list) and len(result_2.feedbacks) == 1\n",
      "<23>    assert (\n",
      "<24>        'Argument 1 to \"fib\" has incompatible type \"int\"; expected \"str\"'\n",
      "<25>        in result_2.feedbacks[0].message\n",
      "<26>    )\n",
      "<27>\n",
      "===========spot.critic/CriticOutput.logits===========\n",
      "<add> # spot.critic/CriticOutput.logits\n",
      " <add>     logits: torch.Tensor\n",
      "===========spot.critic/CriticOutput.loss===========\n",
      "<add> # spot.critic/CriticOutput.loss\n",
      " <add>     loss: Optional[torch.Tensor]\n",
      "===========spot.critic/CriticOutput.n_preds===========\n",
      "<add> # spot.critic/CriticOutput.n_preds\n",
      " <add>     n_preds: list[int]\n",
      "===========spot.critic/CriticModel._keys_to_ignore_on_load_unexpected===========\n",
      "<add> # spot.critic/CriticModel._keys_to_ignore_on_load_unexpected\n",
      " <add>     _keys_to_ignore_on_load_unexpected = [r\"decoder\\.\", r\"lm_head\\.\"]\n",
      "===========spot.critic/CriticModel.__init__===========\n",
      "<add> # spot.critic/CriticModel.__init__\n",
      " <add>     def __init__(self, config: T5Config):\n",
      " <add>         super().__init__(config)\n",
      " <add>         self.t5enc = T5EncoderModel(config)\n",
      " <add>         self.base_model_prefix = \"t5enc\"\n",
      " <add> \n",
      " <add>         self.config = config\n",
      " <add>         self.critic_dropout = nn.Dropout(config.dropout_rate)\n",
      " <add>         self.critic_classifier = nn.Linear(config.d_model, 1)\n",
      " <add>         self.tokenizer = tokenizer = load_tokenizer_spot()\n",
      " <add>         self.extra_id_min = tokenizer.additional_special_tokens_ids[0]\n",
      " <add>         self.extra_id_max = tokenizer.additional_special_tokens_ids[-1]\n",
      " <add> \n",
      " <add>         self.post_init()\n",
      "\n",
      "===========spot.critic/CriticModel.t5enc===========\n",
      "<add> # spot.critic/CriticModel.t5enc\n",
      " <add>     t5enc:...\n",
      "===========spot.critic/CriticModel.base_model_prefix===========\n",
      "<add> # spot.critic/CriticModel.base_model_prefix\n",
      " <add>     base_model_prefix:...\n",
      "===========spot.critic/CriticModel.config===========\n",
      "<add> # spot.critic/CriticModel.config\n",
      " <add>     config:...\n",
      "===========spot.critic/CriticModel.critic_dropout===========\n",
      "<add> # spot.critic/CriticModel.critic_dropout\n",
      " <add>     critic_dropout:...\n",
      "===========spot.critic/CriticModel.critic_classifier===========\n",
      "<add> # spot.critic/CriticModel.critic_classifier\n",
      " <add>     critic_classifier:...\n",
      "===========spot.critic/CriticModel.tokenizer===========\n",
      "<add> # spot.critic/CriticModel.tokenizer\n",
      " <add>     tokenizer:...\n",
      "===========spot.critic/CriticModel.extra_id_min===========\n",
      "<add> # spot.critic/CriticModel.extra_id_min\n",
      " <add>     extra_id_min:...\n",
      "===========spot.critic/CriticModel.extra_id_max===========\n",
      "<add> # spot.critic/CriticModel.extra_id_max\n",
      " <add>     extra_id_max:...\n",
      "===========spot.critic/CriticModel.forward===========\n",
      "<add> # spot.critic/CriticModel.forward\n",
      " <add>     def forward(\n",
      " <add>         self,\n",
      " <add>         input_ids: torch.LongTensor,\n",
      " <add>         labels: Optional[torch.BoolTensor] = None,\n",
      " <add>         **kwargs,\n",
      " <add>     ):\n",
      " <add>         kwargs[\"return_dict\"] = True\n",
      " <add>         kwargs[\"output_hidden_states\"] = True\n",
      " <add>         outputs = self.t5enc.forward(input_ids, **kwargs)\n",
      " <add>         assert isinstance(outputs, BaseModelOutputWithPastAndCrossAttentions)\n",
      " <add>         hidden_states = not_none(outputs.hidden_states)[-1]\n",
      " <add>         assert len(hidden_states.shape) == 3\n",
      " <add> \n",
      " <add>         isextra = (self.extra_id_min <= input_ids).bitwise_and(\n",
      " <add>             input_ids <= self.extra_id_max\n",
      " <add>         )\n",
      " <add>         n_preds = isextra.count_nonzero(dim=1).tolist()\n",
      " <add>         hidden_states = hidden_states[isextra, :]\n",
      " <add>         assert len(hidden_states.shape) == 2\n",
      " <add>         hidden_states = self.critic_dropout(hidden_states)\n",
      " <add>         logits = self.critic_classifier(hidden_states).reshape(-1)\n",
      " <add>         loss = None\n",
      " <add>         if labels is not None:\n",
      " <add>             loss = torch.binary_cross_entropy_with_logits(\n",
      " <add>                 logits,\n",
      " <add>                 labels.to(dtype=logits.dtype),\n",
      " <add>             ).mean()\n",
      " <add> \n",
      " <add>         return CriticOutput(logits, loss, n_preds)\n",
      "\n",
      "===========spot.critic/CriticModel.eval_on_dataset (0)===========\n",
      "<add> # spot.critic/CriticModel.eval_on_dataset\n",
      " <add>     def eval_on_dataset(\n",
      " <add>         self,\n",
      " <add>         src_data: SrcDataset,\n",
      " <add>         ctx_args: CtxArgs,\n",
      " <add>         sampling_max_tokens: int,\n",
      " <add>         tqdm_args: dict = {},\n",
      " <add>     ) -> tuple[ChunkedDataset, list[list[float]], dict]:\n",
      " <add>         chunks = src_data.to_chunks(self.tokenizer, ctx_args, tqdm_args=tqdm_args)\n",
      " <add>         collator = CriticCollator(self.tokenizer)\n",
      " <add>         loader = dynamic_dataloader(\n",
      " <add>             chunks.data,\n",
      " <add>             max_tokens=sampling_max_tokens,\n",
      " <add>             collate_fn=collator,\n",
      " <add>             shuffle=True,\n",
      " <add>         )\n",
      " <add>         device = self.device\n",
      " <add>         preds = dict[int, list[float]]()\n",
      " <add>         tqdm_bar = tqdm(total=len(chunks.data), desc=\"predict\", **tqdm_args)\n",
      " <add>         self.eval()\n",
      " <add>         with torch.no_grad():\n",
      " <add>             for batch in loader:\n",
      " <add>                 batch_size = batch[\"input_ids\"].shape[0]\n",
      " <add>                 out = self.forward(input_ids=batch[\"input_ids\"].to(device))\n",
      " <add>                 pred_vec: list[float] = (out.logits.sigmoid()).tolist()\n",
      " <add>                 pred_counter = 0\n",
      " <add>                 for i, c_id in enumerate(batch[\"chunk_id\"]):\n",
      " <add>                     c_id = int(c_id)\n",
      " <add>                     pred_counter_next = pred_counter + out.n_preds[i]\n",
      " <add>                     preds[c_id] = pred_vec[pred_counter:pred_counter_next]\n",
      " <add>                     pred_counter = pred_counter_next\n",
      " <add> \n",
      " <add>                 tqdm_bar.update(batch_size)\n",
      " <add>         tqdm_bar.close()\n",
      " <add>         preds = [preds[int(c_id)] for c_id in chunks.data[\"chunk_id\"]]\n",
      " <add>         target = list(seq_flatten(to_critic_dataset(chunks)[\"labels\"]</s>\n",
      "===========spot.critic/CriticModel.eval_on_dataset (1)===========\n",
      "<add> # spot.critic/CriticModel.eval_on_dataset\n",
      "<s>flatten(to_critic_dataset(chunks)[\"labels\"]))\n",
      " <add>         pred_bools = [p >= 0.5 for p in seq_flatten(preds)]\n",
      " <add> \n",
      " <add>         metrics = {\n",
      " <add>             \"Acc\": accuracy_score(pred_bools, target),\n",
      " <add>             \"F1\": f1_score(pred_bools, target),\n",
      " <add>         }\n",
      " <add> \n",
      " <add>         return chunks, preds, metrics\n",
      "\n",
      "===========spot.critic/stack_and_pad===========\n",
      "<add> # spot.critic/stack_and_pad\n",
      " <add> def stack_and_pad(xs: list[list[int]], pad_id: int) -> torch.LongTensor:\n",
      " <add>     max_len = max(len(x) for x in xs)\n",
      " <add>     xs = [x + [pad_id] * (max_len - len(x)) for x in xs]\n",
      " <add>     return torch.LongTensor(xs)\n",
      "\n",
      "===========spot.critic/CriticTrainArgs.ctx_args===========\n",
      "<add> # spot.critic/CriticTrainArgs.ctx_args\n",
      " <add>     ctx_args: CtxArgs\n",
      "===========spot.critic/CriticTrainArgs.train_max_tokens===========\n",
      "<add> # spot.critic/CriticTrainArgs.train_max_tokens\n",
      " <add>     train_max_tokens: int\n",
      "===========spot.critic/CriticTrainArgs.eval_max_tokens===========\n",
      "<add> # spot.critic/CriticTrainArgs.eval_max_tokens\n",
      " <add>     eval_max_tokens: int\n",
      "===========spot.critic/CriticTrainArgs.max_epochs===========\n",
      "<add> # spot.critic/CriticTrainArgs.max_epochs\n",
      " <add>     max_epochs: int\n",
      "===========spot.critic/train_critic_model (0)===========\n",
      "<add> # spot.critic/train_critic_model\n",
      " <add> def train_critic_model(\n",
      " <add>     critic_datasets: dict[str, SrcDataset],\n",
      " <add>     train_args: CriticTrainArgs,\n",
      " <add>     model_name: str,\n",
      " <add>     gpus: list[int],\n",
      " <add>     quicktest=False,\n",
      " <add>     use_early_stop=True,\n",
      " <add>     use_small_model=False,\n",
      " <add> ) -> tuple[CriticModel, dict]:\n",
      " <add>     os.chdir(proj_root())\n",
      " <add> \n",
      " <add>     datadir = Path(os.getenv(\"datadir\", \"data\"))\n",
      " <add> \n",
      " <add>     running_dir = datadir / \"checkpoints/lit-running\" / model_name\n",
      " <add>     if running_dir.exists():\n",
      " <add>         shutil.rmtree(running_dir)\n",
      " <add>     running_dir.mkdir(parents=True, exist_ok=True)\n",
      " <add> \n",
      " <add>     model_path = (\n",
      " <add>         \"Salesforce/codet5-small\" if use_small_model else \"Salesforce/codet5-base\"\n",
      " <add>     )\n",
      " <add>     lit_model = TrainCriticModelWrapper(model_path)\n",
      " <add>     model = lit_model.model\n",
      " <add>     tokenizer: TokenizerSPOT = lit_model.model.tokenizer\n",
      " <add> \n",
      " <add>     datasets: dict[str, Dataset] = {}\n",
      " <add>     with run_long_task(\"Preparing critic datasets\", notify=False):\n",
      " <add>         for n in [\"valid\", \"test\", \"train\"]:\n",
      " <add>             cdata = critic_datasets[n].to_chunks(tokenizer, train_args.ctx_args)\n",
      " <add>             datasets[n] = to_critic_dataset(cdata)\n",
      " <add> \n",
      " <add>     wandb_logger = WandbLogger()\n",
      " <add> \n",
      " <add>     collate_fn = CriticCollator(tokenizer)\n",
      " <add>     dataloaders = dict[str, DataLoader]()\n",
      " <add>     for n, data in datasets.items():\n",
      " <add>         dataloaders[n] = dynamic_dataloader(\n",
      " <add>             data,\n",
      " <add>             max_tokens=(\n",
      " <add>                 train_args.train_max_tokens\n",
      " <add>                 if n == \"train</s>\n",
      "===========spot.critic/train_critic_model (1)===========\n",
      "<add> # spot.critic/train_critic_model\n",
      "<s>.train_max_tokens\n",
      " <add>                 if n == \"train\"\n",
      " <add>                 else train_args.eval_max_tokens\n",
      " <add>             ),\n",
      " <add>             collate_fn=collate_fn,\n",
      " <add>             shuffle=True,\n",
      " <add>         )\n",
      " <add> \n",
      " <add>     ckpt_interval = max(1, len(dataloaders[\"train\"]) // 8)\n",
      " <add>     val_interval = 1 if quicktest else max(500, ckpt_interval)\n",
      " <add> \n",
      " <add>     checkpoint_cb = ModelCheckpoint(\n",
      " <add>         dirpath=running_dir,\n",
      " <add>         save_top_k=3,\n",
      " <add>         monitor=\"valid/loss\",\n",
      " <add>         mode=\"min\",\n",
      " <add>         save_on_train_epoch_end=False,\n",
      " <add>         verbose=quicktest,\n",
      " <add>     )\n",
      " <add> \n",
      " <add>     trainer = pl.Trainer(\n",
      " <add>         default_root_dir=str(running_dir),\n",
      " <add>         accelerator=\"gpu\" if gpus else \"cpu\",\n",
      " <add>         gpus=gpus,\n",
      " <add>         precision=16,\n",
      " <add>         max_epochs=train_args.max_epochs,\n",
      " <add>         logger=wandb_logger,\n",
      " <add>         val_check_interval=val_interval,\n",
      " <add>         callbacks=(\n",
      " <add>             [checkpoint_cb, EarlyStopping(\"valid/loss\", mode=\"min\", verbose=quicktest)]\n",
      " <add>             if use_early_stop\n",
      " <add>             else []\n",
      " <add>         ),\n",
      " <add>         gradient_clip_val=1.0,\n",
      " <add>         gradient_clip_algorithm=\"norm\",\n",
      " <add>         accumulate_grad_batches=None,\n",
      " <add>     )\n",
      " <add> \n",
      " <add>     warnings.filterwarnings(\"ignore\", \"The dataloader.*does not have many workers.*\")\n",
      " <add> \n",
      " <add>     with run_long_task(f\"Training {model_name}\"):\n",
      " <add>         trainer.fit(\n",
      " <add>             model=lit_model,\n",
      " <add>             train_dataloaders=dataloaders[\"train\"],\n",
      " <add>             val_dataloaders=dataloaders[\"valid\"],\n",
      " <add>         )\n",
      " <add> \n",
      " <add>     extra = dict[str, Any]()\n",
      " <add>     save_dir = datadir / \"checkpoints/</s>\n",
      "===========spot.critic/train_critic_model (2)===========\n",
      "<add> # spot.critic/train_critic_model\n",
      "<s>\n",
      " <add>     save_dir = datadir / \"checkpoints/lit-saved\" / model_name\n",
      " <add> \n",
      " <add>     final_eval = trainer.validate(model=lit_model, dataloaders=dataloaders[\"valid\"])[0]\n",
      " <add> \n",
      " <add>     try:\n",
      " <add>         if (\n",
      " <add>             use_early_stop\n",
      " <add>             and (best_loss := checkpoint_cb.best_model_score) is not None\n",
      " <add>             and best_loss < final_eval[\"valid/loss\"]\n",
      " <add>         ):\n",
      " <add>             print(\n",
      " <add>                 f\"Loading best model with score {best_loss} from: {checkpoint_cb.best_model_path}\"\n",
      " <add>             )\n",
      " <add>             model = TrainCriticModelWrapper.load_from_checkpoint(\n",
      " <add>                 checkpoint_cb.best_model_path\n",
      " <add>             ).model\n",
      " <add>             lit_model.model = model\n",
      " <add>             trainer.test(model=lit_model, dataloaders=dataloaders[\"test\"])\n",
      " <add>         if save_dir.exists():\n",
      " <add>             shutil.rmtree(save_dir)\n",
      " <add>         save_dir.mkdir(parents=True, exist_ok=True)\n",
      " <add> \n",
      " <add>         model.save_pretrained(save_dir)\n",
      " <add> \n",
      " <add>         shutil.rmtree(running_dir)\n",
      " <add>     except Exception as e:\n",
      " <add>         logging.error(\n",
      " <add>             \"Error encountered after training, returning partial results... Error:\\n\", e\n",
      " <add>         )\n",
      " <add> \n",
      " <add>     return model, extra\n",
      "\n",
      "===========spot.critic/to_critic_dataset===========\n",
      "<add> # spot.critic/to_critic_dataset\n",
      " <add> def to_critic_dataset(cdata: ChunkedDataset) -> Dataset:\n",
      " <add>     new_data = dict()\n",
      " <add>     new_data[\"input_ids\"] = cdata.data[\"input_ids\"]\n",
      " <add>     new_data[\"labels\"] = labels = list[list[bool]]()\n",
      " <add>     for info in cdata.chunks_info:\n",
      " <add>         labels.append(\n",
      " <add>             [\n",
      " <add>                 normalize_type(p) == normalize_type(l)\n",
      " <add>                 for p, l in zip(not_none(info.prev_types), info.types)\n",
      " <add>             ]\n",
      " <add>         )\n",
      " <add> \n",
      " <add>     new_data[\"chunk_id\"] = cdata.data[\"chunk_id\"]\n",
      " <add>     return Dataset.from_dict(new_data)\n",
      "\n",
      "===========spot.critic/CriticCollator.tokenizer===========\n",
      "<add> # spot.critic/CriticCollator.tokenizer\n",
      " <add>     tokenizer: TokenizerSPOT\n",
      "===========spot.critic/CriticCollator.__call__===========\n",
      "<add> # spot.critic/CriticCollator.__call__\n",
      " <add>     def __call__(self, batch: Sequence[dict]) -> dict:\n",
      " <add>         pad_id = not_none(self.tokenizer.pad_token_id)\n",
      " <add>         return {\n",
      " <add>             \"input_ids\": stack_and_pad([b[\"input_ids\"] for b in batch], pad_id),\n",
      " <add>             \"labels\": torch.BoolTensor([l for chunk in batch for l in chunk[\"labels\"]]),\n",
      " <add>             \"chunk_id\": [chunk[\"chunk_id\"] for chunk in batch],\n",
      " <add>         }\n",
      "\n",
      "===========spot.critic/TrainCriticModelWrapper.__init__===========\n",
      "<add> # spot.critic/TrainCriticModelWrapper.__init__\n",
      " <add>     def __init__(self, model_checkpoint: str | Path) -> None:\n",
      " <add>         super().__init__()\n",
      " <add>         self.save_hyperparameters()\n",
      " <add>         model = CriticModel.from_pretrained(model_checkpoint)\n",
      " <add>         assert isinstance(model, CriticModel)\n",
      " <add>         self.model: CriticModel = model\n",
      "\n",
      "===========spot.critic/TrainCriticModelWrapper.model===========\n",
      "<add> # spot.critic/TrainCriticModelWrapper.model\n",
      " <add>     model:...\n",
      "===========spot.critic/TrainCriticModelWrapper.configure_optimizers===========\n",
      "<add> # spot.critic/TrainCriticModelWrapper.configure_optimizers\n",
      " <add>     def configure_optimizers(self):\n",
      " <add>         return _configure_optimizers(self.model)\n",
      "\n",
      "===========spot.critic/TrainCriticModelWrapper.training_step===========\n",
      "<add> # spot.critic/TrainCriticModelWrapper.training_step\n",
      " <add>     def training_step(self, batch, batch_idx):\n",
      " <add>         outputs = self.model.forward(\n",
      " <add>             input_ids=batch[\"input_ids\"],\n",
      " <add>             labels=batch[\"labels\"],\n",
      " <add>         )\n",
      " <add>         batch_size = batch[\"input_ids\"].shape[0]\n",
      " <add>         loss = not_none(outputs.loss)\n",
      " <add>         self.log(\"train/loss\", loss.item(), batch_size=batch_size)\n",
      " <add>         self.log(\"train/lr\", self.lr_schedulers().get_last_lr()[0], batch_size=batch_size)\n",
      " <add>         return loss\n",
      "\n",
      "===========spot.critic/TrainCriticModelWrapper._eval_step===========\n",
      "<add> # spot.critic/TrainCriticModelWrapper._eval_step\n",
      " <add>     def _eval_step(self, batch, name: str):\n",
      " <add>         outputs = self.model.forward(\n",
      " <add>             input_ids=batch[\"input_ids\"],\n",
      " <add>             labels=batch[\"labels\"],\n",
      " <add>         )\n",
      " <add>         batch_size = batch[\"input_ids\"].shape[0]\n",
      " <add>         loss = not_none(outputs.loss)\n",
      " <add>         self.log(f\"{name}/loss\", loss.item(), batch_size=batch_size)\n",
      " <add> \n",
      " <add>         preds = (outputs.logits >= 0).cpu()\n",
      " <add>         target = batch[\"labels\"].cpu()\n",
      " <add> \n",
      " <add>         acc = accuracy_score(preds, target)\n",
      " <add>         self.log(f\"{name}/accuracy\", as_any(acc), batch_size=batch_size)\n",
      " <add> \n",
      " <add>         f1 = f1_score(preds, target)\n",
      " <add>         self.log(f\"{name}/f1\", as_any(f1), batch_size=batch_size)\n",
      "\n",
      "===========spot.critic/TrainCriticModelWrapper.validation_step===========\n",
      "<add> # spot.critic/TrainCriticModelWrapper.validation_step\n",
      " <add>     def validation_step(self, batch, batch_idx):\n",
      " <add>         self._eval_step(batch, \"valid\")\n",
      "\n",
      "===========spot.critic/TrainCriticModelWrapper.test_step===========\n",
      "<add> # spot.critic/TrainCriticModelWrapper.test_step\n",
      " <add>     def test_step(self, batch, batch_idx):\n",
      " <add>         self._eval_step(batch, \"test\")\n",
      "\n",
      "===========spot.critic/evaluate_critic===========\n",
      "<add> # spot.critic/evaluate_critic\n",
      " <add> def evaluate_critic(\n",
      " <add>     critic: CriticModel,\n",
      " <add>     args: CtxArgs,\n",
      " <add>     sampling_max_tokens: int,\n",
      " <add>     r1_srcs: SrcDataset,\n",
      " <add>     eval_cache: Optional[PickleCache] = None,\n",
      " <add>     tqdm_args={},\n",
      " <add> ) -> tuple[ChunkedDataset, list[list[float]], dict]:\n",
      " <add>     def cached(name, f):\n",
      " <add>         if eval_cache is None:\n",
      " <add>             return f()\n",
      " <add>         return eval_cache.cached(name, f)\n",
      " <add> \n",
      " <add>     return cached(\n",
      " <add>         f\"critic_eval-{args}.pkl\",\n",
      " <add>         lambda: critic.eval_on_dataset(\n",
      " <add>             r1_srcs, args, sampling_max_tokens, tqdm_args=tqdm_args\n",
      " <add>         ),\n",
      " <add>     )\n",
      "\n",
      "===========spot.data/TokenizedSrc.prev_types===========\n",
      "<add> # spot.data/TokenizedSrc.prev_types\n",
      " <add>     prev_types: list[PythonType] | None = None\n",
      "===========spot.data/SrcChunkInfo.prev_types===========\n",
      "<add> # spot.data/SrcChunkInfo.prev_types\n",
      " <add>     prev_types: list[PythonType] | None\n",
      "===========spot.data/_TokenizedSrcHelper.dict_to_tokenized_src===========\n",
      "# spot.data/_TokenizedSrcHelper.dict_to_tokenized_src\n",
      "    def dict_to_tokenized_src(self, d: dict) -> TokenizedSrc:\n",
      "        r = TokenizedSrc(\n",
      "            file=d[\"file\"],\n",
      "            repo=d[\"repo\"],\n",
      "            origin_code=d[\"cst_code\"],\n",
      "            tokenized_code=list[int](),\n",
      "            types=list[PythonType](),\n",
      "            types_pos=list[int](),\n",
      "            types_str=list[str](),\n",
      "            types_info=list[AnnotInfo](),\n",
      "            types_tks=list[list[int]](),\n",
      " <add>             prev_types=d[\"prev_types\"],\n",
      "        )\n",
      "\n",
      "        match d:\n",
      "            case {\n",
      "                \"code_segs\": segs,\n",
      "                \"types\": types,\n",
      "                \"types_str\": types_str,\n",
      "                \"annots_info\": annots_info,\n",
      "                \"is_label\": is_label,\n",
      "            }:\n",
      "                assert len(segs) == len(types) + 1\n",
      "            case _:\n",
      "                raise ValueError(f\"Invalid dict with keys: {d.keys()}\")\n",
      "\n",
      "        tkn = self.tokenizer\n",
      "        bos_id = not_none(tkn.bos_token_id)\n",
      "        eos_id = not_none(tkn.eos_token_id)\n",
      "        mask_id = not_none(tkn.mask_token_id)\n",
      "        all_tks = r.tokenized_code\n",
      "        all_tks.append(bos_id)\n",
      "        for i in range(len(types)):\n",
      "            all_tks.extend(tkn.encode(segs[i], add_special_tokens=False))\n",
      "            if is_label is None or is_label[i]:\n",
      "                r.types_pos.append(len(all_tks))\n",
      "                r.types.append(types[i])\n",
      "                r.types_tks.append(tkn.encode(str(types[i]), add_special_tokens=False))\n",
      "                r.types_str.append(types_str[i])\n",
      "                r.types_info.append(annots_</s>\n",
      "===========spot.data/chunk_srcs_per_file (0)===========\n",
      "# spot.data/chunk_srcs_per_file\n",
      "def chunk_srcs_per_file(\n",
      "    repos_root: Path,\n",
      "    srcs: Sequence[TokenizedSrc],\n",
      "    ctx_args: \"CtxArgs\",\n",
      "    tokenizer: TokenizerSPOT,\n",
      "    tqdm_args: dict,\n",
      ") -> \"ChunkedDataset\":\n",
      "\n",
      "    chunks: dict[str, list] = {\n",
      "        \"input_ids\": [],\n",
      "        \"labels\": [],\n",
      "        \"n_labels\": [],\n",
      "        \"chunk_id\": [],\n",
      "    }\n",
      "    chunks_info: list[SrcChunkInfo] = []\n",
      "\n",
      "    special_tks = [tokenizer.additional_special_tokens_ids[99 - i] for i in range(100)]\n",
      "    bos_id, eos_id = not_none(tokenizer.bos_token_id), not_none(tokenizer.eos_token_id)\n",
      "\n",
      "    chunk_id = 0\n",
      "\n",
      "    def src_to_chunks(src: TokenizedSrc, src_id: int, window_start: int) -> None:\n",
      "        label_ids, (ctx_start, ctx_end) = _compute_ctx(src, window_start, ctx_args)\n",
      "        tks = src.tokenized_code[ctx_start:ctx_end]\n",
      "        label_tkns = [bos_id]\n",
      "        types = list[PythonType]()\n",
      "        types_info = list[AnnotInfo]()\n",
      " <add>         prev_types = list[PythonType]() if src.prev_types is not None else None\n",
      "        for i, l_id in enumerate(label_ids):\n",
      "            label_pos = src.types_pos[l_id] - ctx_start\n",
      "            tks[label_pos] = special_tks[i]\n",
      "            label_tkns.append(special_tks[i])\n",
      "            label_tkns.extend(src.types_tks[l_id])\n",
      "            types.append(src.types[l_id])\n",
      "            types_info.append(src.types_info[l_id])\n",
      " <add>             if prev_types is not None:\n",
      " <add>                 prev_types.append(not_none(src.prev_types)[l_</s>\n",
      "===========spot.data/chunk_srcs_per_file (1)===========\n",
      "# spot.data/chunk_srcs_per_file\n",
      "<s>append(not_none(src.prev_types)[l_id])\n",
      "        label_tkns.append(eos_id)\n",
      "\n",
      "        assert len(label_ids) > 0\n",
      "        assert len(tks) <= ctx_args.ctx_size\n",
      "        chunks[\"input_ids\"].append(tks)\n",
      "        chunks[\"labels\"].append(label_tkns)\n",
      "        chunks[\"n_labels\"].append(len(label_ids))\n",
      "        nonlocal chunk_id\n",
      "        chunks[\"chunk_id\"].append(chunk_id)\n",
      "        chunk_id += 1\n",
      "        meta = SrcChunkInfo(\n",
      "            types,\n",
      "            types_info,\n",
      "            src_ids=[src_id] * len(label_ids),\n",
      "            label_ids=label_ids,\n",
      " <add>             prev_types=prev_types,\n",
      "        )\n",
      "        chunks_info.append(meta)\n",
      "\n",
      "        if len(src.types) > label_ids[-1] + 1:\n",
      "            new_start = src.types_pos[label_ids[-1] + 1]\n",
      "            src_to_chunks(src, src_id, new_start)\n",
      "\n",
      "    for src_id, src in enumerate(tqdm(srcs, desc=\"chunk_srcs_per_file\", **tqdm_args)):\n",
      "        if len(src.types) == 0:\n",
      "            continue\n",
      "        window_start = src.types_pos[0]\n",
      "        src_to_chunks(src, src_id, window_start)\n",
      "\n",
      "    files = [(repos_root / s.file).resolve() for s in srcs]\n",
      "    return ChunkedDataset(\n",
      "        data=Dataset.from_dict(chunks),\n",
      "        chunks_info=chunks_info,\n",
      "        files=files,\n",
      "        file2src={f: s.origin_code for f, s in zip(files, srcs)},\n",
      "        file2repo={f: (repos_root / s.repo).resolve() for f, s in zip(files, srcs)},\n",
      "        tokenizer=tokenizer,\n",
      "    )\n",
      "\n",
      "===========spot.data/R1_srcs_from_preds===========\n",
      "# spot.data/R1_srcs_from_preds\n",
      "def R1_srcs_from_preds(\n",
      "    tokenizer: TokenizerSPOT,\n",
      "    r0_src: SrcDataset,\n",
      "    chunks_info: list[SrcChunkInfo],\n",
      "    src_files: list[Path],\n",
      "    r0_preds: list[list[PythonType]],\n",
      "    check_in_isolation: bool,\n",
      "    max_workers: int,\n",
      "    tqdm_args: dict = {},\n",
      ") -> SrcDataset:\n",
      "    file2preds = dict[Path, dict[AnnotPath, str]]()\n",
      " <del>     assert_eq(len(r0_preds), len(chunks_info))\n",
      "    for preds, chunk_info in zip(r0_preds, chunks_info):\n",
      "        assert_eq(len(preds), len(chunk_info.types))\n",
      "        for i, pred in enumerate(preds):\n",
      "            sid = chunk_info.src_ids[i]\n",
      "            file = src_files[sid]\n",
      "            if file not in file2preds:\n",
      "                file2preds[file] = dict()\n",
      "            label_path = chunk_info.annots_info[i].path\n",
      "            file2preds[file][label_path] = str(pred)\n",
      "\n",
      "    file2src = r0_src.file2src()\n",
      "    file2preds1 = dict[Path, dict[int, str]]()\n",
      "\n",
      "    for f, ls in file2preds.items():\n",
      "        src = file2src[f]\n",
      "        path2id = {info.path: i for i, info in enumerate(src.types_info)}\n",
      "        try:\n",
      "            file2preds1[f] = {path2id[path]: label for path, label in ls.items()}\n",
      "        except Exception as e:\n",
      "            raise RuntimeError(f\"In file {f}. path2id={path2id}\") from e\n",
      "\n",
      "    return r0_src.add_type_checker_feedback(\n",
      "        tokenizer,\n",
      "        file2preds1,\n",
      "        in_isolation=check_in_isolation,\n",
      "        max_workers=max_workers,\n",
      "        tqdm_args=tqdm_args,\n",
      "    )\n",
      "\n",
      "===========spot.data/SrcDataset.add_type_checker_feedback (0)===========\n",
      "# spot.data/SrcDataset.add_type_checker_feedback\n",
      "    def add_type_checker_feedback(\n",
      "        self,\n",
      "        tokenizer: TokenizerSPOT,\n",
      "        file2preds: dict[Path, dict[int, str]],\n",
      "        max_workers: int,\n",
      "        tqdm_args: dict,\n",
      "        in_isolation: bool,\n",
      "        include_all_errors: bool = False,\n",
      "        mypy_path: Optional[Path] = None,\n",
      "    ) -> \"SrcDataset\":\n",
      "\n",
      "        file2src = self.file2src()\n",
      "        src_list = [file2src[f.resolve()] for f in file2preds]\n",
      "        chunksize = max(1, len(src_list) // (8 * max_workers))\n",
      "\n",
      "        try:\n",
      "            if in_isolation:\n",
      "                check_rs: list[SrcCheckResult] = process_map(\n",
      "                    type_check_src,\n",
      "                    src_list,\n",
      "                    list(file2preds.values()),\n",
      "                    [mypy_path for _ in src_list],\n",
      "                    max_workers=max_workers,\n",
      "                    desc=\"map type_check_src\",\n",
      "                    chunksize=chunksize,\n",
      "                    **tqdm_args,\n",
      "                )\n",
      "            else:\n",
      "                check_rs = self.type_check_each_file_in_project(\n",
      "                    file2preds,\n",
      "                    max_workers,\n",
      "                    mypy_path,\n",
      "                    include_all_errors=include_all_errors,\n",
      "                    tqdm_args=tqdm_args,\n",
      "                )\n",
      "        finally:\n",
      "            MypyChecker.clear_temp_cache()\n",
      "        n_checked = 0\n",
      "        code_list = list[str]()\n",
      "        feedback_list = list[list[MypyFeedback]]()\n",
      " <del>         n_error_list = list[int]()\n",
      "        check_failure_reasons = list[str]()\n",
      "        for i in range(len(src_list)):\n",
      "            errors, new_code = check_rs[i]\n",
      "            if isinstance(errors, str):\n",
      "                check_failure_reasons.append(errors)\n",
      "                errors = []\n",
      "            else:\n",
      "                n_checked += 1\n",
      "            code_list.append(new_code</s>\n",
      "===========spot.data/SrcDataset.add_type_checker_feedback (1)===========\n",
      "# spot.data/SrcDataset.add_type_checker_feedback\n",
      "<s>checked += 1\n",
      "            code_list.append(new_code)\n",
      "            feedback_list.append(errors)\n",
      " <del>             n_error_list.append(len(errors))\n",
      "        result = SrcDataset(self.repos_root)\n",
      "        silent = tqdm_args.get(\"disable\", False)\n",
      "        result.add_stats(\n",
      "            {\n",
      "                \"type_check_success_ratio\": n_checked / len(src_list),\n",
      " <add>                 \"feedbacks_per_file\": scalar_stats([len(fs) for fs in feedback_list]),\n",
      " <del>                 \"feedbacks_per_file\": scalar_stats(n_error_list),\n",
      "            },\n",
      "            not silent,\n",
      "        )\n",
      "        result.add_stats(\n",
      " <add>             {\n",
      " <add>                 \"check_failure_reasons\": check_failure_reasons,\n",
      " <add>                 \"mypy_feedbacks\": feedback_list,\n",
      " <add>             },\n",
      " <add>             should_print=False,\n",
      " <del>             {\"check_failure_reasons\": check_failure_reasons}, should_print=False\n",
      "        )\n",
      "\n",
      "        helper = _TokenizedSrcHelper(tokenizer)\n",
      "        new_srcs = process_map(\n",
      "            helper.feedbacks_to_tokenized_src,\n",
      "            src_list,\n",
      "            code_list,\n",
      "            feedback_list,\n",
      "            max_workers=max_workers,\n",
      "            desc=\"feedbacks_to_tokenized_src\",\n",
      "            chunksize=chunksize,\n",
      "            **tqdm_args,\n",
      "        )\n",
      "        result.all_srcs = new_srcs\n",
      " <del>         result.add_stats({\"mypy_feedbacks\": feedback_list}, should_print=False)\n",
      "        return result\n",
      "\n",
      "===========spot.data/_TokenizedSrcHelper.feedbacks_to_tokenized_src (0)===========\n",
      "# spot.data/_TokenizedSrcHelper.feedbacks_to_tokenized_src\n",
      "    def feedbacks_to_tokenized_src(\n",
      "        self,\n",
      "        src: TokenizedSrc,\n",
      "        current_code: str,\n",
      "        feedbacks: list[MypyFeedback],\n",
      "    ) -> TokenizedSrc:\n",
      "        try:\n",
      "            m = cst.parse_module(current_code)\n",
      "        except Exception as e:\n",
      "            raise RuntimeError(\n",
      "                f\"Failed to parse file: '{src.file}' with content:\\n{current_code}\"\n",
      "            ) from e\n",
      "        m_code = m.code\n",
      "        assert (\n",
      "            m_code.rstrip() == current_code.rstrip()\n",
      "        ), f\"String diffferences: {show_string_diff(current_code, m_code)}\"\n",
      "        current_annots, _ = collect_user_annotations(m)\n",
      "        preds_map = dict[CodeRange, str]()\n",
      " <add>         prev_types = list[PythonType]()\n",
      "        types = list[PythonType]()\n",
      "        types_str = list[str]()\n",
      "        annots_info = list[AnnotInfo]()\n",
      "        path2label_id = {info.path: i for i, info in enumerate(src.types_info)}\n",
      "\n",
      "        for a in current_annots:\n",
      "            if a.path in path2label_id:\n",
      "                assert (r := a.annot_range) is not None\n",
      "                assert (annot := a.annot) is not None\n",
      " <add>                 prev_type = preds_map[r] = m.code_for_node(annot.annotation)\n",
      " <add>                 prev_types.append(parse_type_str(prev_type))\n",
      " <del>                 preds_map[r] = m.code_for_node(annot.annotation)\n",
      "                li = path2label_id[a.path]\n",
      "                types.append(src.types[li])\n",
      "                types_str.append(src.types_str[li])\n",
      "                annots_info.append(a)\n",
      "        pos_to_msg = {f.position: f.message for f in feedbacks}\n",
      "        new_code = patch_code_with_extra(current_code, preds_map, pos_to_msg</s>\n",
      "===========spot.data/_TokenizedSrcHelper.feedbacks_to_tokenized_src (1)===========\n",
      "# spot.data/_TokenizedSrcHelper.feedbacks_to_tokenized_src\n",
      "<s>(current_code, preds_map, pos_to_msg)\n",
      "        code_segs = new_code.split(SpecialNames.TypeMask)\n",
      "        assert len(code_segs) == len(types) + 1, f\"{len(code_segs)}!= {len(types)} + 1\"\n",
      "\n",
      "        d = {\n",
      "            \"file\": src.file,\n",
      "            \"repo\": src.repo,\n",
      "            \"cst_code\": new_code,\n",
      "            \"code_segs\": code_segs,\n",
      "            \"types\": types,\n",
      "            \"types_str\": types_str,\n",
      "            \"annots_info\": annots_info,\n",
      " <add>             \"prev_types\": prev_types,\n",
      "            \"is_label\": None,\n",
      "        }\n",
      "        return self.dict_to_tokenized_src(d)\n",
      "\n",
      "===========spot.data/SrcDataset.from_repos===========\n",
      "# spot.data/SrcDataset.from_repos\n",
      "<s> rands = random.getstate()\n",
      "        random.seed(seed)\n",
      "        for i, x in enumerate(masked_srcs):\n",
      "            if x is None:\n",
      "                continue\n",
      "            n = len(x[\"types\"])\n",
      "            x[\"is_label\"] = [random.random() < label_ratio for _ in range(n)]\n",
      "            x[\"file\"] = srcs_list[i][0].relative_to(repos_root)\n",
      "            x[\"repo\"] = srcs_list[i][1][1].relative_to(repos_root)\n",
      " <add>             x[\"prev_types\"] = None\n",
      "            filtered_srcs.append(x)\n",
      "        random.setstate(rands)\n",
      "\n",
      "        helper = _TokenizedSrcHelper(tokenizer)\n",
      "        tk_srcs: list[TokenizedSrc] = process_map(\n",
      "            helper.dict_to_tokenized_src,\n",
      "            filtered_srcs,\n",
      "            max_workers=max_workers,\n",
      "            desc=\"dict_to_tokenized_src\",\n",
      "            chunksize=max(1, len(filtered_srcs) // (8 * max_workers)),\n",
      "            **tqdm_args,\n",
      "        )\n",
      "\n",
      "        for f, g in groupby(tk_srcs, lambda s: s.file).items():\n",
      "            assert len(g) == 1, f\"{f} appears {len(g)} times.\"\n",
      "\n",
      "        result.all_srcs = tk_srcs\n",
      "        return result\n",
      "\n",
      "===========spot.data/TokenizedSrc.truncate===========\n",
      "# spot.data/TokenizedSrc.truncate\n",
      "    def truncate(self, max_tkns: int) -> \"TokenizedSrc\":\n",
      "        n_types = 0\n",
      "        for p in self.types_pos:\n",
      "            if p < max_tkns:\n",
      "                n_types += 1\n",
      "        result = copy(self)\n",
      "        result.tokenized_code = self.tokenized_code[:max_tkns]\n",
      "        result.types = self.types[:n_types]\n",
      "        result.types_pos = self.types_pos[:n_types]\n",
      "        result.types_str = self.types_str[:n_types]\n",
      "        result.types_tks = self.types_tks[:n_types]\n",
      "        result.types_info = self.types_info[:n_types]\n",
      " <add>         if self.prev_types is not None:\n",
      " <add>             result.prev_types = self.prev_types[:n_types]\n",
      "        return result\n",
      "\n",
      "===========spot.model/encode_model_outputs===========\n",
      "<add> # spot.model/encode_model_outputs\n",
      " <add> def encode_model_outputs(\n",
      " <add>     types: list[PythonType], tokenizer: TokenizerSPOT\n",
      " <add> ) -> list[int]:\n",
      " <add>     out = list[int]()\n",
      " <add>     for i, t in enumerate(types):\n",
      " <add>         extra_id = tokenizer.additional_special_tokens_ids[99 - i]\n",
      " <add>         out.append(extra_id)\n",
      " <add>         out.extend(tokenizer.encode(str(t)))\n",
      " <add>     return out\n",
      "\n",
      "===========spot.train/TrainModelWrapper.saving_counter===========\n",
      "<add> # spot.train/TrainModelWrapper.saving_counter\n",
      " <add>     saving_counter:...\n",
      "===========spot.train/_configure_optimizers===========\n",
      "<add> # spot.train/_configure_optimizers\n",
      " <add> def _configure_optimizers(model: nn.Module):\n",
      " <add>     no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
      " <add>     grouped_params = [\n",
      " <add>         {\n",
      " <add>             \"params\": [\n",
      " <add>                 p\n",
      " <add>                 for pn, p in model.named_parameters()\n",
      " <add>                 if not any(n in pn for n in no_decay)\n",
      " <add>             ],\n",
      " <add>             \"weight_decay\": 0.01,\n",
      " <add>         },\n",
      " <add>         {\n",
      " <add>             \"params\": [\n",
      " <add>                 p\n",
      " <add>                 for pn, p in model.named_parameters()\n",
      " <add>                 if any(n in pn for n in no_decay)\n",
      " <add>             ],\n",
      " <add>             \"weight_decay\": 0.0,\n",
      " <add>         },\n",
      " <add>     ]\n",
      " <add>     optimizer = AdamW(grouped_params, lr=2e-5)\n",
      " <add>     lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.2)\n",
      " <add>     return [optimizer], [lr_scheduler]\n",
      "\n",
      "===========spot.train/TrainModelWrapper.saving_coutner===========\n",
      "<del> # spot.train/TrainModelWrapper.saving_coutner\n",
      " <del>     saving_coutner:...\n",
      "===========spot.train/R1_srcs_from_ckpts (0)===========\n",
      "# spot.train/R1_srcs_from_ckpts\n",
      "def R1_srcs_from_ckpts(\n",
      "    tokenizer: TokenizerSPOT,\n",
      "    dec_args: DecodingArgs,\n",
      "    r0_src: SrcDataset,\n",
      "    cdata: ChunkedDataset,\n",
      "    chunk_ids: list[list[int]],\n",
      "    check_in_isolation: bool,\n",
      "    ckpt_dir: Path,\n",
      "    ckpt_interval: int,\n",
      "    max_workers: int,\n",
      "    device,\n",
      "    tqdm_args={\"leave\": False},\n",
      " <add> ):\n",
      " <add>     if (n_got := sum(len(x) for x in chunk_ids))!= len(cdata.chunks_info):\n",
      " <add>         logging.warning(\n",
      " <add>             f\"Some chunks are missing. Got {n_got} chunks, but expected {len(cdata.chunks_info)}\"\n",
      " <add>         )\n",
      " <del> ) -> SrcDataset:\n",
      "    chunks_info = list[SrcChunkInfo]()\n",
      "    model_preds = list[list[PythonType]]()\n",
      "    for i in tqdm(\n",
      " <add>         range(0, len(chunk_ids), ckpt_interval),\n",
      " <del>         range(ckpt_interval, len(chunk_ids), ckpt_interval),\n",
      "        desc=\"R1_srcs_from_ckpts\",\n",
      "        **tqdm_args,\n",
      "    ):\n",
      "        ids = list(seq_flatten(chunk_ids[i : i + ckpt_interval]))\n",
      "        model = load_model_spot(ckpt_dir / f\"n_batches={i}\")\n",
      "        wrapper = ModelWrapper(model, tokenizer, dec_args)\n",
      "        wrapper = wrapper.to(device)\n",
      "        try:\n",
      "            data_sub = cdata[ids]\n",
      "        except IndexError as e:\n",
      "            raise IndexError(\n",
      "                f\"ids: {ids},\\nchunk_ids: {cdata.data['chunk_id']}\\ncdata: {cdata}\"\n",
      "            ) from e\n",
      "        chunks_info.extend(data_sub.chunks_info)\n",
      "        preds = wrapper.predict(data_sub.data, tqdm_args=tqdm_args)\n",
      "        model_preds.extend(preds)\n",
      " <add>     srcs = R1_src</s>\n",
      "===========spot.train/R1_srcs_from_ckpts (1)===========\n",
      "# spot.train/R1_srcs_from_ckpts\n",
      "<s>extend(preds)\n",
      " <add>     srcs = R1_srcs_from_preds(\n",
      " <del>     return R1_srcs_from_preds(\n",
      "        tokenizer,\n",
      "        r0_src,\n",
      "        chunks_info,\n",
      "        cdata.files,\n",
      "        model_preds,\n",
      "        check_in_isolation=check_in_isolation,\n",
      "        max_workers=max_workers,\n",
      "        tqdm_args=tqdm_args,\n",
      "    )\n",
      " <add>     return srcs, chunks_info, model_preds\n",
      "\n",
      "===========spot.train/TrainModelWrapper.on_fit_start===========\n",
      "# spot.train/TrainModelWrapper.on_fit_start\n",
      "    def on_fit_start(self):\n",
      "        if self.model_saving_interval is not None:\n",
      "            self.batch_ids: list[list[int]] = []\n",
      " <add>             self.saving_counter = 0\n",
      " <add>             self.model.save_pretrained(self.model_saving_path / f\"n_batches=0\")\n",
      " <del>             self.saving_coutner = 0\n",
      "\n",
      "===========spot.train/train_spot_model (0)===========\n",
      "# spot.train/train_spot_model\n",
      "def train_spot_model(\n",
      "    src_datasets: dict[str, SrcDataset],\n",
      "    model_name: str,\n",
      "    train_args: ModelTrainingArgs,\n",
      "    record_batches: bool,\n",
      "    gpus: list[int],\n",
      "    quicktest=False,\n",
      "    use_early_stop=True,\n",
      "    use_small_model=False,\n",
      " <add> ) -> tuple[ModelWrapper, dict]:\n",
      " <del> ) -> Tuple[ModelWrapper, dict]:\n",
      "    os.chdir(proj_root())\n",
      "    train_ctx_args = train_args.train_ctx_args\n",
      "    dec_args = train_args.dec_args\n",
      "\n",
      "    datadir = Path(os.getenv(\"datadir\", \"data\"))\n",
      "\n",
      "    running_dir = datadir / \"checkpoints/lit-running\" / model_name\n",
      "    if running_dir.exists():\n",
      "        shutil.rmtree(running_dir)\n",
      "    running_dir.mkdir(parents=True, exist_ok=True)\n",
      "\n",
      "    model_path = (\n",
      "        \"Salesforce/codet5-small\" if use_small_model else \"Salesforce/codet5-base\"\n",
      "    )\n",
      "    lit_model = TrainModelWrapper(model_path, model_saving_path=running_dir / \"models\")\n",
      "    tokenizer: TokenizerSPOT = lit_model.tokenizer\n",
      "    wrapper = ModelWrapper(lit_model.model, tokenizer, dec_args)\n",
      "\n",
      "    chunks: dict[str, ChunkedDataset] = {}\n",
      "    with run_long_task(\"Preparing chunked datasets\", notify=False):\n",
      "        for n in [\"valid\", \"train\"]:\n",
      "            src = src_datasets[n]\n",
      "            chunks[n] = src.to_chunks(tokenizer, train_ctx_args)\n",
      "\n",
      " <add>     wandb_logger = WandbLogger()\n",
      " <del>     wandb_logger = WandbLogger(\n",
      " <del>         project=\"SPOT\",\n",
      " <del>         name=model_name,\n",
      " <del>         save_dir=str(datadir),\n",
      " <del>     )\n",
      "\n",
      "    collate_fn = DataCollatorForSeq2Seq(lit_model.tokenizer, lit_model.model)\n",
      "   </s>\n",
      "===========spot.train/train_spot_model (1)===========\n",
      "# spot.train/train_spot_model\n",
      "<s>_model.tokenizer, lit_model.model)\n",
      "    train_dataloader = dynamic_dataloader(\n",
      "        cast(Any, chunks[\"train\"].data),\n",
      "        max_tokens=train_args.train_max_tokens,\n",
      "        collate_fn=collate_fn,\n",
      "        shuffle=True,\n",
      "    )\n",
      "    valid_dataloader = dynamic_dataloader(\n",
      "        cast(Any, chunks[\"valid\"].data),\n",
      "        max_tokens=train_args.eval_max_tokens,\n",
      "        collate_fn=collate_fn,\n",
      "        shuffle=True,\n",
      "    )\n",
      "\n",
      " <add>     ckpt_interval = max(1, len(train_dataloader) // 10)\n",
      " <del>     ckpt_interval = max(1, len(train_dataloader) // 8)\n",
      "    val_interval = 1 if quicktest else max(500, ckpt_interval)\n",
      "\n",
      "    if record_batches:\n",
      "        lit_model.model_saving_interval = ckpt_interval\n",
      "\n",
      "    checkpoint_cb = ModelCheckpoint(\n",
      "        dirpath=running_dir,\n",
      "        save_top_k=3,\n",
      "        monitor=\"valid/loss\",\n",
      "        mode=\"min\",\n",
      "        save_on_train_epoch_end=False,\n",
      "        verbose=quicktest,\n",
      "    )\n",
      "\n",
      "    trainer = pl.Trainer(\n",
      "        default_root_dir=str(running_dir),\n",
      "        accelerator=\"gpu\" if gpus else \"cpu\",\n",
      "        gpus=gpus,\n",
      "        precision=16,\n",
      "        max_epochs=train_args.max_epochs,\n",
      "        logger=wandb_logger,\n",
      "        val_check_interval=val_interval,\n",
      "        callbacks=(\n",
      "            [checkpoint_cb, EarlyStopping(\"valid/loss\", mode=\"min\", verbose=quicktest)]\n",
      "            if use_early_stop\n",
      "            else []\n",
      "        ),\n",
      "        gradient_clip_val=1.0,\n",
      "        gradient_clip_algorithm=\"norm\",\n",
      "        accumulate_grad_batches=train_args.accumulate_grad_batches,\n",
      "    )\n",
      "\n",
      "    warnings.filterwarnings(\"ignore\", \"The dataloader.*does not</s>\n",
      "===========spot.train/R1_srcs_from_extra (0)===========\n",
      "# spot.train/R1_srcs_from_extra\n",
      "def R1_srcs_from_extra(\n",
      "    wrapper: ModelWrapper,\n",
      "    src_datasets: dict[str, SrcDataset],\n",
      "    extra: dict[str, Any],\n",
      " <add>     ckpt_dir: Optional[Path] = None,\n",
      " <add>     ckpt_interval: Optional[int] = None,\n",
      " <del>     ckpt_dir: Path,\n",
      " <del>     ckpt_interval: int,\n",
      "):\n",
      "    tokenizer = wrapper.tokenizer\n",
      "    batch_ids = extra[\"batch_ids\"]\n",
      "    check_in_isolation = extra[\"check_in_isolation\"]\n",
      "    print(f\"Generating R1 dataset: train\")\n",
      "\n",
      "    chunk_datasets = dict[str, ChunkedDataset]()\n",
      "    for n in [\"test\", \"valid\", \"train\"]:\n",
      "        src = src_datasets[n]\n",
      "        chunk_datasets[n] = src.to_chunks(tokenizer, wrapper.args.ctx_args)\n",
      "\n",
      "    R1_src_datasets = dict[str, SrcDataset]()\n",
      " <add>     if ckpt_dir is None or ckpt_interval is None:\n",
      " <add>         chunks_info = extra[\"chunks_info\"]\n",
      " <add>         model_preds = extra[\"model_preds\"]\n",
      " <add>         R1_src_datasets[\"train\"] = R1_srcs_from_preds(\n",
      " <add>             tokenizer,\n",
      " <add>             src_datasets[\"train\"],\n",
      " <add>             chunks_info,\n",
      " <add>             chunk_datasets[\"train\"].files,\n",
      " <add>             model_preds,\n",
      " <add>             check_in_isolation=check_in_isolation,\n",
      " <add>             max_workers=wrapper.args.max_workers,\n",
      " <add>         )\n",
      " <add>     else:\n",
      " <add>         R1_src_datasets[\"train\"], chunks_info, model_preds = R1_srcs_from_ckpts(\n",
      " <add>             tokenizer,\n",
      " <add>             wrapper.args,\n",
      " <add>             src_datasets[\"train\"],\n",
      " <add>             chunk_datasets[\"train\"],\n",
      " <add>             batch_ids,\n",
      " <add>             check_in_isolation=check_in_isolation,\n",
      " <add>             ckpt_dir=ckpt_dir,\n",
      " <add>             ckpt_interval=ckpt_interval</s>\n",
      "===========spot.train/R1_srcs_from_extra (1)===========\n",
      "# spot.train/R1_srcs_from_extra\n",
      "<s>dir,\n",
      " <add>             ckpt_interval=ckpt_interval,\n",
      " <add>             max_workers=wrapper.args.max_workers,\n",
      " <add>             device=wrapper.model.device,\n",
      " <add>         )\n",
      " <add>         extra[\"chunks_info\"] = chunks_info\n",
      " <add>         extra[\"model_preds\"] = model_preds\n",
      " <del>     R1_src_datasets[\"train\"] = R1_srcs_from_ckpts(\n",
      " <del>         tokenizer,\n",
      " <del>         wrapper.args,\n",
      " <del>         src_datasets[\"train\"],\n",
      " <del>         chunk_datasets[\"train\"],\n",
      " <del>         batch_ids,\n",
      " <del>         check_in_isolation=check_in_isolation,\n",
      " <del>         ckpt_dir=ckpt_dir,\n",
      " <del>         ckpt_interval=ckpt_interval,\n",
      " <del>         max_workers=wrapper.args.max_workers,\n",
      " <del>         device=wrapper.model.device,\n",
      " <del>     )\n",
      "    for n in [\"valid\", \"test\"]:\n",
      "        print(f\"Generating R1 dataset: {n}\")\n",
      "        preds = wrapper.predict(chunk_datasets[n].data, {})\n",
      "        R1_src_datasets[n] = R1_srcs_from_preds(\n",
      "            tokenizer,\n",
      "            src_datasets[n],\n",
      "            chunk_datasets[n].chunks_info,\n",
      "            chunk_datasets[n].files,\n",
      "            preds,\n",
      "            check_in_isolation=check_in_isolation,\n",
      "            max_workers=wrapper.args.max_workers,\n",
      "        )\n",
      "    return R1_src_datasets\n",
      "\n",
      "===========spot.train/TrainModelWrapper.configure_optimizers===========\n",
      "# spot.train/TrainModelWrapper.configure_optimizers\n",
      "    def configure_optimizers(self):\n",
      " <add>         return _configure_optimizers(self.model)\n",
      " <del>         no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
      " <del>         grouped_params = [\n",
      " <del>             {\n",
      " <del>                 \"params\": [\n",
      " <del>                     p\n",
      " <del>                     for pn, p in self.model.named_parameters()\n",
      " <del>                     if not any(n in pn for n in no_decay)\n",
      " <del>                 ],\n",
      " <del>                 \"weight_decay\": 0.01,\n",
      " <del>             },\n",
      " <del>             {\n",
      " <del>                 \"params\": [\n",
      " <del>                     p\n",
      " <del>                     for pn, p in self.model.named_parameters()\n",
      " <del>                     if any(n in pn for n in no_decay)\n",
      " <del>                 ],\n",
      " <del>                 \"weight_decay\": 0.0,\n",
      " <del>             },\n",
      " <del>         ]\n",
      " <del>         optimizer = AdamW(grouped_params, lr=2e-5)\n",
      " <del>         lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.2)\n",
      " <del>         return [optimizer], [lr_scheduler]\n",
      "\n",
      "===========spot.train/TrainModelWrapper.training_step===========\n",
      "# spot.train/TrainModelWrapper.training_step\n",
      "    def training_step(self, batch, batch_idx):\n",
      "        if self.model_saving_interval is not None and self.current_epoch == 0:\n",
      "            self.batch_ids.append(batch[\"chunk_id\"].tolist())\n",
      " <add>             self.saving_counter += 1\n",
      " <add>             if self.saving_counter >= self.model_saving_interval:\n",
      " <add>                 self.saving_counter = 0\n",
      " <del>             self.saving_coutner += 1\n",
      " <del>             if self.saving_coutner >= self.model_saving_interval:\n",
      " <del>                 self.saving_coutner = 0\n",
      "                self.model.save_pretrained(\n",
      "                    self.model_saving_path / f\"n_batches={len(self.batch_ids)}\"\n",
      "                )\n",
      "\n",
      "        outputs = self.model(\n",
      "            input_ids=batch[\"input_ids\"],\n",
      "            labels=batch[\"labels\"],\n",
      "        )\n",
      "        loss = outputs.loss\n",
      "        self.log(\"train/loss\", loss.item())\n",
      "        self.log(\"train/lr\", self.lr_schedulers().get_last_lr()[0])\n",
      "        return loss\n",
      "\n",
      "===========spot.type_check/MypyChecker.Preamble===========\n",
      "# spot.type_check/MypyChecker.Preamble\n",
      " <add>     Preamble = \"from typing import Any, List, Tuple, Dict, Set # SPOT\\n\"\n",
      " <del>     Preamble = \"from typing import Any # SPOT\\n\"\n",
      "===========spot.utils/as_any===========\n",
      "<add> # spot.utils/as_any\n",
      " <add> def as_any(x) -> Any:\n",
      " <add>     return x\n",
      "\n",
      "===========spot.utils/get_spot_tokenizer===========\n",
      "<add> # new: spot.utils/load_tokenizer_spot\n",
      " <del> # old: spot.utils/get_spot_tokenizer\n",
      " <add> def load_tokenizer_spot() -> TokenizerSPOT:...\n",
      " <del> def get_spot_tokenizer() -> TokenizerSPOT:...\n"
     ]
    }
   ],
   "source": [
    "rus = [e for e in data.all_edits() if e.is_rename_update]\n",
    "print(\"renamed updates:\", len(rus))\n",
    "print(rus[-1].show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting repo signatures:   8%|▊         | 28/360 [00:03<00:41,  7.98it/s]warning: inexact rename detection was skipped due to too many files.\n",
      "warning: you may want to set your diff.renameLimit variable to at least 2368 and retry the command.\n",
      "Getting repo signatures: 100%|██████████| 360/360 [02:02<00:00,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 repos have the same signature: ('Update README.md', 'Update README.md', 'Update README.md', 'Update README.md', 'Update README.md') ...\n",
      "   train/Embedding?Chinese-Word-Vectors\n",
      "   valid/KingOfBugbounty?KingOfBugBountyTips\n",
      "Total number of duplicates: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# detect duplicate repos using commit messages\n",
    "dataset_name = \"large\"\n",
    "repos_dir = get_dataset_dir(dataset_name) / \"repos\"\n",
    "repo_paths = [repo for split in repos_dir.iterdir() for repo in split.iterdir()]\n",
    "reposigs = pmap(get_repo_signature, repo_paths, desc=\"Getting repo signatures\")\n",
    "repo2sig = dict(zip(repo_paths, reposigs))\n",
    "groups = groupby(repo_paths, lambda repo: repo2sig[repo])\n",
    "n_duplicates = 0\n",
    "for sig, repos in groups.items():\n",
    "    if len(repos) > 1:\n",
    "        n_duplicates += len(repos) - 1\n",
    "        print(f\"{len(repos)} repos have the same signature: {sig[:5]} ...\")\n",
    "        for repo in repos:\n",
    "            print(\"  \", repo.relative_to(repos_dir))\n",
    "print(\"Total number of duplicates:\", n_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_datasets(get_dataset_dir(\"small\") / \"BasicQueryEditEncoder(VERSION=2)\", splits=[\"test\"])[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_projects: 2\n",
      "n_edits: 2039\n",
      "n_additions: 0\n",
      "input_tks:\n",
      "   mean: 230.32\n",
      "   median: 182\n",
      "   min: 28\n",
      "   max: 512\n",
      "output_tks:\n",
      "   mean: 57.2\n",
      "   median: 43\n",
      "   min: 4\n",
      "   max: 256\n",
      "references:\n",
      "   mean: 75.127\n",
      "   median: 8\n",
      "   min: 1\n",
      "   max: 877\n",
      "ref_size_sum:\n",
      "   mean: 17824\n",
      "   median: 1345\n",
      "   min: 43\n",
      "   max: 467294\n"
     ]
    }
   ],
   "source": [
    "pretty_print_dict(data.overall_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edits with updated calls: 23 / 126\n",
      "Number of updated calls: 30\n"
     ]
    }
   ],
   "source": [
    "call_exs = [ex for ex in data.all_edits() if ex.updated_calls]\n",
    "print(f\"Edits with updated calls: {len(call_exs)} / {len(data.all_edits())}\")\n",
    "print(f\"Number of updated calls: {sum(len(ex.updated_calls) for ex in call_exs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex=call_exs[1]\n",
    "print(f\"updated calls: {[p for p, _ in ex.updated_calls]}\")\n",
    "print(ex.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_name = \"SPOT\"\n",
    "dataset_name = \"medium\"\n",
    "# encoder = AnalysisBasedEditEncoder(extra_ctx_names=(\"usees\", \"post-usees\"))\n",
    "encoder = CstBasedEditEncoder()\n",
    "save_dir = get_dataset_dir(dataset_name) / repr_modified_args(encoder)\n",
    "datasets = load_datasets(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== test ====================\n",
      "n_projects: 20\n",
      "n_edits: 12666\n",
      "input_size:\n",
      "   mean: 2759.8\n",
      "   median: 3137.5\n",
      "   min: 44\n",
      "   max: 4006\n",
      "output_size:\n",
      "   mean: 113.73\n",
      "   median: 75\n",
      "   min: 3\n",
      "   max: 4776\n",
      "==================== valid ====================\n",
      "n_projects: 10\n",
      "n_edits: 2297\n",
      "input_size:\n",
      "   mean: 2902.8\n",
      "   median: 3629\n",
      "   min: 58\n",
      "   max: 4006\n",
      "output_size:\n",
      "   mean: 116.93\n",
      "   median: 89\n",
      "   min: 5\n",
      "   max: 1567\n",
      "==================== train ====================\n",
      "n_projects: 100\n",
      "n_edits: 50801\n",
      "input_size:\n",
      "   mean: 2579.5\n",
      "   median: 2720\n",
      "   min: 37\n",
      "   max: 4006\n",
      "output_size:\n",
      "   mean: 98.522\n",
      "   median: 64\n",
      "   min: 3\n",
      "   max: 8434\n"
     ]
    }
   ],
   "source": [
    "for group, dataset in datasets.items():\n",
    "    print(\"=\" * 20, group, \"=\" * 20)\n",
    "    pretty_print_dict(dataset.overall_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All edits: 1331\n",
      "All edits with extra context: 360\n"
     ]
    }
   ],
   "source": [
    "with_ctx = [e for e in data.all_edits() if e.extra_tks]\n",
    "\n",
    "print(\"All edits:\", len(data.all_edits()))\n",
    "print(\"All edits with extra context:\", len(with_ctx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coeditor.model import DatasetDecodingResult\n",
    "\n",
    "model_name = \"coeditor-medium-analysis-post_usees\"\n",
    "dec_result: DatasetDecodingResult = pickle_load(get_model_dir() / model_name / \"dec_result.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "saving examples: 100%|██████████| 200/200 [00:03<00:00, 57.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output examples saved to: /mnt/nas/jiayi/coeditor/models/models/trained/coeditor-medium-analysis-post_usees/pred_samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "max_saved_samples = 200\n",
    "random.seed(42)\n",
    "exs_to_save = list(range(len(dec_result.predictions)))\n",
    "random.shuffle(exs_to_save)\n",
    "exs_to_save = exs_to_save[:max_saved_samples]\n",
    "out_dir = get_model_dir() / model_name / \"pred_samples\"\n",
    "dec_result.save_examples_to_dir(out_dir, exs_to_save)\n",
    "print(\"Output examples saved to:\", out_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.venv': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0 | packaged by conda-forge | (main, Oct 25 2022, 06:24:40) [GCC 10.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6ffc72953da4dd16b2e00785be9c4013ef131f465a8658f3921b6634d4eeec8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
