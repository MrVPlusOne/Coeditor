{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from coeditor.common import *\n",
    "from coeditor.dataset import *\n",
    "from coeditor.encoding import *\n",
    "from spot.utils import pretty_print_dict\n",
    "\n",
    "os.chdir(proj_root())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting commit histories: 100%|██████████| 1/1 [00:01<00:00,  1.56s/repo]\n",
      "Create tokenized edits: 100%|██████████| 1/1 [00:16<00:00, 16.15s/chunk]\n"
     ]
    }
   ],
   "source": [
    "# encoder = AnalysisBasedEditEncoder(extra_ctx_names=(\"usees\", \"post-usees\"))\n",
    "encoder = FileBasedEditEncoder()\n",
    "data = dataset_from_projects([proj_root()], encoder, max_history_per_repo=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========Ground Truth========\n",
      " <0>:<add> def collect_annotations(\n",
      "     <add>     code: cst.CSTNode,\n",
      "     <add> ) -> Tuple[list[AnnotPath], dict[AnnotPath, cst.Annotation]]:\n",
      "     <del> def collect_annotations(code: cst.CSTNode) -> Dict[AnnotPath, Optional[cst.Annotation]]:\n",
      "\n",
      " <3>:<add>     return collector.annot_paths, collector.annotations\n",
      "     <del>     return collector.annotations\n",
      "\n",
      "========Main Code========\n",
      "<extra_id_0>def collect_annotations(code: cst.CSTNode) -> Dict[AnnotPath, Optional[cst.Annotation]]:\n",
      "<extra_id_1>    collector = AnnotCollector()\n",
      "<extra_id_2>    code.visit(collector)\n",
      "<extra_id_3>    return collector.annotations\n",
      "<extra_id_4>\n",
      "\n",
      "===========left context===========\n",
      "from dataclasses import dataclass\n",
      "from distutils.log import error\n",
      " <add> from logging import warn\n",
      "from posixpath import dirname, realpath\n",
      "import re\n",
      "from typing import Iterable\n",
      "from.utils import *\n",
      "import subprocess\n",
      "\n",
      "\n",
      " <add> @dataclass(order=True, unsafe_hash=True)\n",
      " <del> @dataclass(frozen=True)\n",
      "class AnnotPath:\n",
      "\n",
      "    value: Tuple[str,...]\n",
      "\n",
      " <add>     def __repr__(self):\n",
      " <add>         return f\"AnnotPath('{'.'.join(self.value)}')\"\n",
      " <add> \n",
      " <add>     def __str__(self):\n",
      " <add>         return f\"'{'.'.join(self.value)}'\"\n",
      " <add> \n",
      " <add> \n",
      "def annot_path(*segs: str) -> AnnotPath:\n",
      "    return AnnotPath(tuple(segs))\n",
      "\n",
      " <add> \n",
      "\n",
      "===========right context===========\n",
      "\n",
      "\n",
      " <add> def apply_annotations(code: cst.CSTNode, annots: dict[AnnotPath, cst.Annotation]):\n",
      " <del> def apply_annotations(code: cst.CSTNode, annots: Dict[AnnotPath, cst.Annotation]):\n",
      "    applier = AnnotApplier(annots)\n",
      "    return code.visit(applier)\n",
      "\n",
      "\n",
      "class AnnotCollector(cst.CSTVisitor):\n",
      "    def __init__(self):\n",
      "        self.stack: List[str] = []\n",
      " <add>         self.annot_paths: List[AnnotPath] = []\n",
      " <add>         self.annotations: Dict[AnnotPath, cst.Annotation] = {}\n",
      " <del>         self.annotations: Dict[AnnotPath, Optional[cst.Annotation]] = {}\n",
      "\n",
      "    def on_visit(self, node):\n",
      "        if (\n",
      "            isinstance(node, cst.FunctionDef)\n",
      "            or isinstance(node, cst.ClassDef)\n",
      "            or isinstance(node, cst.Param)\n",
      "        ):\n",
      "            self.stack.append(node.name.value)\n",
      "        elif isinstance(node, cst.AnnAssign):\n",
      "            self.stack.append(node.target.value)\n",
      "        return super().on_visit(node)\n",
      "\n",
      "    def on_leave(self, node):\n",
      "        r = super().on_leave(node)\n",
      "        if (\n",
      "            isinstance(node, cst.FunctionDef)\n",
      "            or isinstance(node, cst.ClassDef)\n",
      "            or isinstance(node, cst.Param)\n",
      "            or isinstance(node, cst.AnnAssign)\n",
      "        ):\n",
      "            self.stack.pop()\n",
      "        return r\n",
      "\n",
      "    def _current_path(self):\n",
      "        return AnnotPath(tuple(self.stack))\n",
      "\n",
      " <add>     def _record_annot(self, annot: Union[cst.Annotation, None]):\n",
      " <add>         path = self._current_path()\n",
      " <add>         self.annot_paths.append(path)\n",
      " <add>         if annot is not None:\n",
      " <add>             self.annotations[path] = annot\n",
      " <add> \n",
      "    def visit_FunctionDef(self, node: cst.FunctionDef):\n",
      "        self.stack.append(SpecialNames.Return)\n",
      " <add>         self._record_annot(node.returns)\n",
      " <del>         self.annotations[self._current_path()] = node.returns\n",
      "        self.stack.pop()\n",
      "\n",
      "    def visit_Param(self, node: cst.Param):\n",
      " <add>         self._record_annot(node.annotation)\n",
      " <del>         self.annotations[self._current_path()] = node.annotation\n",
      "\n",
      " <add>     def visit_AnnAssign(self, node: cst.AnnAssign):\n",
      " <add>         self._record_annot(node.annotation)\n",
      " <del>     def visit_AnnAssign(self, ndoe: cst.AnnAssign):\n",
      " <del>         self.annotations[self._current_path()] = ndoe.annotation\n",
      "\n",
      "\n",
      "class AnnotApplier(cst.CSTTransformer):\n",
      "    def __init__(self, annots: Dict[AnnotPath, cst.Annotation]):\n",
      "        self.annots = annots\n",
      "        self.stack: List[str] = []\n",
      "        self.prefixes: Set[Tuple[str,...]] = set()\n",
      "        for path in annots.keys():\n",
      "            self.prefixes.update(path.value[0:i] for i in range(len(path.value) + 1))\n",
      "\n",
      "    def _current_path(self):\n",
      "        return AnnotPath(tuple(self.stack))\n",
      "\n",
      "    def on_visit(self, node):\n",
      "        if (\n",
      "            isinstance(node, cst.FunctionDef)\n",
      "            or isinstance(node, cst.ClassDef)\n",
      "            or isinstance(node, cst.Param)\n",
      "        ):\n",
      "            self.stack.append(node.name.value)\n",
      "        elif isinstance(node, cst.AnnAssign):\n",
      "            self.stack.append(node.target.value)\n",
      "        if tuple(self.stack) not in self.prefixes:\n",
      "            return False\n",
      "        return super().on_visit(node)\n",
      "\n",
      "    def on_leave(self, node, updated):\n",
      "        r = super().on_leave(node, updated)\n",
      "        if (\n",
      "            isinstance(node, cst.FunctionDef)\n",
      "            or isinstance(node, cst.ClassDef)\n",
      "            or isinstance(node, cst.Param)\n",
      "            or isinstance(node, cst.AnnAssign)\n",
      "        ):\n",
      "            self.stack.pop()\n",
      "        return r\n",
      "\n",
      "    def leave_FunctionDef(\n",
      "        self, node: cst.FunctionDef, updated: cst.FunctionDef\n",
      "    ) -> cst.FunctionDef:\n",
      "        self.stack.append(SpecialNames.Return)\n",
      "        patch = self.annots.get(self._current_path())\n",
      "        self.stack.pop()\n",
      "        return updated if patch is None else updated.with_changes(returns=patch)\n",
      "\n",
      "    def leave_Param(self, node: cst.Param, updated: cst.Param) -> cst.Param:\n",
      "        patch = self.annots.get(self._current_path())\n",
      "        return updated if patch is None else updated.with_changes(annotation=patch)\n",
      "\n",
      "\n",
      " <add> @dataclass\n",
      " <add> class MypyResult:\n",
      " <add>     num_errors: int\n",
      " <add>     num_error_dict: Dict[str, int]\n",
      " <add>     output_str: str\n",
      " <add> \n",
      " <add> \n",
      "class MypyChecker:\n",
      "\n",
      "    def __init__(self, dmypy_path, code_dir) -> None:\n",
      "        self.code_dir = realpath(code_dir)\n",
      "        self.dmypy_path = realpath(dmypy_path)\n",
      " <add>         subprocess.run(\n",
      " <add>             [\n",
      " <add>                 \"python\",\n",
      " <add>                 self.dmypy_path,\n",
      " <add>                 \"start\",\n",
      " <add>                 \"--\",\n",
      " <add>                 \"--follow-imports=skip\",\n",
      " <add>             ],\n",
      " <add>             cwd=self.code_dir,\n",
      " <add>         )\n",
      " <add>         subprocess.run(\n",
      " <add>             [\"python\", self.dmypy_path, \"check\", self.code_dir],\n",
      " <add>             cwd=self.code_dir,\n",
      " <del>         self.mypy_version = subprocess.run(\n",
      " <del>             [\"python\", self.dmypy_path, \"-V\"],\n",
      "            capture_output=True,\n",
      " <add>         )\n",
      " <del>             text=True,\n",
      " <del>             cwd=self.code_dir,\n",
      " <del>         ).stdout\n",
      "\n",
      " <add>     def close(self):\n",
      " <del>     def stop_daemon(self):\n",
      "        subprocess.run(\n",
      "            [\"python\", self.dmypy_path, \"stop\"],\n",
      "            cwd=self.code_dir,\n",
      "        )\n",
      "\n",
      " <add>     def check_code_dir(self) -> MypyResult:\n",
      " <add>         return self._run_mypy([\"python\", self.dmypy_path, \"check\", self.code_dir])\n",
      " <del>     def __del__(self):\n",
      " <del>         self.stop_daemon()\n",
      "\n",
      " <add> \n",
      " <add>     def recheck_files(self, *updated_files: str) -> MypyResult:\n",
      " <add>         return self._run_mypy([\"python\", self.dmypy_path, \"recheck\", \"--update\", *updated_files])\n",
      " <add>         \n",
      " <add> \n",
      " <add>     def _run_mypy(self, cmd: list[str]) -> MypyResult:\n",
      " <del>     def check_file(self, fpath):\n",
      " <del>         print(\"chekcing!\")\n",
      " <del>         cmd = [\"python\", self.dmypy_path, \"run\", \"--\", fpath]\n",
      "        result = subprocess.run(\n",
      "            cmd,\n",
      "            capture_output=True,\n",
      "            text=True,\n",
      "            cwd=self.code_dir,\n",
      "        )\n",
      "        lines = result.stdout.splitlines()\n",
      " <add>         assert (\n",
      " <add>             len(lines) > 0\n",
      " <add>         ), f\"mypy failed. Command: `{' '.join(cmd)}`\\nError: {result.stderr}\"\n",
      " <add>         num_error_dict: dict[str, int] = {}\n",
      " <del>         assert len(lines) > 0, f\"mypy failed. Error: {result.stderr}\"\n",
      " <del>         num_error_dict = {}\n",
      "        for l in lines:\n",
      "            m = re.match(r\"(.*\\.py):\\d+: error:.+\", l)\n",
      "            if m is not None:\n",
      "                num_error_dict[m.group(1)] = num_error_dict.get(m.group(1), 0) + 1\n",
      "\n",
      "        m = re.match(r\"Found (\\d+) errors? in\", lines[-1])\n",
      "        if m is None:\n",
      "            num_errors = 0\n",
      "        else:\n",
      "            num_errors = int(m.group(1))\n",
      "\n",
      "        total_errors = sum(num_error_dict.values())\n",
      "        assert (\n",
      "            num_errors == total_errors\n",
      "        ), f\"{num_errors}!= {total_errors}. mypy output: {result.stdout}\"\n",
      "        return MypyResult(num_errors, num_error_dict, result.stdout)\n",
      "\n",
      "\n",
      "@dataclass\n",
      " <add> class mypy_checker:\n",
      " <del> class MypyResult:\n",
      " <del>     num_errors: int\n",
      " <del>     num_error_dict: Dict[str, int]\n",
      " <del>     output_str: str\n",
      "\n",
      " <add>     dmypy_path: str\n",
      " <add>     code_dir: str\n",
      " <add> \n",
      " <add>     def __enter__(self):\n",
      " <add>         self.checker = MypyChecker(self.dmypy_path, self.code_dir)\n",
      " <add>         return self.checker\n",
      " <add> \n",
      " <add>     def __exit__(self, exc_type, exc_val, exc_tb):\n",
      " <add>         self.checker.close()\n"
     ]
    }
   ],
   "source": [
    "ex=data.all_edits()[1]\n",
    "# print(ex.elems)\n",
    "print(ex.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_name = \"SPOT\"\n",
    "dataset_name = \"medium\"\n",
    "window = WindowArgs(4096)\n",
    "encoder = AnalysisBasedEditEncoder(\n",
    "    window=window, extra_ctx_min_size=1000, extra_ctx_names=(\"usees\", \"post-usees\")\n",
    ")\n",
    "save_dir = get_dataset_dir(dataset_name) / \"tokenized-file_collapsed\"# repr_modified_args(encoder)\n",
    "datasets = load_datasets(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== test ====================\n",
      "n_projects: 20\n",
      "n_edits: 12666\n",
      "input_size:\n",
      "   mean: 1838.3\n",
      "   median: 1537.5\n",
      "   min: 15\n",
      "   max: 4096\n",
      "output_size:\n",
      "   mean: 113.59\n",
      "   median: 75\n",
      "   min: 5\n",
      "   max: 4776\n",
      "==================== valid ====================\n",
      "n_projects: 10\n",
      "n_edits: 2297\n",
      "input_size:\n",
      "   mean: 1770.5\n",
      "   median: 1541\n",
      "   min: 19\n",
      "   max: 4096\n",
      "output_size:\n",
      "   mean: 116.91\n",
      "   median: 89\n",
      "   min: 5\n",
      "   max: 1567\n",
      "==================== train ====================\n",
      "n_projects: 100\n",
      "n_edits: 50804\n",
      "input_size:\n",
      "   mean: 1819.5\n",
      "   median: 1489\n",
      "   min: 19\n",
      "   max: 4096\n",
      "output_size:\n",
      "   mean: 97.962\n",
      "   median: 64\n",
      "   min: 5\n",
      "   max: 8434\n"
     ]
    }
   ],
   "source": [
    "for group, dataset in datasets.items():\n",
    "    print(\"=\" * 20, group, \"=\" * 20)\n",
    "    pretty_print_dict(dataset.overall_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== test ====================\n",
      "n_projects: 20\n",
      "n_edits: 12667\n",
      "input_size:\n",
      "   mean: 1773.5\n",
      "   median: 1648\n",
      "   min: 19\n",
      "   max: 4102\n",
      "output_size:\n",
      "   mean: 113.57\n",
      "   median: 75\n",
      "   min: 5\n",
      "   max: 4776\n",
      "==================== valid ====================\n",
      "n_projects: 10\n",
      "n_edits: 2297\n",
      "input_size:\n",
      "   mean: 1702.5\n",
      "   median: 1559\n",
      "   min: 23\n",
      "   max: 4102\n",
      "output_size:\n",
      "   mean: 116.91\n",
      "   median: 89\n",
      "   min: 5\n",
      "   max: 1567\n",
      "==================== train ====================\n",
      "n_projects: 100\n",
      "n_edits: 50792\n",
      "input_size:\n",
      "   mean: 1700.8\n",
      "   median: 1559\n",
      "   min: 23\n",
      "   max: 4102\n",
      "output_size:\n",
      "   mean: 97.974\n",
      "   median: 64\n",
      "   min: 5\n",
      "   max: 8434\n"
     ]
    }
   ],
   "source": [
    "for group, dataset in datasets.items():\n",
    "    print(\"=\" * 20, group, \"=\" * 20)\n",
    "    pretty_print_dict(dataset.overall_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All edits: 1331\n",
      "All edits with extra context: 360\n"
     ]
    }
   ],
   "source": [
    "with_ctx = [e for e in data.all_edits() if decode_tokens(AnalysisBasedEditEncoder.CtxSepTokens) in decode_tokens(e.input_tks)]\n",
    "\n",
    "print(\"All edits:\", len(data.all_edits()))\n",
    "print(\"All edits with extra context:\", len(with_ctx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========Left Context========\n",
      "\n",
      "class UsageAnalysis:\n",
      " <add>     def __init__(self, project: PythonProject, add_override_usages: bool):\n",
      " <del>     def __init__(self, project: PythonProject):\n",
      "        self.project = project\n",
      " <add>         self.add_override_usages = add_override_usages\n",
      "        self.ns_hier = ModuleHierarchy.from_modules(project.modules.keys())\n",
      "        module2ns = build_project_namespaces(project)\n",
      "        self.sorted_modules = list(module2ns.keys())\n",
      "\n",
      "        self.path2elem = {v.path: v for v in project.all_elems()}\n",
      "        self.path2class = {\n",
      "            cls.path: cls\n",
      "            for mod in project.modules.values()\n",
      "            for cls in mod.all_classes()\n",
      "        }\n",
      "\n",
      "        for mname, ns in module2ns.items():\n",
      "            for s, p in ns.items():\n",
      "                if p in self.path2class:\n",
      "                    cls = self.path2class[p]\n",
      "                    self.path2class.setdefault(ProjectPath(mname, s), cls)\n",
      "                elif p in self.path2elem:\n",
      "                    self.path2elem.setdefault(ProjectPath(mname, s), self.path2elem[p])\n",
      "\n",
      "        self.mod2analysis = {\n",
      "            mname: ModuleAnlaysis(project.modules[mname])\n",
      "            for mname in self.sorted_modules\n",
      "        }\n",
      "        for ma in self.mod2analysis.values():\n",
      "            ma.udpate_superclasses_()\n",
      "\n",
      "        self.cls2members = cls2members = dict[ProjectPath, dict[str, PythonElem]]()\n",
      " <add>         all_usages = list[ProjectUsage]()\n",
      "\n",
      "        def process_cls(cls: PythonClass):\n",
      "            if (cpath := cls.path) in cls2members:\n",
      "                return\n",
      "            members = cls2members[cpath] = dict[str, PythonElem]()\n",
      "            bases = not_none(cls.superclasses)\n",
      "            parents = [\n",
      "                x for p in bases if (x := self.find_class(cpath.module, p)) is not None\n",
      "            ]\n",
      "            for parent in parents:\n",
      "                process_cls(parent)\n",
      "                members.update(cls2members[parent.path])\n",
      "            for a in cls.attributes.values():\n",
      " <add>                 if self.add_override_usages and a.name in members:\n",
      " <add>                     all_usages.append(ProjectUsage(a.path, members[a.name].path, True))\n",
      "                members[a.name] = a\n",
      "            for m in cls.methods.values():\n",
      " <add>                 if self.add_override_usages and m.name in members:\n",
      " <add>                     all_usages.append(ProjectUsage(m.path, members[m.name].path, True))\n",
      "                members[m.name] = m\n",
      "            for name, el in members.items():\n",
      "                self.path2elem[cpath.append(name)] = el\n",
      "\n",
      "        for mname in self.sorted_modules:\n",
      "            for cls in project.modules[mname].all_classes():\n",
      "                process_cls(cls)\n",
      "\n",
      "        all_class_members = {x.path for x in project.all_elems() if x.in_class}\n",
      "        self.name2class_member = groupby(\n",
      "            [self.path2elem[p] for p in all_class_members], lambda e: e.name\n",
      "        )\n",
      "\n",
      "        all_fixtures = [f for f in project.all_funcs() if f.is_fixture]\n",
      "        name2fixtures = groupby(all_fixtures, lambda f: f.name)\n",
      "        self.name2fixtures = {\n",
      "            n: {f.path for f in fs} for n, fs in name2fixtures.items()\n",
      "        }\n",
      "\n",
      " <add>         for mname, ma in self.mod2analysis.items():\n",
      " <add>             for caller, span, qname, is_call in ma.compute_module_usages():\n",
      " <add>                 all_usages.extend(self.generate_usages(mname, caller, qname, is_call))\n",
      " <add> \n",
      "        best_usages = dict[tuple[ProjectPath, ProjectPath], ProjectUsage]()\n",
      " <add>         for u in all_usages:\n",
      " <add>             up = u.user, u.used\n",
      " <add>             if up not in best_usages or u.is_certain > best_usages[up].is_certain:\n",
      " <add>                 best_usages[up] = u\n",
      " <add> \n",
      " <del>         for mname, ma in self.mod2analysis.items():\n",
      " <del>             usages = ma.compute_module_usages()\n",
      " <del>             for caller, span, qname, is_call in usages:\n",
      " <del>                 for u in self.generate_usages(mname, caller, span, qname, is_call):\n",
      " <del>                     up = u.user, u.used\n",
      " <del>                     if (\n",
      " <del>                         up not in best_usages\n",
      " <del>                         or u.is_certain > best_usages[up].is_certain\n",
      " <del>                     ):\n",
      " <del>                         best_usages[up] = u\n",
      "        all_usages = list(best_usages.values())\n",
      "        self.all_usages = all_usages\n",
      "        self.user2used = groupby(all_usages, lambda u: u.user)\n",
      "        self.used2user = groupby(all_usages, lambda u: u.used)\n",
      "\n",
      " <add>     add_override_usages:...\n",
      "\n",
      "@dataclass\n",
      "class PreprocessArgs:\n",
      " <add>     add_override_usages: bool = False\n",
      "# Usees ends\n",
      "def dataset_from_repos(\n",
      "    repos_root: Path,\n",
      "    repos_paths: Iterable[Path],\n",
      "    pre_args: PreprocessArgs,\n",
      "    max_line_width: int = 400,\n",
      "    max_workers: int | None = None,\n",
      "    tqdm_args: dict = {},\n",
      ") -> \"TokenizedSrcSet\":...\n",
      "\n",
      "def mk_preamble(\n",
      "    mod: cst.Module,\n",
      "    pre_args: PreprocessArgs,\n",
      ") -> tuple[str, TokenSeq]:...\n",
      "\n",
      "def wrap_main_code(code: str) -> str:...\n",
      "\n",
      "def data_project_from_dir(\n",
      "    root: Path,\n",
      "    max_line_width: int = 400,\n",
      "    drop_comments: bool = True,\n",
      "    file_filter: Callable[[Path], bool] = lambda p: True,\n",
      ") -> PythonProject:...\n",
      "\n",
      "# EDIT:\n",
      "\n",
      "========Ground Truth========\n",
      " <8>:<add>     analysis = UsageAnalysis(proj, pre_args.add_override_usages)\n",
      "     <del>     analysis = UsageAnalysis(proj)\n",
      "\n",
      "========Main Code========\n",
      " <0>def repo_to_tk_srcs(\n",
      " <1>    repo: Path,\n",
      " <2>    pre_args: PreprocessArgs,\n",
      " <3>    max_line_width: int = 400,\n",
      " <4>) -> list[TokenizedSrc]:\n",
      " <5>    proj = data_project_from_dir(\n",
      " <6>        repo, max_line_width=max_line_width, drop_comments=pre_args.drop_comments\n",
      " <7>    )\n",
      " <8>    analysis = UsageAnalysis(proj)\n",
      " <9>    sorted_moduels = analysis.sorted_modules\n",
      "<10>\n",
      "<11>    srcs = list[TokenizedSrc]()\n",
      "<12>    for mpath in sorted_moduels:\n",
      "<13>        mod = proj.modules[mpath]\n",
      "<14>        preamble, tokenized_preamble = mk_preamble(mod.tree, pre_args)\n",
      "<15>\n",
      "<16>        signature_map = dict() if pre_args.drop_env_types else None\n",
      "<17>        for elem in mod.all_elements():\n",
      "<18>            main_m = cst.Module(\n",
      "<19>                reformat_elems([elem], analysis.path2class, None, keep_body_types=True)\n",
      "<20>            )\n",
      "<21>            annots_info, types = collect_user_annotations(main_m)\n",
      "<22>            if len(annots_info) == 0:\n",
      "<23>                continue\n",
      "<24>            for info in annots_info:\n",
      "<25>                info.annot_range = None\n",
      "<26>            types_str = [\n",
      "<27>                main_m.code_for_node(not_none(info.annot).annotation)\n",
      "<28>                for info in annots_info\n",
      "<29>            ]\n",
      "<30>            mask_annot = cst.Annotation(cst.Name(SpecialNames.TypeMask))\n",
      "<31>            replaces = dict()\n",
      "<32>            for info in annots_info:\n",
      "<33>                replaces[info.path] = mask_annot\n",
      "<34>            new_code = wrap_main_code(apply_annotations(main_m, replaces).code)\n",
      "<35>            code_segs = new_code.split(SpecialNames.TypeMask)\n",
      "<36>            assert (\n",
      "<37>                len(code_segs) == len(types) + 1\n",
      "<38>            ), f\"{len(code_segs)}!= {len(types) + 1}. replaces: {replaces}\\ncode: {new_code}\"\n",
      "<39>\n",
      "<40>            left_m, right_m = ctx_modules_for_elem(\n",
      "<41>                elem, analysis, pre_args, signature_map\n",
      "<42>            )\n",
      "<43>            left_tks = None\n",
      "<44>            if left_m is not None:\n",
      "<45>                left_tks = DefaultTokenizer.encode(\n",
      "<46>                    left_m.code, add_special_tokens=False\n",
      "<47>                )\n",
      "<48>            right_tks = None\n",
      "<49>            if right_m is not None:\n",
      "<50>                right_tks = DefaultTokenizer.encode(\n",
      "<51>                    right_m.code, add_special_tokens=False\n",
      "<52>                )\n",
      "<53>\n",
      "<54>            file = proj.root_dir / proj.module2src_file[mpath] / elem.path.path\n",
      "<55>            src = tokenized_src_from_segs(\n",
      "<56>                file=file,\n",
      "<57>                repo=repo,\n",
      "<58>                preamble=preamble,\n",
      "<59>                tokenized_preamble=tokenized_preamble,\n",
      "<60>                code_segs=code_segs,\n",
      "<61>                types=types,\n",
      "<62>                types_str=types_str,\n",
      "<63>                annots_info=annots_info,\n",
      "<64>                cst_code=main_m.code,\n",
      "<65>                left_extra_tks=left_tks,\n",
      "<66>                right_extra_tks=right_tks,\n",
      "<67>            )\n",
      "<68>            srcs.append(src)\n",
      "<69>\n",
      "<70>    return srcs\n",
      "<71>\n",
      "========Right Context========\n",
      "\n",
      "def ctx_modules_for_elem(\n",
      "    elem: PythonElem,\n",
      "    analysis: UsageAnalysis,\n",
      "    pre_args: PreprocessArgs,\n",
      "    signature_map: SignatureMap | None,\n",
      ") -> tuple[cst.Module | None, cst.Module | None]:...\n",
      "\n",
      "def reformat_elems(\n",
      "    elems: Sequence[PythonElem],\n",
      "    path2class: dict[ProjectPath, PythonClass],\n",
      "    signature_map: SignatureMap | None,\n",
      "    reversed: bool = False,\n",
      "    signature_only=False,\n",
      "    keep_body_types=False,\n",
      "):...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(with_ctx[4].show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"medium\"\n",
    "save_dir = get_dataset_dir(dataset_name) / \"tokenized-file_collapsed\"\n",
    "datasets = load_datasets(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== test ====================\n",
      "n_projects: 20\n",
      "n_edits: 12666\n",
      "input_size:\n",
      "   mean: 1838.3\n",
      "   median: 1537.5\n",
      "   min: 15\n",
      "   max: 4096\n",
      "output_size:\n",
      "   mean: 113.59\n",
      "   median: 75\n",
      "   min: 5\n",
      "   max: 4776\n",
      "==================== valid ====================\n",
      "n_projects: 10\n",
      "n_edits: 2297\n",
      "input_size:\n",
      "   mean: 1770.5\n",
      "   median: 1541\n",
      "   min: 19\n",
      "   max: 4096\n",
      "output_size:\n",
      "   mean: 116.91\n",
      "   median: 89\n",
      "   min: 5\n",
      "   max: 1567\n",
      "==================== train ====================\n",
      "n_projects: 100\n",
      "n_edits: 50804\n",
      "input_size:\n",
      "   mean: 1819.5\n",
      "   median: 1489\n",
      "   min: 19\n",
      "   max: 4096\n",
      "output_size:\n",
      "   mean: 97.962\n",
      "   median: 64\n",
      "   min: 5\n",
      "   max: 8434\n"
     ]
    }
   ],
   "source": [
    "for group, dataset in datasets.items():\n",
    "    print(\"=\" * 20, group, \"=\" * 20)\n",
    "    pretty_print_dict(dataset.overall_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in datasets:\n",
    "    if not (save_dir / f\"{group}.pkl\").exists():\n",
    "        continue\n",
    "    dataset = pickle_load(save_dir / f\"{group}.pkl\")\n",
    "    print(\"=\" * 20, group, \"=\" * 20)\n",
    "    display(dataset.per_repo_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(datasets[\"test\"].all_edits())[1].show_prediction())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.venv': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6ffc72953da4dd16b2e00785be9c4013ef131f465a8658f3921b6634d4eeec8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
