{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from coeditor.common import *\n",
    "from coeditor.dataset import *\n",
    "from coeditor.encoding import *\n",
    "from spot.utils import pretty_print_dict\n",
    "\n",
    "os.chdir(proj_root())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = WindowArgs(4096)\n",
    "cst_encoder = CstBasedEditEncoder(window)\n",
    "encoder = AnalysisBasedEditEncoder(window, extra_ctx_min_size=1000)\n",
    "# data = dataset_from_projects([proj_root()], encoder, max_history_per_repo=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AnalysisBasedEditEncoder(window=(max_window_size=4096), extra_ctx_min_size=1000)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repr_modified_args(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_ctx = [e for e in data.all_edits() if decode_tokens(AnalysisBasedEditEncoder.ctx_sep_line) in decode_tokens(e.input_tks)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.all_edits())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(with_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========Left Context========\n",
      "<add> def read_file(path) -> str:\n",
      " <del> def read_file(path: str) -> str:\n",
      "    with open(path, \"r\") as f:\n",
      "        return f.read()\n",
      "\n",
      "# Usees ends\n",
      "TypeExpr = cst.BaseExpression\n",
      "\n",
      "@dataclass\n",
      "class TypeInfState:\n",
      "    module: cst.Module\n",
      "    to_annot: list[AnnotPath]\n",
      "    annotated: dict[AnnotPath, TypeExpr]\n",
      "    num_errors: int\n",
      "    def __str__(self):\n",
      "        return f\"\"\"\n",
      "    num_errors: {self.num_errors}\n",
      "    num_to_annot: {len(self.to_annot)}\n",
      "    to_annotate: {self.to_annot}\n",
      "    ------------------------ code -------------------------------\n",
      "    {self.module.code}\n",
      " <add>     \"\"\"\n",
      " <del>             \"\"\"\n",
      "\n",
      "\n",
      "class SelectAnnotations:\n",
      " <add>     @staticmethod\n",
      " <add>     def select_annotated(paths, annotated) -> list[AnnotPath]:\n",
      " <add>         return list(annotated.keys())\n",
      "\n",
      " <add>     @staticmethod\n",
      " <add>     def select_all_paths(paths, annotated) -> list[AnnotPath]:\n",
      " <add>         return paths\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class TypeInfAction:\n",
      "    path: AnnotPath\n",
      "    type: TypeExpr\n",
      "\n",
      "class TypeInfEnv:\n",
      "# EDIT:\n",
      "\n",
      "========Ground Truth========\n",
      " <0>:<add>     def __init__(\n",
      "     <add>         self,\n",
      "     <add>         checker: MypyChecker,\n",
      "     <add>         src_file,\n",
      "     <add>         select_annotations: Callable,\n",
      "     <add>     ):\n",
      "     <del>     def __init__(self, checker: MypyChecker, src_file: str):\n",
      "\n",
      " <4>:<add>         if ImportsAdder.SpecialComment in self.original_src:\n",
      "     <add>             raise RuntimeError(\n",
      "     <add>                 f\"The file {src_file} has already been modified by SPOT since it contains the special comment.\"\n",
      "     <add>             )\n",
      "     <add>         self.select_annotations = select_annotations\n",
      "     <add>         self.state: TypeInfState = None\n",
      "     <del>         self.state = TypeInfState(cst.Module([]), [], {}, 0)\n",
      "\n",
      " <5>:<del>         self.reset()\n",
      "\n",
      "========Main Code========\n",
      " <0>    def __init__(self, checker: MypyChecker, src_file: str):\n",
      " <1>        self.checker = checker\n",
      " <2>        self.src_file = realpath(src_file)\n",
      " <3>        self.original_src = read_file(src_file)\n",
      " <4>        self.state = TypeInfState(cst.Module([]), [], {}, 0)\n",
      " <5>        self.reset()\n",
      " <6>\n",
      "========Right Context========\n",
      "\n",
      "    checker:...\n",
      "    src_file:...\n",
      "    original_src:...\n",
      " <add>     select_annotations:...\n",
      "    state:...\n",
      " <add>     def restore_file(self) -> None:\n",
      " <add>         write_file(self.src_file, self.original_src)\n",
      "\n",
      " <add>     def to_annot(self):\n",
      " <add>         module = cst.parse_module(self.original_src)\n",
      " <add>         paths, annots = collect_annotations(module)\n",
      " <add>         to_annot: list[AnnotPath] = self.select_annotations(paths, annots)\n",
      " <add>         assert isinstance(to_annot, list)\n",
      " <add>         return to_annot\n",
      "\n",
      "    def reset(self) -> None:\n",
      " <add>         self.restore_file()\n",
      " <add>         module = cst.parse_module(self.original_src)\n",
      " <add>         paths, annots = collect_annotations(module)\n",
      " <add>         to_annot: list[AnnotPath] = self.select_annotations(paths, annots)\n",
      " <add>         to_remove = {p for p in annots.keys() if p in to_annot}\n",
      " <add>         module = apply_annotations(module, {p: AnyAnnot for p in to_remove})\n",
      " <add>         module = add_imports(\n",
      " <add>             module, [(\"typing\", \"Any\")]\n",
      " <add>         )\n",
      " <add>         write_file(self.src_file, module.code)\n",
      " <add>         annotated = {\n",
      " <add>             p: annots[p].annotation for p in annots.keys() if p not in to_remove\n",
      " <add>         }\n",
      " <add>         num_errors = self.checker.recheck_files(self.src_file).num_errors\n",
      " <add>         self.state = TypeInfState(module, to_annot, annotated, num_errors)\n",
      " <del>         write_file(self.src_file, self.original_src)\n",
      " <del>         self.state.module = cst.parse_module(self.original_src)\n",
      " <del>         paths, annots = collect_annotations(self.state.module)\n",
      " <del>         self.state.annotated = {k: v.annotation for k, v in annots.items()}\n",
      " <del>         self.state.to_annot = [p for p in paths if p not in annots]\n",
      " <del>         self.state.num_errors = self.checker.recheck_files(self.src_file).num_errors\n",
      "\n",
      "    def step(self, action: TypeInfAction, check_any=False) -> None:\n",
      " <add>         state = self.state\n",
      " <add>         assert state is not None, \"Did you forget to call reset()?\"\n",
      " <add>         assert (\n",
      " <add>             action.path in state.to_annot\n",
      " <add>         ), f\"Invalid action: path {action.path} not in `to_annot`.\"\n",
      " <del>         assert action.path in self.state.to_annot, f\"Invalid action: path {action.path} already annotated.\"\n",
      "        type = action.type\n",
      " <add>         mod = apply_annotations(state.module, {action.path: cst.Annotation(type)})\n",
      " <del>         mod = apply_annotations(self.state.module, {action.path: cst.Annotation(type)})\n",
      "        write_file(self.src_file, mod.code)\n",
      "        ne = self.checker.recheck_files(self.src_file).num_errors\n",
      " <add>         if ne > state.num_errors:\n",
      " <del>         if ne > self.state.num_errors:\n",
      "            type = cst.Name(\"Any\")\n",
      " <add>             mod = apply_annotations(state.module, {action.path: cst.Annotation(type)})\n",
      " <del>             mod = apply_annotations(self.state.module, {action.path: cst.Annotation(type)})\n",
      "            write_file(self.src_file, mod.code)\n",
      "            if check_any:\n",
      "                check_r = self.checker.recheck_files(self.src_file)\n",
      " <add>                 assert check_r.num_errors == state.num_errors, (\n",
      " <del>                 assert check_r.num_errors == self.state.num_errors, (\n",
      "                    \"Adding Any should not trigger more type errors.\\n\"\n",
      "                    f\"action: {action}\\n\"\n",
      "                    f\"mypy output: {check_r.output_str}\\n\"\n",
      "                    f\"---------code---------\\n {mod.code}\\n\"\n",
      "                )\n",
      " <add>         state.to_annot.remove(action.path)\n",
      " <add>         state.annotated[action.path] = type\n",
      " <add>         state.module = mod\n",
      " <del>         self.state.to_annot.remove(action.path)\n",
      " <del>         self.state.annotated[action.path] = type\n",
      " <del>         self.state.module = mod\n",
      "\n",
      " <add> @contextmanager\n",
      " <add> def type_inf_env(\n",
      " <add>     checker: MypyChecker,\n",
      " <add>     src_file,\n",
      " <add>     select_annotations: Callable = SelectAnnotations.select_annotated,\n",
      " <add> ):\n",
      " <add>     env = TypeInfEnv(checker, src_file, select_annotations)\n",
      " <add>     env.reset()\n",
      " <add>     yield env\n",
      " <add>     env.restore_file()\n",
      "\n",
      " <add> def _test_inference_performance(src_root, src_files):\n",
      " <add>     dmypy_path = proj_root() / \".venv/bin/dmypy\"\n",
      " <add>     with mypy_checker(dmypy_path, src_root) as checker:\n",
      " <add>         n_checks = 0\n",
      " <add>         t_s = time.time()\n",
      "</s>\n"
     ]
    }
   ],
   "source": [
    "print(with_ctx[4].show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"medium\"\n",
    "save_dir = get_dataset_dir(dataset_name) / \"tokenized-file_collapsed\"\n",
    "datasets = load_datasets(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== test ====================\n",
      "n_projects: 20\n",
      "n_edits: 12666\n",
      "input_size:\n",
      "   mean: 1838.3\n",
      "   median: 1537.5\n",
      "   min: 15\n",
      "   max: 4096\n",
      "output_size:\n",
      "   mean: 113.59\n",
      "   median: 75\n",
      "   min: 5\n",
      "   max: 4776\n",
      "==================== valid ====================\n",
      "n_projects: 10\n",
      "n_edits: 2297\n",
      "input_size:\n",
      "   mean: 1770.5\n",
      "   median: 1541\n",
      "   min: 19\n",
      "   max: 4096\n",
      "output_size:\n",
      "   mean: 116.91\n",
      "   median: 89\n",
      "   min: 5\n",
      "   max: 1567\n",
      "==================== train ====================\n",
      "n_projects: 100\n",
      "n_edits: 50804\n",
      "input_size:\n",
      "   mean: 1819.5\n",
      "   median: 1489\n",
      "   min: 19\n",
      "   max: 4096\n",
      "output_size:\n",
      "   mean: 97.962\n",
      "   median: 64\n",
      "   min: 5\n",
      "   max: 8434\n"
     ]
    }
   ],
   "source": [
    "for group, dataset in datasets.items():\n",
    "    print(\"=\" * 20, group, \"=\" * 20)\n",
    "    pretty_print_dict(dataset.overall_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in datasets:\n",
    "    if not (save_dir / f\"{group}.pkl\").exists():\n",
    "        continue\n",
    "    dataset = pickle_load(save_dir / f\"{group}.pkl\")\n",
    "    print(\"=\" * 20, group, \"=\" * 20)\n",
    "    display(dataset.per_repo_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(datasets[\"test\"].all_edits())[1].show_prediction())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.venv': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6ffc72953da4dd16b2e00785be9c4013ef131f465a8658f3921b6634d4eeec8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
