{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# turn off autoreload so that we can use the old model \n",
    "# when editing the current project\n",
    "\n",
    "from coeditor.common import *\n",
    "import os\n",
    "from coeditor.retrieval_model import RetrievalEditorModel, AttentionMode\n",
    "from coeditor.api import EditPredictionService, QueryRefEditEncoder, BatchArgs, DecodingArgs\n",
    "\n",
    "os.chdir(proj_root())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_dir = proj_root() / \"../SPOT-copy/\"\n",
    "target_dir = proj_root()\n",
    "model_name = \"coeditor-xl-bi-request-stub-comments-v4\"\n",
    "model_path = get_model_dir(True) / model_name\n",
    "model = RetrievalEditorModel.load(model_path)\n",
    "model.to(\"cuda:2\")\n",
    "batch_args = BatchArgs.service_default()\n",
    "service = EditPredictionService(\n",
    "    target_dir,\n",
    "    model,\n",
    "    batch_args=batch_args,\n",
    "    encoder=QueryRefEditEncoder(\n",
    "        max_ref_tks=batch_args.max_ref_tks,\n",
    "        max_query_tks=batch_args.max_query_tks,\n",
    "        max_output_tks=batch_args.max_output_tks,\n",
    "    ),\n",
    "    # dec_args = DecodingArgs(do_sample=False, num_beams=8, length_penalty=0.0)\n",
    "    dec_args = DecodingArgs(do_sample=True, top_p=0.95, marginalize_samples=50)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/MrVPlusOne/coeditor-xl-bi-request-stub-comments-v4/commit/23e44770495c87f5a13acb28c6e6885b970e1401', commit_message='Upload model', commit_description='', oid='23e44770495c87f5a13acb28c6e6885b970e1401', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(f\"MrVPlusOne/{model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "respect_lines = 71\n",
      "stub files: dict_keys(['coeditor.ctx_change_encoder', 'spot.static_analysis', 'coeditor.retrieval_model'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "edits_to_batches: 100%|██████████| 1/1 [00:00<00:00, 7973.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mnum batches: 1,\u001b[0m \u001b[34mbatch stats: {'mean': '1.0', 'median': '1.0', 'min': '1.0', 'max': '1.0'}\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 38/512 [00:02<00:31, 15.13it/s, unfinished=0] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing logs to: .coeditor_logs\n",
      "Target file: /home/jiayi/Projects/SPOT/src/coeditor/retrieval_model.py\n",
      "Edit range: (387, 0) - (485, 24)\n",
      "\t--------------- Suggestion 0 (score: 0.52) ---------------\n",
      "\t... (no change) ...\n",
      "\t--------------- Suggestion 1 (score: 0.12) ---------------\n",
      "\t... 6 lines omitted...\n",
      "\t                  change = req.target.map(lambda x: x.code)\n",
      "\t                  change_tks = change_to_tokens(change)\n",
      "\t                  pred = apply_output_tks_to_change(change_tks, req.respect_lines, out)\n",
      "\t                  pred_changes.append(pred)\n",
      "\t          assert_eq(len(pred_changes), len(out_tks), len(pred_scores))\n",
      " \n",
      "\t-         solutions = list[list[PredictedChange]]()\n",
      "\t+         solutions = list[tuple[str, PredictedChange]]()\n",
      "\t          for i in range(0, len(pred_changes), N):\n",
      "\t              sols = marginalize_preds(\n",
      "\t                  pred_changes[i : i + N],\n",
      "\t                  out_tks[i : i + N],\n",
      "\t                  pred_weights[i : i + N],\n",
      "\t                  pred_scores[i : i + N],\n",
      "\t... 4 lines omitted...\n",
      "\t--------------- Suggestion 2 (score: 0.12) ---------------\n",
      "\t... 7 lines omitted...\n",
      "\t                  change_tks = change_to_tokens(change)\n",
      "\t                  pred = apply_output_tks_to_change(change_tks, req.respect_lines, out)\n",
      "\t                  pred_changes.append(pred)\n",
      "\t          assert_eq(len(pred_changes), len(out_tks), len(pred_scores))\n",
      " \n",
      "\t          solutions = list[list[PredictedChange]]()\n",
      "\t-         for i in range(0, len(pred_changes), N):\n",
      "\t+         for i, change in enumerate(pred_changes):\n",
      "\t              sols = marginalize_preds(\n",
      "\t                  pred_changes[i : i + N],\n",
      "\t                  out_tks[i : i + N],\n",
      "\t                  pred_weights[i : i + N],\n",
      "\t                  pred_scores[i : i + N],\n",
      "\t              )\n",
      "\t... 3 lines omitted...\n",
      "original code:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>count</th>\n",
       "      <th>avg_time</th>\n",
       "      <th>total_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>run model</td>\n",
       "      <td>1</td>\n",
       "      <td>2.967389</td>\n",
       "      <td>2.967389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>construct project edit</td>\n",
       "      <td>1</td>\n",
       "      <td>2.679183</td>\n",
       "      <td>2.679183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>model.generate</td>\n",
       "      <td>1</td>\n",
       "      <td>2.514444</td>\n",
       "      <td>2.514444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>encode edits</td>\n",
       "      <td>1</td>\n",
       "      <td>0.926231</td>\n",
       "      <td>0.926231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>get target element</td>\n",
       "      <td>1</td>\n",
       "      <td>0.556301</td>\n",
       "      <td>0.556301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>decoder.forward</td>\n",
       "      <td>38</td>\n",
       "      <td>0.013228</td>\n",
       "      <td>0.502646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>assemble changes</td>\n",
       "      <td>1</td>\n",
       "      <td>0.414982</td>\n",
       "      <td>0.414982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     name  count  avg_time  total_time\n",
       "6               run model      1  2.967389    2.967389\n",
       "1  construct project edit      1  2.679183    2.679183\n",
       "4          model.generate      1  2.514444    2.514444\n",
       "2            encode edits      1  0.926231    0.926231\n",
       "0      get target element      1  0.556301    0.556301\n",
       "3         decoder.forward     38  0.013228    0.502646\n",
       "5        assemble changes      1  0.414982    0.414982"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "service.dec_args = DecodingArgs(do_sample=True, top_p=0.9, marginalize_samples=25)\n",
    "# service.dec_args = DecodingArgs(do_sample=False, num_beams=8, length_penalty=0.0)\n",
    "service.tlogger.clear()\n",
    "response = service.suggest_edit(Path(\"src/coeditor/retrieval_model.py\"), 463)\n",
    "print(response)\n",
    "service.tlogger.as_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"target_file\": \"/home/jiayi/Projects/SPOT/src/coeditor/api.py\",\n",
      "  \"edit_start\": [\n",
      "    235,\n",
      "    4\n",
      "  ],\n",
      "  \"edit_end\": [\n",
      "    373,\n",
      "    9\n",
      "  ],\n",
      "  \"old_code\": \"def suggest_edit(\\n        self,\\n        file: Path,\\n        line: int,\\n        log_file: Path | None = Path(\\\"coeditor-log.txt\\\"),\\n    ) -> ServiceResponse:\\n        \\\"\\\"\\\"Make the suggestion in-place at the given location.\\\"\\\"\\\"\\n        timed = self.tlogger.timed\\n        project = self.project\\n\\n        if not file.is_absolute():\\n            file = project / file\\n\\n        with timed(\\\"get target element\\\"):\\n            mname = PythonProject.rel_path_to_module_name(file.relative_to(project))\\n            stamp = os.stat(file).st_mtime\\n            now_code = file.read_text()\\n            now_mod = self.parse_cache.cached(\\n                mname,\\n                stamp,\\n                lambda: PythonModule.from_cst(\\n                    cst.parse_module(now_code), mname, drop_comments=False\\n                ),\\n            )\\n            now_elem = get_elem_by_line(now_mod, line)\\n            if now_elem is None:\\n                raise ValueError(\\n                    f\\\"No code element found at line {line} in file {file}.\\\"\\n                )\\n            if not isinstance(now_elem, PythonFunction):\\n                raise ValueError(f\\\"Only functions can be edited by the model.\\\")\\n\\n        with timed(\\\"construct project edit\\\"):\\n            pedit = self.config.get_pedit(\\n                project, file, self.prev_cache, self.now_cache\\n            )\\n            if mname not in pedit.changes:\\n                assert mname in pedit.before.modules\\n                assert mname in pedit.after.modules\\n                pedit.changes[mname] = ModuleEdit.from_no_change(\\n                    pedit.before.modules[mname]\\n                )\\n        match [\\n            c for c in pedit.all_elem_changes() if get_change_path(c) == now_elem.path\\n        ]:\\n            case [Modified(PythonFunction(), PythonFunction()) as mf]:\\n                elem_change = cast(Modified[PythonFunction], mf)\\n            case [Added(PythonFunction()) as mf]:\\n                elem_change = cast(Added[PythonFunction], mf)\\n            case _:\\n                trans_elem = copy.copy(now_elem)\\n                if self.config.drop_comments:\\n                    trans_elem.tree = remove_comments(trans_elem.tree)\\n                elem_change = Modified(trans_elem, trans_elem)\\n\\n        with timed(\\\"encode edits\\\"):\\n            respect_lines = (\\n                self.compute_offset(now_mod, now_elem, line, drop_comments=True) + 1\\n            )\\n            print(f\\\"{respect_lines = }\\\")\\n            req = EditRequest(elem_change, respect_lines)\\n            qedits = list(\\n                self.encoder.encode_pedit(\\n                    pedit,\\n                    self.stub_cache,\\n                    queries=[req],\\n                    training=False,\\n                )\\n            )\\n            if qedits[0].tk_pedit.module_stubs:\\n                print(\\\"stub files:\\\", qedits[0].tk_pedit.module_stubs.keys())\\n            assert len(qedits) == 1\\n            batches = query_edits_to_batches(qedits, self.batch_args)\\n            assert len(batches) == 1\\n            batch = batches[0]\\n\\n        with timed(\\\"run model\\\"), torch.autocast(\\\"cuda\\\"):\\n            predictions = self.model.predict_on_batch(\\n                batch, [req], self.dec_args, self.show_max_solutions\\n            )\\n            assert_eq(len(predictions), 1)\\n            predictions = predictions[0]\\n            assert predictions\\n\\n        # for i, (pred_change, _, score) in enumerate(predictions):\\n        #     print(\\\"=\\\" * 10, f\\\"Sugeestion {i}\\\", \\\"=\\\" * 10)\\n        #     print(f\\\"score: {score:.4g}\\\")\\n        #     print(show_change(pred_change))\\n\\n        best_output = predictions[0].out_tks\\n\\n        if log_file is not None:\\n            with log_file.open(\\\"w\\\") as f:\\n                input_tks = batch[\\\"input_ids\\\"][0]\\n                references = batch[\\\"references\\\"]\\n                output_truth = batch[\\\"labels\\\"][0]\\n                print(f\\\"{respect_lines = }\\\", file=f)\\n                print(f\\\"{len(input_tks) = }\\\", file=f)\\n                print(f\\\"{len(references) = }\\\", file=f)\\n                pred = RetrievalModelPrediction(\\n                    input_ids=input_tks,\\n                    output_ids=best_output,\\n                    labels=output_truth,\\n                    references=references,\\n                )\\n                pred_str = RetrievalDecodingResult.show_prediction(None, pred)\\n                print(pred_str, file=f)\\n\\n        now_span = now_mod.location_map[now_elem.tree]\\n        old_elem_code = get_span(now_code, now_span)\\n        respect_lines = (\\n            self.compute_offset(now_mod, now_elem, line, drop_comments=False) + 1\\n        )\\n\\n        suggestions = list[EditSuggestion]()\\n        for pred in predictions:\\n            new_elem_code = self.apply_edit_to_elem(\\n                file,\\n                now_mod,\\n                now_elem,\\n                line,\\n                pred.out_tks,\\n            )\\n            preview = self.preview_changes(old_elem_code, new_elem_code, respect_lines)\\n            suggestion = EditSuggestion(\\n                score=pred.score,\\n                change_preview=preview,\\n                new_code=new_elem_code,\\n            )\\n            suggestions.append(suggestion)\\n        as_tuple = lambda x: (x.line, x.column)\\n\\n        return ServiceResponse(\\n            target_file=file.as_posix(),\\n            edit_start=as_tuple(now_span.start),\\n            edit_end=as_tuple(now_span.end),\\n            old_code=old_elem_code,\\n            suggestions=suggestions,\\n        )\\n\",\n",
      "  \"suggestions\": [\n",
      "    {\n",
      "      \"score\": 0.04,\n",
      "      \"change_preview\": \"* Modified: \\n    + predictions=suggestions,\\n    +             scores=as_tuple,\\n    - edit_start=as_tuple(now_span.start),\\n    -             edit_end=as_tuple(now_span.end),\\n    -             old_code=old_elem_code,\\n    -             suggestions=suggestions,\\n              )\",\n",
      "      \"new_code\": \"    def suggest_edit(\\n        self,\\n        file: Path,\\n        line: int,\\n        log_file: Path | None = Path(\\\"coeditor-log.txt\\\"),\\n    ) -> ServiceResponse:\\n        \\\"\\\"\\\"Make the suggestion in-place at the given location.\\\"\\\"\\\"\\n        timed = self.tlogger.timed\\n        project = self.project\\n\\n        if not file.is_absolute():\\n            file = project / file\\n\\n        with timed(\\\"get target element\\\"):\\n            mname = PythonProject.rel_path_to_module_name(file.relative_to(project))\\n            stamp = os.stat(file).st_mtime\\n            now_code = file.read_text()\\n            now_mod = self.parse_cache.cached(\\n                mname,\\n                stamp,\\n                lambda: PythonModule.from_cst(\\n                    cst.parse_module(now_code), mname, drop_comments=False\\n                ),\\n            )\\n            now_elem = get_elem_by_line(now_mod, line)\\n            if now_elem is None:\\n                raise ValueError(\\n                    f\\\"No code element found at line {line} in file {file}.\\\"\\n                )\\n            if not isinstance(now_elem, PythonFunction):\\n                raise ValueError(f\\\"Only functions can be edited by the model.\\\")\\n\\n        with timed(\\\"construct project edit\\\"):\\n            pedit = self.config.get_pedit(\\n                project, file, self.prev_cache, self.now_cache\\n            )\\n            if mname not in pedit.changes:\\n                assert mname in pedit.before.modules\\n                assert mname in pedit.after.modules\\n                pedit.changes[mname] = ModuleEdit.from_no_change(\\n                    pedit.before.modules[mname]\\n                )\\n        match [\\n            c for c in pedit.all_elem_changes() if get_change_path(c) == now_elem.path\\n        ]:\\n            case [Modified(PythonFunction(), PythonFunction()) as mf]:\\n                elem_change = cast(Modified[PythonFunction], mf)\\n            case [Added(PythonFunction()) as mf]:\\n                elem_change = cast(Added[PythonFunction], mf)\\n            case _:\\n                trans_elem = copy.copy(now_elem)\\n                if self.config.drop_comments:\\n                    trans_elem.tree = remove_comments(trans_elem.tree)\\n                elem_change = Modified(trans_elem, trans_elem)\\n\\n        with timed(\\\"encode edits\\\"):\\n            respect_lines = (\\n                self.compute_offset(now_mod, now_elem, line, drop_comments=True) + 1\\n            )\\n            print(f\\\"{respect_lines = }\\\")\\n            req = EditRequest(elem_change, respect_lines)\\n            qedits = list(\\n                self.encoder.encode_pedit(\\n                    pedit,\\n                    self.stub_cache,\\n                    queries=[req],\\n                    training=False,\\n                )\\n            )\\n            if qedits[0].tk_pedit.module_stubs:\\n                print(\\\"stub files:\\\", qedits[0].tk_pedit.module_stubs.keys())\\n            assert len(qedits) == 1\\n            batches = query_edits_to_batches(qedits, self.batch_args)\\n            assert len(batches) == 1\\n            batch = batches[0]\\n\\n        with timed(\\\"run model\\\"), torch.autocast(\\\"cuda\\\"):\\n            predictions = self.model.predict_on_batch(\\n                batch, [req], self.dec_args, self.show_max_solutions\\n            )\\n            assert_eq(len(predictions), 1)\\n            predictions = predictions[0]\\n            assert predictions\\n\\n        # for i, (pred_change, _, score) in enumerate(predictions):\\n        #     print(\\\"=\\\" * 10, f\\\"Sugeestion {i}\\\", \\\"=\\\" * 10)\\n        #     print(f\\\"score: {score:.4g}\\\")\\n        #     print(show_change(pred_change))\\n\\n        best_output = predictions[0].out_tks\\n\\n        if log_file is not None:\\n            with log_file.open(\\\"w\\\") as f:\\n                input_tks = batch[\\\"input_ids\\\"][0]\\n                references = batch[\\\"references\\\"]\\n                output_truth = batch[\\\"labels\\\"][0]\\n                print(f\\\"{respect_lines = }\\\", file=f)\\n                print(f\\\"{len(input_tks) = }\\\", file=f)\\n                print(f\\\"{len(references) = }\\\", file=f)\\n                pred = RetrievalModelPrediction(\\n                    input_ids=input_tks,\\n                    output_ids=best_output,\\n                    labels=output_truth,\\n                    references=references,\\n                )\\n                pred_str = RetrievalDecodingResult.show_prediction(None, pred)\\n                print(pred_str, file=f)\\n\\n        now_span = now_mod.location_map[now_elem.tree]\\n        old_elem_code = get_span(now_code, now_span)\\n        respect_lines = (\\n            self.compute_offset(now_mod, now_elem, line, drop_comments=False) + 1\\n        )\\n\\n        suggestions = list[EditSuggestion]()\\n        for pred in predictions:\\n            new_elem_code = self.apply_edit_to_elem(\\n                file,\\n                now_mod,\\n                now_elem,\\n                line,\\n                pred.out_tks,\\n            )\\n            preview = self.preview_changes(old_elem_code, new_elem_code, respect_lines)\\n            suggestion = EditSuggestion(\\n                score=pred.score,\\n                change_preview=preview,\\n                new_code=new_elem_code,\\n            )\\n            suggestions.append(suggestion)\\n        as_tuple = lambda x: (x.line, x.column)\\n\\n        return ServiceResponse(\\n            target_file=file.as_posix(),\\n            predictions=suggestions,\\n            scores=as_tuple,\\n        )\\n\"\n",
      "    },\n",
      "    {\n",
      "      \"score\": 0.04,\n",
      "      \"change_preview\": \"* Modified: \\n    + score=as_tuple(pred.score),\\n    - edit_start=as_tuple(now_span.start),\\n    -             edit_end=as_tuple(now_span.end),\\n    -             old_code=old_elem_code,\\n    -             suggestions=suggestions,\\n    +             predictions=suggestions\\n              )\",\n",
      "      \"new_code\": \"    def suggest_edit(\\n        self,\\n        file: Path,\\n        line: int,\\n        log_file: Path | None = Path(\\\"coeditor-log.txt\\\"),\\n    ) -> ServiceResponse:\\n        \\\"\\\"\\\"Make the suggestion in-place at the given location.\\\"\\\"\\\"\\n        timed = self.tlogger.timed\\n        project = self.project\\n\\n        if not file.is_absolute():\\n            file = project / file\\n\\n        with timed(\\\"get target element\\\"):\\n            mname = PythonProject.rel_path_to_module_name(file.relative_to(project))\\n            stamp = os.stat(file).st_mtime\\n            now_code = file.read_text()\\n            now_mod = self.parse_cache.cached(\\n                mname,\\n                stamp,\\n                lambda: PythonModule.from_cst(\\n                    cst.parse_module(now_code), mname, drop_comments=False\\n                ),\\n            )\\n            now_elem = get_elem_by_line(now_mod, line)\\n            if now_elem is None:\\n                raise ValueError(\\n                    f\\\"No code element found at line {line} in file {file}.\\\"\\n                )\\n            if not isinstance(now_elem, PythonFunction):\\n                raise ValueError(f\\\"Only functions can be edited by the model.\\\")\\n\\n        with timed(\\\"construct project edit\\\"):\\n            pedit = self.config.get_pedit(\\n                project, file, self.prev_cache, self.now_cache\\n            )\\n            if mname not in pedit.changes:\\n                assert mname in pedit.before.modules\\n                assert mname in pedit.after.modules\\n                pedit.changes[mname] = ModuleEdit.from_no_change(\\n                    pedit.before.modules[mname]\\n                )\\n        match [\\n            c for c in pedit.all_elem_changes() if get_change_path(c) == now_elem.path\\n        ]:\\n            case [Modified(PythonFunction(), PythonFunction()) as mf]:\\n                elem_change = cast(Modified[PythonFunction], mf)\\n            case [Added(PythonFunction()) as mf]:\\n                elem_change = cast(Added[PythonFunction], mf)\\n            case _:\\n                trans_elem = copy.copy(now_elem)\\n                if self.config.drop_comments:\\n                    trans_elem.tree = remove_comments(trans_elem.tree)\\n                elem_change = Modified(trans_elem, trans_elem)\\n\\n        with timed(\\\"encode edits\\\"):\\n            respect_lines = (\\n                self.compute_offset(now_mod, now_elem, line, drop_comments=True) + 1\\n            )\\n            print(f\\\"{respect_lines = }\\\")\\n            req = EditRequest(elem_change, respect_lines)\\n            qedits = list(\\n                self.encoder.encode_pedit(\\n                    pedit,\\n                    self.stub_cache,\\n                    queries=[req],\\n                    training=False,\\n                )\\n            )\\n            if qedits[0].tk_pedit.module_stubs:\\n                print(\\\"stub files:\\\", qedits[0].tk_pedit.module_stubs.keys())\\n            assert len(qedits) == 1\\n            batches = query_edits_to_batches(qedits, self.batch_args)\\n            assert len(batches) == 1\\n            batch = batches[0]\\n\\n        with timed(\\\"run model\\\"), torch.autocast(\\\"cuda\\\"):\\n            predictions = self.model.predict_on_batch(\\n                batch, [req], self.dec_args, self.show_max_solutions\\n            )\\n            assert_eq(len(predictions), 1)\\n            predictions = predictions[0]\\n            assert predictions\\n\\n        # for i, (pred_change, _, score) in enumerate(predictions):\\n        #     print(\\\"=\\\" * 10, f\\\"Sugeestion {i}\\\", \\\"=\\\" * 10)\\n        #     print(f\\\"score: {score:.4g}\\\")\\n        #     print(show_change(pred_change))\\n\\n        best_output = predictions[0].out_tks\\n\\n        if log_file is not None:\\n            with log_file.open(\\\"w\\\") as f:\\n                input_tks = batch[\\\"input_ids\\\"][0]\\n                references = batch[\\\"references\\\"]\\n                output_truth = batch[\\\"labels\\\"][0]\\n                print(f\\\"{respect_lines = }\\\", file=f)\\n                print(f\\\"{len(input_tks) = }\\\", file=f)\\n                print(f\\\"{len(references) = }\\\", file=f)\\n                pred = RetrievalModelPrediction(\\n                    input_ids=input_tks,\\n                    output_ids=best_output,\\n                    labels=output_truth,\\n                    references=references,\\n                )\\n                pred_str = RetrievalDecodingResult.show_prediction(None, pred)\\n                print(pred_str, file=f)\\n\\n        now_span = now_mod.location_map[now_elem.tree]\\n        old_elem_code = get_span(now_code, now_span)\\n        respect_lines = (\\n            self.compute_offset(now_mod, now_elem, line, drop_comments=False) + 1\\n        )\\n\\n        suggestions = list[EditSuggestion]()\\n        for pred in predictions:\\n            new_elem_code = self.apply_edit_to_elem(\\n                file,\\n                now_mod,\\n                now_elem,\\n                line,\\n                pred.out_tks,\\n            )\\n            preview = self.preview_changes(old_elem_code, new_elem_code, respect_lines)\\n            suggestion = EditSuggestion(\\n                score=pred.score,\\n                change_preview=preview,\\n                new_code=new_elem_code,\\n            )\\n            suggestions.append(suggestion)\\n        as_tuple = lambda x: (x.line, x.column)\\n\\n        return ServiceResponse(\\n            target_file=file.as_posix(),\\n            score=as_tuple(pred.score),\\n            predictions=suggestions\\n        )\\n\"\n",
      "    },\n",
      "    {\n",
      "      \"score\": 0.04,\n",
      "      \"change_preview\": \"* Modified: \\n    + target_file=file.as_posix()\\n    - edit_start=as_tuple(now_span.start),\\n    -             edit_end=as_tuple(now_span.end),\\n    -             old_code=old_elem_code,\\n    -             suggestions=suggestions,\\n              )\",\n",
      "      \"new_code\": \"    def suggest_edit(\\n        self,\\n        file: Path,\\n        line: int,\\n        log_file: Path | None = Path(\\\"coeditor-log.txt\\\"),\\n    ) -> ServiceResponse:\\n        \\\"\\\"\\\"Make the suggestion in-place at the given location.\\\"\\\"\\\"\\n        timed = self.tlogger.timed\\n        project = self.project\\n\\n        if not file.is_absolute():\\n            file = project / file\\n\\n        with timed(\\\"get target element\\\"):\\n            mname = PythonProject.rel_path_to_module_name(file.relative_to(project))\\n            stamp = os.stat(file).st_mtime\\n            now_code = file.read_text()\\n            now_mod = self.parse_cache.cached(\\n                mname,\\n                stamp,\\n                lambda: PythonModule.from_cst(\\n                    cst.parse_module(now_code), mname, drop_comments=False\\n                ),\\n            )\\n            now_elem = get_elem_by_line(now_mod, line)\\n            if now_elem is None:\\n                raise ValueError(\\n                    f\\\"No code element found at line {line} in file {file}.\\\"\\n                )\\n            if not isinstance(now_elem, PythonFunction):\\n                raise ValueError(f\\\"Only functions can be edited by the model.\\\")\\n\\n        with timed(\\\"construct project edit\\\"):\\n            pedit = self.config.get_pedit(\\n                project, file, self.prev_cache, self.now_cache\\n            )\\n            if mname not in pedit.changes:\\n                assert mname in pedit.before.modules\\n                assert mname in pedit.after.modules\\n                pedit.changes[mname] = ModuleEdit.from_no_change(\\n                    pedit.before.modules[mname]\\n                )\\n        match [\\n            c for c in pedit.all_elem_changes() if get_change_path(c) == now_elem.path\\n        ]:\\n            case [Modified(PythonFunction(), PythonFunction()) as mf]:\\n                elem_change = cast(Modified[PythonFunction], mf)\\n            case [Added(PythonFunction()) as mf]:\\n                elem_change = cast(Added[PythonFunction], mf)\\n            case _:\\n                trans_elem = copy.copy(now_elem)\\n                if self.config.drop_comments:\\n                    trans_elem.tree = remove_comments(trans_elem.tree)\\n                elem_change = Modified(trans_elem, trans_elem)\\n\\n        with timed(\\\"encode edits\\\"):\\n            respect_lines = (\\n                self.compute_offset(now_mod, now_elem, line, drop_comments=True) + 1\\n            )\\n            print(f\\\"{respect_lines = }\\\")\\n            req = EditRequest(elem_change, respect_lines)\\n            qedits = list(\\n                self.encoder.encode_pedit(\\n                    pedit,\\n                    self.stub_cache,\\n                    queries=[req],\\n                    training=False,\\n                )\\n            )\\n            if qedits[0].tk_pedit.module_stubs:\\n                print(\\\"stub files:\\\", qedits[0].tk_pedit.module_stubs.keys())\\n            assert len(qedits) == 1\\n            batches = query_edits_to_batches(qedits, self.batch_args)\\n            assert len(batches) == 1\\n            batch = batches[0]\\n\\n        with timed(\\\"run model\\\"), torch.autocast(\\\"cuda\\\"):\\n            predictions = self.model.predict_on_batch(\\n                batch, [req], self.dec_args, self.show_max_solutions\\n            )\\n            assert_eq(len(predictions), 1)\\n            predictions = predictions[0]\\n            assert predictions\\n\\n        # for i, (pred_change, _, score) in enumerate(predictions):\\n        #     print(\\\"=\\\" * 10, f\\\"Sugeestion {i}\\\", \\\"=\\\" * 10)\\n        #     print(f\\\"score: {score:.4g}\\\")\\n        #     print(show_change(pred_change))\\n\\n        best_output = predictions[0].out_tks\\n\\n        if log_file is not None:\\n            with log_file.open(\\\"w\\\") as f:\\n                input_tks = batch[\\\"input_ids\\\"][0]\\n                references = batch[\\\"references\\\"]\\n                output_truth = batch[\\\"labels\\\"][0]\\n                print(f\\\"{respect_lines = }\\\", file=f)\\n                print(f\\\"{len(input_tks) = }\\\", file=f)\\n                print(f\\\"{len(references) = }\\\", file=f)\\n                pred = RetrievalModelPrediction(\\n                    input_ids=input_tks,\\n                    output_ids=best_output,\\n                    labels=output_truth,\\n                    references=references,\\n                )\\n                pred_str = RetrievalDecodingResult.show_prediction(None, pred)\\n                print(pred_str, file=f)\\n\\n        now_span = now_mod.location_map[now_elem.tree]\\n        old_elem_code = get_span(now_code, now_span)\\n        respect_lines = (\\n            self.compute_offset(now_mod, now_elem, line, drop_comments=False) + 1\\n        )\\n\\n        suggestions = list[EditSuggestion]()\\n        for pred in predictions:\\n            new_elem_code = self.apply_edit_to_elem(\\n                file,\\n                now_mod,\\n                now_elem,\\n                line,\\n                pred.out_tks,\\n            )\\n            preview = self.preview_changes(old_elem_code, new_elem_code, respect_lines)\\n            suggestion = EditSuggestion(\\n                score=pred.score,\\n                change_preview=preview,\\n                new_code=new_elem_code,\\n            )\\n            suggestions.append(suggestion)\\n        as_tuple = lambda x: (x.line, x.column)\\n\\n        return ServiceResponse(\\n            target_file=file.as_posix(),\\n            target_file=file.as_posix()\\n        )\\n\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(json.dumps(response.to_json(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stub files: dict_keys(['coeditor.api', 'coeditor.common', 'coeditor.encoders', 'coeditor.retrieval_model'])\n",
      "\u001b[34mnum batches: 1\u001b[0m\n",
      "\u001b[34mBatch stats: {'mean': '1.0', 'median': '1.0', 'min': '1.0', 'max': '1.0'}\u001b[0m\n",
      "========== Sugeestion 0 ==========\n",
      "score: -11.35\n",
      "* Modified: \n",
      "        # edit: coeditor.common/random_subset\n",
      "    -   def random_subset(all, n: int, seed: int = 42):\n",
      "    +   def random_subset(all, n: int, rng: random.Random | None = None):\n",
      "    +       if rng is None:\n",
      "    +           rng = random.Random(42)\n",
      "            if isinstance(all, Sequence):\n",
      "                xs = [x for x in all]\n",
      "    -           random.Random(seed).shuffle(xs)\n",
      "    +           rng.shuffle(xs)\n",
      "                return xs[:n]\n",
      "            elif isinstance(all, Mapping):\n",
      "                keys = [k for k in all]\n",
      "    -           random.Random(seed).shuffle(keys)\n",
      "    +           rng.shuffle(keys)\n",
      "                return {k: all[k] for k in keys[:n]}\n",
      "            else:\n",
      "                raise ArgumentError(all, f\"Unsupported arg type: {type(all)}\")\n",
      "========== Sugeestion 1 ==========\n",
      "score: -11.79\n",
      "* Modified: \n",
      "        # edit: coeditor.common/random_subset\n",
      "    -   def random_subset(all, n: int, seed: int = 42):\n",
      "    +   def random_subset(all, n: int, rng: random.Random | None = None):\n",
      "    +       if rng is None:\n",
      "    +           rng = random.Random(42)\n",
      "    +\n",
      "            if isinstance(all, Sequence):\n",
      "                xs = [x for x in all]\n",
      "    -           random.Random(seed).shuffle(xs)\n",
      "    +           rng.shuffle(xs)\n",
      "                return xs[:n]\n",
      "            elif isinstance(all, Mapping):\n",
      "                keys = [k for k in all]\n",
      "    -           random.Random(seed).shuffle(keys)\n",
      "    +           rng.shuffle(keys)\n",
      "                return {k: all[k] for k in keys[:n]}\n",
      "            else:\n",
      "                raise ArgumentError(all, f\"Unsupported arg type: {type(all)}\")\n",
      "========== Sugeestion 2 ==========\n",
      "score: -15.15\n",
      "* Modified: \n",
      "        # edit: coeditor.common/random_subset\n",
      "    -   def random_subset(all, n: int, seed: int = 42):\n",
      "    +   def random_subset(all, n: int, rng: random.Random | None = None):\n",
      "    +       if rng is None:\n",
      "    +           rng = random.Random(42)\n",
      "            if isinstance(all, Sequence):\n",
      "                xs = [x for x in all]\n",
      "    -           random.Random(seed).shuffle(xs)\n",
      "    +           rng.shuffle(xs)\n",
      "                return xs[:n]\n",
      "            elif isinstance(all, Mapping):\n",
      "                keys = [k for k in all]\n",
      "    -           random.Random(seed).shuffle(keys)\n",
      "    +           rng.shuffle(keys)\n",
      "                return {k: all[k] for k in keys[:n]}\n",
      "            else:\n",
      "                raise ArgumentError(all, f\"Unsupported arg type: {type(all)}\")\n"
     ]
    }
   ],
   "source": [
    "service.suggest_edit(Path(\"src/coeditor/common.py\"), 294)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stub files: dict_keys(['coeditor.api', 'coeditor.common', 'coeditor.encoders', 'coeditor.retrieval_model'])\n",
      "\u001b[34mnum batches: 1\u001b[0m\n",
      "\u001b[34mBatch stats: {'mean': '1.0', 'median': '1.0', 'min': '1.0', 'max': '1.0'}\u001b[0m\n",
      "========== Sugeestion 0 ==========\n",
      "score: -2.132\n",
      "* Modified: \n",
      "        # edit: coeditor.encoders/has_change\n",
      "    -   def has_change(tks: TokenSeq) -> bool:\n",
      "    -       return Add_id in tks or Del_id in tks\n",
      "    +   def has_change(tks: TokenSeq, skip_prefix: int = 16) -> bool:\n",
      "    +       return Add_id in tks or Del_id in tks[skip_prefix:]\n",
      "========== Sugeestion 1 ==========\n",
      "score: -3.006\n",
      "* Modified: \n",
      "        # edit: coeditor.encoders/has_change\n",
      "    -   def has_change(tks: TokenSeq) -> bool:\n",
      "    -       return Add_id in tks or Del_id in tks\n",
      "    +   def has_change(tks: TokenSeq, skip_prefix: int = 16) -> bool:\n",
      "    +       return Add_id in tks or Del_id in tks or skip_prefix in tks\n",
      "========== Sugeestion 2 ==========\n",
      "score: -3.18\n",
      "* Modified: \n",
      "        # edit: coeditor.encoders/has_change\n",
      "    -   def has_change(tks: TokenSeq) -> bool:\n",
      "    -       return Add_id in tks or Del_id in tks\n",
      "    +   def has_change(tks: TokenSeq, skip_prefix: int = 16) -> bool:\n",
      "    +       return Add_id in tks or Del_id in tks[:skip_prefix]\n"
     ]
    }
   ],
   "source": [
    "service.suggest_edit(Path(\"src/coeditor/encoders.py\"), 292)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>count</th>\n",
       "      <th>avg_time</th>\n",
       "      <th>total_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>run model</td>\n",
       "      <td>3</td>\n",
       "      <td>2.283796</td>\n",
       "      <td>6.851387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>construct project edit</td>\n",
       "      <td>3</td>\n",
       "      <td>0.674862</td>\n",
       "      <td>2.024587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>get target element</td>\n",
       "      <td>3</td>\n",
       "      <td>0.257443</td>\n",
       "      <td>0.772328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>encode edits</td>\n",
       "      <td>3</td>\n",
       "      <td>0.151020</td>\n",
       "      <td>0.453060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     name  count  avg_time  total_time\n",
       "3               run model      3  2.283796    6.851387\n",
       "1  construct project edit      3  0.674862    2.024587\n",
       "0      get target element      3  0.257443    0.772328\n",
       "2            encode edits      3  0.151020    0.453060"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "service.tlogger.as_dataframe()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0 | packaged by conda-forge | (main, Oct 25 2022, 06:24:40) [GCC 10.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6ffc72953da4dd16b2e00785be9c4013ef131f465a8658f3921b6634d4eeec8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
