{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from coeditor.common import *\n",
    "import os\n",
    "\n",
    "from coeditor.model import CoeditorModel, _Tokenizer\n",
    "from coeditor.dataset import TokenizedEditDataset\n",
    "from coeditor.history import show_change\n",
    "from coeditor.encoding import TokenizedEdit, decode_tokens\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "os.chdir(proj_root())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CoeditorModel.load_pretrained(get_model_dir() / \"small\")\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 50\n",
    "\n",
    "test_data: TokenizedEditDataset = pickle_load(get_dataset_dir(\"small\") / \"tokenized-file_based\" / \"test.pkl\")\n",
    "samples = [e for e in test_data.all_edits() if len(e.output_tks) < 30]\n",
    "random.seed(123)\n",
    "random.shuffle(samples)\n",
    "samples = samples[:n_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [03:31<00:00,  4.24s/it]\n"
     ]
    }
   ],
   "source": [
    "out_dir = Path(\"output/inspect_coeditor_model\")\n",
    "shutil.rmtree(out_dir, ignore_errors=True)\n",
    "out_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "for i, ex in enumerate(tqdm(samples)):\n",
    "    change = ex.as_change()\n",
    "    (out_dir / f\"{i}-truth.txt\").write_text(show_change(change, name=str(ex.path)))\n",
    "    pred_tks = model.predict(ex.input_tks)\n",
    "    pred_change = TokenizedEdit(ex.path, ex.input_tks, pred_tks).as_change()\n",
    "    (out_dir / f\"{i}-pred.txt\").write_text(show_change(pred_change, name=str(ex.path)))\n",
    "    compare_str = f\"* Ground truth: {decode_tokens(ex.output_tks)}\\n*Prediction: {decode_tokens(pred_tks)}\\n\"\n",
    "    (out_dir / f\"{i}-compare.txt\").write_text(compare_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<extra_id_0><extra_id_1> <add>     self.url = None\n",
      "<extra_id_2><extra_id_3><extra_id_4> <add>     self.data = {}\n",
      " <del> <extra_id_5><extra_id_6><extra_id_7><extra_id_8>\n"
     ]
    }
   ],
   "source": [
    "print(decode_tokens(samples[1].output_tks))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.venv': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6ffc72953da4dd16b2e00785be9c4013ef131f465a8658f3921b6634d4eeec8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
