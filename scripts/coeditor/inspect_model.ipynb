{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from coeditor.common import *\n",
    "import os\n",
    "\n",
    "from coeditor.model import CoeditorModel, _Tokenizer, DecodingArgs\n",
    "from coeditor.dataset import TokenizedEditDataset\n",
    "from coeditor.history import show_change\n",
    "from coeditor.encoding import TokenizedEdit, decode_tokens, WindowArgs\n",
    "from spot.data import output_ids_as_seqs\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "os.chdir(proj_root())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = get_model_dir(trained=False) / \"small\" / \"checkpoint-11244\"\n",
    "model = CoeditorModel.load_pretrained(model_dir)\n",
    "model.to(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 50\n",
    "\n",
    "test_data_name = \"small\"\n",
    "test_data: TokenizedEditDataset = pickle_load(get_dataset_dir(test_data_name) / \"tokenized-file_based\" / \"test.pkl\")\n",
    "samples = [e for e in test_data.all_edits() if len(e.output_tks) < 50]\n",
    "random.seed(123)\n",
    "random.shuffle(samples)\n",
    "samples = samples[:n_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:55<00:00,  1.12s/it]\n"
     ]
    }
   ],
   "source": [
    "out_dir = Path(\"output/inspect_coeditor_model\")\n",
    "shutil.rmtree(out_dir, ignore_errors=True)\n",
    "out_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "decode_args = DecodingArgs()\n",
    "\n",
    "for i, ex in enumerate(tqdm(samples)):\n",
    "    wargs = WindowArgs(4096)\n",
    "    ex = ex.truncate_ctx(wargs)\n",
    "    pred_tks = model.predict(ex.input_tks, decode_args)\n",
    "\n",
    "    (out_dir / f\"{i}-truth.txt\").write_text(ex.show())\n",
    "    pred_ex = TokenizedEdit(ex.path, ex.input_tks, pred_tks)\n",
    "    (out_dir / f\"{i}-pred.txt\").write_text(pred_ex.show())\n",
    "    compare_str = ex.show_prediction(pred_tks)\n",
    "    (out_dir / f\"{i}-compare.txt\").write_text(compare_str)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.venv': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6ffc72953da4dd16b2e00785be9c4013ef131f465a8658f3921b6634d4eeec8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
