{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext snakeviz\n",
    "%load_ext line_profiler\n",
    "\n",
    "# turn off autoreload so that we can use the old model \n",
    "# when editing the current project\n",
    "\n",
    "from coeditor.common import *\n",
    "import os\n",
    "\n",
    "os.chdir(proj_root())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coeditor.retrieval_model import RetrievalEditorModel, AttentionMode, BatchArgs\n",
    "from coeditor.api import EditPredictionService, QueryRefEditEncoder, BatchArgs, DecodingArgs\n",
    "from coeditor.dataset import load_datasets\n",
    "import torch\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = get_model_dir(True) / \"coeditor-large-request-stub-v2\"\n",
    "model = RetrievalEditorModel.load(model_path)\n",
    "model.to(\"cuda:2\")\n",
    "model.attention_mode = AttentionMode.bidirectional\n",
    "\n",
    "batch_args = copy.deepcopy(BatchArgs())\n",
    "batch_args.max_total_ref_tks //= 3\n",
    "batch_args.min_queires *= 3\n",
    "batch_args.max_queries *= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = QueryRefEditEncoder()\n",
    "dataset_dir = get_dataset_dir(\"large\") / (repr_modified_args(encoder))\n",
    "test_data = load_datasets(dataset_dir, [\"train\"])[\"train\"]\n",
    "test_edits = test_data.all_edits()[:100]\n",
    "del test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mnum batches: 7,\u001b[0m \u001b[34mbatch stats: {'mean': '14.3', 'median': '16.0', 'min': '9.0', 'max': '16.0'}\u001b[0m\n",
      "\u001b[34mnum batches: 7,\u001b[0m \u001b[34mbatch stats: {'mean': '14.3', 'median': '16.0', 'min': '4.0', 'max': '16.0'}\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0: 100%|██████████| 7/7 [00:13<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mnum batches: 7,\u001b[0m \u001b[34mbatch stats: {'mean': '14.3', 'median': '16.0', 'min': '9.0', 'max': '16.0'}\u001b[0m\n",
      "\u001b[34mnum batches: 7,\u001b[0m \u001b[34mbatch stats: {'mean': '14.3', 'median': '16.0', 'min': '4.0', 'max': '16.0'}\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0: 100%|██████████| 7/7 [00:13<00:00,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.4 s ± 16.4 ms per loop (mean ± std. dev. of 2 runs, 1 loop each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# query_ref_layer not batched\n",
    "%timeit -n 1 -r 2 model.run_on_edits(test_edits, batch_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mnum batches: 7,\u001b[0m \u001b[34mbatch stats: {'mean': '14.3', 'median': '16.0', 'min': '4.0', 'max': '16.0'}\u001b[0m\n",
      "\u001b[34mnum batches: 7,\u001b[0m \u001b[34mbatch stats: {'mean': '14.3', 'median': '16.0', 'min': '5.0', 'max': '16.0'}\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0: 100%|██████████| 7/7 [00:14<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mnum batches: 7,\u001b[0m \u001b[34mbatch stats: {'mean': '14.3', 'median': '16.0', 'min': '7.0', 'max': '16.0'}\u001b[0m\n",
      "\u001b[34mnum batches: 7,\u001b[0m \u001b[34mbatch stats: {'mean': '14.3', 'median': '16.0', 'min': '8.0', 'max': '16.0'}\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0: 100%|██████████| 7/7 [00:14<00:00,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.2 s ± 102 ms per loop (mean ± std. dev. of 2 runs, 1 loop each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 2 model.run_on_edits(test_edits, batch_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mnum batches: 7,\u001b[0m \u001b[34mbatch stats: {'mean': '14.3', 'median': '16.0', 'min': '9.0', 'max': '16.0'}\u001b[0m\n",
      "\u001b[34mnum batches: 7,\u001b[0m \u001b[34mbatch stats: {'mean': '14.3', 'median': '16.0', 'min': '4.0', 'max': '16.0'}\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0: 100%|██████████| 7/7 [00:16<00:00,  2.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "*** Profile stats marshalled to file '/tmp/tmpshd62_qx'.\n",
      "Opening SnakeViz in a new tab...\n",
      "snakeviz web server started on 127.0.0.1:8080; enter Ctrl-C to exit\n",
      "http://127.0.0.1:8080/snakeviz/%2Ftmp%2Ftmpshd62_qx\n"
     ]
    }
   ],
   "source": [
    "%snakeviz -t model.run_on_edits(test_edits, batch_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test run: 100%|██████████| 50/50 [00:08<00:00,  6.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Profile printout saved to text file 'lprof.txt'. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 5.51746 s\n",
      "File: /home/jiayi/Projects/SPOT/src/coeditor/retrieval_model.py\n",
      "Function: encode_query_block at line 1148\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "  1148                                           def encode_query_block(\n",
      "  1149                                               block: T5Block,\n",
      "  1150                                               query_hidden_states: Tensor,  # (n_queries, query_len, model_dim)\n",
      "  1151                                               ref_hidden_states: Tensor,  # (n_queries, ref_len, model_dim)\n",
      "  1152                                               position_bias: Tensor,\n",
      "  1153                                               output_attentions: bool = False,\n",
      "  1154                                           ) -> tuple[Tensor, ...]:\n",
      "  1155                                               \"\"\"Run a T5Block to encode the query. Instead of using self-attention, this uses\n",
      "  1156                                               a hybrid attention where the query is allowed to attend to both itself and the references.\n",
      "  1157                                               \"\"\"\n",
      "  1158                                           \n",
      "  1159      5064   56508030.0  11158.8      1.0      layer0 = block.layer[0]\n",
      "  1160      5064    3121307.0    616.4      0.1      assert isinstance(layer0, T5LayerSelfAttention)\n",
      "  1161      5064   95915349.0  18940.6      1.7      key_value_states = torch.cat([ref_hidden_states, query_hidden_states], dim=1)\n",
      "  1162      5064 3772841475.0 745031.9     68.4      hybrid_attention_outputs = t5_cross_attention(\n",
      "  1163      5064     898236.0    177.4      0.0          layer0,\n",
      "  1164      5064     771333.0    152.3      0.0          query_hidden_states,\n",
      "  1165      5064     796987.0    157.4      0.0          key_value_states=key_value_states,\n",
      "  1166      5064     730741.0    144.3      0.0          position_bias=position_bias,\n",
      "  1167      5064     677024.0    133.7      0.0          output_attentions=output_attentions,\n",
      "  1168                                               )\n",
      "  1169      5064    2393999.0    472.7      0.0      hidden_states = hybrid_attention_outputs[0]\n",
      "  1170                                           \n",
      "  1171      5064    5996911.0   1184.2      0.1      check_nan(\n",
      "  1172      5064    1147507.0    226.6      0.0          \"hybrid_attention_outputs[0]\",\n",
      "  1173      5064     849883.0    167.8      0.0          hidden_states,\n",
      "  1174      5064    3014621.0    595.3      0.1          {\n",
      "  1175      5064     684963.0    135.3      0.0              \"query_hidden_states\": query_hidden_states,\n",
      "  1176      5064     724430.0    143.1      0.0              \"ref_hidden_states\": ref_hidden_states,\n",
      "  1177                                                   },\n",
      "  1178                                               )\n",
      "  1179                                           \n",
      "  1180                                               # clamp inf values to enable fp16 training\n",
      "  1181      5064    5847009.0   1154.6      0.1      if hidden_states.dtype == torch.float16 and torch.isinf(hidden_states).any():\n",
      "  1182                                                   clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n",
      "  1183                                                   hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n",
      "  1184                                           \n",
      "  1185                                               # Apply Feed Forward layer\n",
      "  1186      5064   54125202.0  10688.2      1.0      ff_layer = block.layer[-1]\n",
      "  1187      5064    2973683.0    587.2      0.1      assert isinstance(ff_layer, T5LayerFF)\n",
      "  1188      5064 1491997942.0 294628.3     27.0      hidden_states: Tensor = ff_layer.forward(hidden_states)\n",
      "  1189                                           \n",
      "  1190                                               # clamp inf values to enable fp16 training\n",
      "  1191      5064    7095795.0   1401.2      0.1      if hidden_states.dtype == torch.float16 and torch.isinf(hidden_states).any():\n",
      "  1192                                                   clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n",
      "  1193                                                   hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n",
      "  1194                                           \n",
      "  1195      5064    8345617.0   1648.0      0.2      return (hidden_states, *hybrid_attention_outputs[1:])"
     ]
    }
   ],
   "source": [
    "%load_ext line_profiler\n",
    "\n",
    "from coeditor.retrieval_model import encode_query_block\n",
    "\n",
    "%lprun -T lprof.txt -f encode_query_block model.profile_run(repeats=50)\n",
    "None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6ffc72953da4dd16b2e00785be9c4013ef131f465a8658f3921b6634d4eeec8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
