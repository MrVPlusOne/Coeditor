{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext snakeviz\n",
    "%load_ext line_profiler\n",
    "\n",
    "# turn off autoreload so that we can use the old model \n",
    "# when editing the current project\n",
    "\n",
    "from coeditor.common import *\n",
    "import os\n",
    "\n",
    "os.chdir(proj_root())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coeditor.retrieval_model import RetrievalEditorModel, AttentionMode, BatchArgs\n",
    "from coeditor.api import EditPredictionService, QueryRefEditEncoder, BatchArgs, DecodingArgs\n",
    "from coeditor.dataset import load_datasets\n",
    "import torch\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = get_model_dir(True) / \"coeditor-large-request-stub-v2\"\n",
    "model = RetrievalEditorModel.load(model_path)\n",
    "model.to(\"cuda:2\")\n",
    "model.attention_mode = AttentionMode.bidirectional\n",
    "\n",
    "batch_args = copy.deepcopy(BatchArgs())\n",
    "batch_args.max_total_ref_tks //= 3\n",
    "batch_args.min_queires *= 3\n",
    "batch_args.max_queries *= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = QueryRefEditEncoder()\n",
    "dataset_dir = get_dataset_dir(\"large\") / (repr_modified_args(encoder))\n",
    "test_data = load_datasets(dataset_dir, [\"train\"])[\"train\"]\n",
    "test_edits = test_data.all_edits()[:100]\n",
    "del test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mnum batches: 7,\u001b[0m \u001b[34mbatch stats: {'mean': '14.3', 'median': '16.0', 'min': '4.0', 'max': '16.0'}\u001b[0m\n",
      "\u001b[34mnum batches: 7,\u001b[0m \u001b[34mbatch stats: {'mean': '14.3', 'median': '16.0', 'min': '4.0', 'max': '16.0'}\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0: 100%|██████████| 7/7 [00:10<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mnum batches: 7,\u001b[0m \u001b[34mbatch stats: {'mean': '14.3', 'median': '16.0', 'min': '4.0', 'max': '16.0'}\u001b[0m\n",
      "\u001b[34mnum batches: 7,\u001b[0m \u001b[34mbatch stats: {'mean': '14.3', 'median': '16.0', 'min': '4.0', 'max': '16.0'}\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0: 100%|██████████| 7/7 [00:10<00:00,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 s ± 3.62 ms per loop (mean ± std. dev. of 2 runs, 1 loop each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# query_ref_layer not batched\n",
    "%timeit -n 1 -r 2 model.run_on_edits(test_edits, batch_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mnum batches: 7,\u001b[0m \u001b[34mbatch stats: {'mean': '14.3', 'median': '16.0', 'min': '4.0', 'max': '16.0'}\u001b[0m\n",
      "\u001b[34mnum batches: 7,\u001b[0m \u001b[34mbatch stats: {'mean': '14.3', 'median': '16.0', 'min': '4.0', 'max': '16.0'}\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0: 100%|██████████| 7/7 [00:10<00:00,  1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mnum batches: 7,\u001b[0m \u001b[34mbatch stats: {'mean': '14.3', 'median': '16.0', 'min': '4.0', 'max': '16.0'}\u001b[0m\n",
      "\u001b[34mnum batches: 7,\u001b[0m \u001b[34mbatch stats: {'mean': '14.3', 'median': '16.0', 'min': '4.0', 'max': '16.0'}\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0: 100%|██████████| 7/7 [00:10<00:00,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.9 s ± 6.59 ms per loop (mean ± std. dev. of 2 runs, 1 loop each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 2 model.run_on_edits(test_edits, batch_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mnum batches: 7,\u001b[0m \u001b[34mbatch stats: {'mean': '14.3', 'median': '16.0', 'min': '9.0', 'max': '16.0'}\u001b[0m\n",
      "\u001b[34mnum batches: 7,\u001b[0m \u001b[34mbatch stats: {'mean': '14.3', 'median': '16.0', 'min': '4.0', 'max': '16.0'}\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0: 100%|██████████| 7/7 [00:10<00:00,  1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "*** Profile stats marshalled to file '/tmp/tmpjlbfo3f1'.\n",
      "Opening SnakeViz in a new tab...\n",
      "snakeviz web server started on 127.0.0.1:8080; enter Ctrl-C to exit\n",
      "http://127.0.0.1:8080/snakeviz/%2Ftmp%2Ftmpjlbfo3f1\n"
     ]
    }
   ],
   "source": [
    "%snakeviz -t model.run_on_edits(test_edits, batch_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mnum batches: 7,\u001b[0m \u001b[34mbatch stats: {'mean': '14.3', 'median': '16.0', 'min': '9.0', 'max': '16.0'}\u001b[0m\n",
      "\u001b[34mnum batches: 7,\u001b[0m \u001b[34mbatch stats: {'mean': '14.3', 'median': '16.0', 'min': '4.0', 'max': '16.0'}\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0: 100%|██████████| 7/7 [00:11<00:00,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Profile printout saved to text file 'lprof.txt'. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 1.80138 s\n",
      "File: /home/jiayi/Projects/SPOT/src/coeditor/retrieval_model.py\n",
      "Function: t5_sparse_attention at line 1220\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "  1220                                           def t5_sparse_attention(\n",
      "  1221                                               self: T5Attention,\n",
      "  1222                                               hidden_states: Tensor,\n",
      "  1223                                               block_lens: Sequence[int],\n",
      "  1224                                               get_bias: Callable[[int, int, bool], Tensor],\n",
      "  1225                                           ):\n",
      "  1226                                               \"\"\"\n",
      "  1227                                               A block-sparse self-attention layer that allows all blocks to attend to the\n",
      "  1228                                               last block (in additon to themselves).\n",
      "  1229                                               \"\"\"\n",
      "  1230                                               # Input is (batch_size, seq_length, dim)\n",
      "  1231                                               # Mask is (batch_size, key_length) (non-causal) or (batch_size, key_length, key_length)\n",
      "  1232                                               # past_key_value[0] is (batch_size, n_heads, q_len - 1, dim_per_head)\n",
      "  1233      1200    3805309.0   3171.1      0.2      batch_size, seq_length = hidden_states.shape[:2]\n",
      "  1234      1200     566594.0    472.2      0.0      if not block_lens:\n",
      "  1235                                                   raise ValueError(\"block_lens must not be empty\")\n",
      "  1236      1200    1055669.0    879.7      0.1      block_lens = list(block_lens)\n",
      "  1237                                           \n",
      "  1238      1200    3446030.0   2871.7      0.2      blocks = list[tuple[int, int]]()\n",
      "  1239      1200     261422.0    217.9      0.0      start = 0\n",
      "  1240      4308    1055386.0    245.0      0.1      for block_len in block_lens:\n",
      "  1241      4308    1079092.0    250.5      0.1          end = start + block_len\n",
      "  1242      4308    1919335.0    445.5      0.1          blocks.append((start, end))\n",
      "  1243      4308     865141.0    200.8      0.0          start = end\n",
      "  1244      1200    3620106.0   3016.8      0.2      assert_eq(start, seq_length)\n",
      "  1245                                           \n",
      "  1246      1200    1117714.0    931.4      0.1      def shape(states: Tensor):\n",
      "  1247                                                   \"\"\"projection\"\"\"\n",
      "  1248                                                   return states.view(\n",
      "  1249                                                       batch_size, -1, self.n_heads, self.key_value_proj_dim\n",
      "  1250                                                   ).transpose(1, 2)\n",
      "  1251                                           \n",
      "  1252      1200     730251.0    608.5      0.0      def unshape(states: Tensor):\n",
      "  1253                                                   \"\"\"reshape from (batch_size, n_heads, seq_length, dim_per_head) to (batch_size, seq_length, dim)\"\"\"\n",
      "  1254                                                   return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)\n",
      "  1255                                           \n",
      "  1256                                               # (batch_size, n_heads, seq_length, dim_per_head)\n",
      "  1257      1200   99509005.0  82924.2      5.5      query_states = shape(self.q(hidden_states))\n",
      "  1258      1200   69738377.0  58115.3      3.9      key_states = shape(self.k(hidden_states))\n",
      "  1259      1200   77967012.0  64972.5      4.3      value_states = shape(self.v(hidden_states))\n",
      "  1260                                           \n",
      "  1261      1200   16205661.0  13504.7      0.9      queries = list(torch.split_with_sizes(query_states, block_lens, dim=2))\n",
      "  1262      1200    9221970.0   7685.0      0.5      keys = list(torch.split_with_sizes(key_states, block_lens, dim=2))\n",
      "  1263      1200    8956059.0   7463.4      0.5      values = list(torch.split_with_sizes(value_states, block_lens, dim=2))\n",
      "  1264                                           \n",
      "  1265      1200     896182.0    746.8      0.0      g_start, g_end = blocks[-1]\n",
      "  1266                                               # add everything except the last block as special blocks\n",
      "  1267      1200   20421137.0  17017.6      1.1      queries.append(query_states[:, :, :g_start, :])\n",
      "  1268      1200   15538748.0  12949.0      0.9      keys.append(key_states[:, :, :g_start, :])\n",
      "  1269      1200   14322029.0  11935.0      0.8      values.append(value_states[:, :, :g_start, :])\n",
      "  1270      1200     816971.0    680.8      0.0      N = len(blocks)\n",
      "  1271                                           \n",
      "  1272      1200     473456.0    394.5      0.0      attn_outputs = []\n",
      "  1273                                           \n",
      "  1274                                               # compute for ref blocks\n",
      "  1275      4308    3349195.0    777.4      0.2      for i, self_len in enumerate(block_lens):\n",
      "  1276      4308    2677634.0    621.5      0.1          is_global = i == len(blocks) - 1\n",
      "  1277      4308    1230854.0    285.7      0.1          j = N if is_global else N - 1\n",
      "  1278                                           \n",
      "  1279      4308    1362804.0    316.3      0.1          query = queries[i]\n",
      "  1280      4308    1048814.0    243.5      0.1          key = keys[i]\n",
      "  1281      4308     985087.0    228.7      0.1          value = values[i]\n",
      "  1282                                           \n",
      "  1283      4308     837573.0    194.4      0.0          other_key = keys[j]\n",
      "  1284      4308     786293.0    182.5      0.0          other_value = values[j]\n",
      "  1285      4308    3443880.0    799.4      0.2          other_len = other_key.size(2)\n",
      "  1286                                           \n",
      "  1287                                                   # compute scores\n",
      "  1288      4308  174873539.0  40592.7      9.7          self_scores = torch.matmul(query, key.transpose(3, 2))\n",
      "  1289      4308  147975775.0  34349.1      8.2          other_scores = torch.matmul(query, other_key.transpose(3, 2))\n",
      "  1290                                                   # (batch_size, n_heads, query_len, query_len + global_len)\n",
      "  1291      4308   86741523.0  20135.0      4.8          scores = torch.cat([other_scores, self_scores], dim=-1)\n",
      "  1292      4308  190637942.0  44252.1     10.6          pos_bias = get_bias(self_len, other_len, is_global)\n",
      "  1293      4308   57566560.0  13362.7      3.2          scores += pos_bias\n",
      "  1294      4308   79084058.0  18357.5      4.4          attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "  1295      4308   37266163.0   8650.5      2.1          attn_weights = nn.functional.dropout(\n",
      "  1296      4308    3310975.0    768.6      0.2              attn_weights, p=self.dropout, training=self.training\n",
      "  1297                                                   )\n",
      "  1298      4308   71788429.0  16664.0      4.0          other_weights = attn_weights[:, :, :, :other_len]\n",
      "  1299      4308   48066417.0  11157.5      2.7          self_weights = attn_weights[:, :, :, other_len:]\n",
      "  1300      4308  164772400.0  38248.0      9.1          self_output = torch.matmul(self_weights, value)\n",
      "  1301      4308  131850560.0  30606.0      7.3          other_output = torch.matmul(other_weights, other_value)\n",
      "  1302                                                   # (batch_size, n_heads, seq_length, dim_per_head)\n",
      "  1303      4308   57373981.0  13318.0      3.2          out = self_output + other_output\n",
      "  1304      4308   21953610.0   5096.0      1.2          assert_eq(out.size(2), self_len)\n",
      "  1305      4308    2677302.0    621.5      0.1          attn_outputs.append(out)\n",
      "  1306                                           \n",
      "  1307                                               # (batch_size, seq_length, dim)\n",
      "  1308      1200   68507815.0  57089.8      3.8      attn_output = unshape(torch.cat(attn_outputs, dim=2))\n",
      "  1309      1200    4011639.0   3343.0      0.2      assert_eq(attn_output.ndim, 3)\n",
      "  1310      1200    2437733.0   2031.4      0.1      assert_eq(attn_output.size(1), seq_length)\n",
      "  1311                                           \n",
      "  1312      1200   70842648.0  59035.5      3.9      attn_output = self.o(attn_output)\n",
      "  1313      1200    1095558.0    913.0      0.1      assert isinstance(attn_output, Tensor)\n",
      "  1314      1200    8720061.0   7266.7      0.5      assert_eq(attn_output.shape, hidden_states.shape)\n",
      "  1315      1200     511665.0    426.4      0.0      return attn_output"
     ]
    }
   ],
   "source": [
    "from coeditor.retrieval_model import t5_sparse_attention\n",
    "\n",
    "%lprun -T outupt/lprof.txt -f t5_sparse_attention model.run_on_edits(test_edits, batch_args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0 | packaged by conda-forge | (main, Oct 25 2022, 06:24:40) [GCC 10.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6ffc72953da4dd16b2e00785be9c4013ef131f465a8658f3921b6634d4eeec8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
