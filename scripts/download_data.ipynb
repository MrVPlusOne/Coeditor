{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from coeditor.common import *\n",
    "import os\n",
    "\n",
    "import requests\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "os.chdir(proj_root())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import dateparser\n",
    "from coeditor.git import GitRepo\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "\n",
    "def request_page(page: int, license: str, n_items: int = 10):\n",
    "    if Path(\"config/github_token.txt\").exists():\n",
    "        token = Path(\"config/github_token.txt\").read_text().strip()\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {token}\"\n",
    "        }\n",
    "    else:\n",
    "        headers = None\n",
    "    return requests.get(\n",
    "        f\"https://api.github.com/search/repositories?q=NOT+interview+NOT+reference+NOT+course+NOT+cheatsheet+created%3A>2018-01-01+stars%3A>100+size%3A<20000+language%3APython+license%3A{license}&sort=stars&order=desc&per_page={n_items}&page={page}\",\n",
    "        headers=headers,\n",
    "    ).json()\n",
    "\n",
    "\n",
    "def fetch_python_repos(license2counts: dict[str, int]):\n",
    "    n_repos = sum(license2counts.values())\n",
    "    repos = dict[str, GitRepo]()\n",
    "    with tqdm(total=n_repos) as pbar:\n",
    "        for license, n_repos in license2counts.items():\n",
    "            for i in range(1, n_repos // 100 + 1):\n",
    "                page = request_page(i, n_items=100, license=license)\n",
    "                if (msg := page.get(\"message\", \"\")) and msg.startswith(\n",
    "                    \"API rate limit exceeded\"\n",
    "                ):\n",
    "                    print(\"API rate limit exceeded, now wait for 1 min\")\n",
    "                    time.sleep(60)\n",
    "                    continue\n",
    "                if not page.get(\"items\"):\n",
    "                    print(\"Fetching page failed:\")\n",
    "                    print(page)\n",
    "                    break\n",
    "                for item in page[\"items\"]:\n",
    "                    r = GitRepo.from_github_item(item)\n",
    "                    if not r.archived:\n",
    "                        if r.authorname() in repos:\n",
    "                            print(f\"[warning] {r.authorname()} already in repos\")\n",
    "                        repos[r.authorname()] = r\n",
    "                pbar.update(len(page[\"items\"]))\n",
    "    return [repos[k] for k in list(repos)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mit': 7386, 'apache-2.0': 2809, 'bsd-3-clause': 523, 'bsd-2-clause': 149}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    l: int(request_page(0, l, n_items=1)[\"total_count\"])\n",
    "    for l in [\"mit\", \"apache-2.0\", \"bsd-3-clause\", \"bsd-2-clause\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [01:17<00:00, 32.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repos: 2445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "license2counts = {\n",
    "    \"mit\": 1000,\n",
    "    \"apache-2.0\": 1000,\n",
    "    \"bsd-3-clause\": 500,\n",
    "}\n",
    "\n",
    "all_repos = fetch_python_repos(license2counts)\n",
    "print(\"Repos:\", len(all_repos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "downloading repos: 100%|██████████| 2445/2445 [22:13<00:00,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: 2444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"perm2K\"  # permissive licensed 2K repos\n",
    "repos_dir = get_dataset_dir(dataset_name)\n",
    "(repos_dir / \"downloading\").mkdir(exist_ok=True, parents=True)\n",
    "(repos_dir / \"downloaded\").mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "downloaded = pmap(\n",
    "    GitRepo.download,\n",
    "    all_repos,\n",
    "    key_args={\"repos_dir\": repos_dir, \"full_history\": True},\n",
    "    desc=\"downloading repos\",\n",
    "    max_workers=4,\n",
    "    chunksize=1,\n",
    ")\n",
    "\n",
    "print(\"Successfully downloaded:\", sum(downloaded))\n",
    "downloaded_repos = [r for r, d in zip(all_repos, downloaded) if d]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: 2444\n"
     ]
    }
   ],
   "source": [
    "print(\"Successfully downloaded:\", len(downloaded_repos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2444/2444 [00:31<00:00, 77.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering by commits: 1664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# now filter out repos with less than 50 commits\n",
    "filtered_repos = [r for r in tqdm(downloaded_repos) if r.count_commits(repos_dir) >= 50]\n",
    "print(\"After filtering by commits:\", len(filtered_repos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totoal duplicates: 15\n",
      "After filtering duplicates: 1650\n"
     ]
    }
   ],
   "source": [
    "from coeditor.dataset import get_repo_signature\n",
    "\n",
    "repo_paths = [repos_dir / \"downloaded\" / r.authorname() for r in filtered_repos]\n",
    "sigs = pmap(get_repo_signature, repo_paths, desc=\"getting repo signatures\", chunksize=1)\n",
    "sig_groups = groupby(enumerate(sigs), lambda x: x[1])\n",
    "\n",
    "duplicates = set[str]()\n",
    "for sig, group in sig_groups.items():\n",
    "    if len(group) > 1:\n",
    "        print(f\"{len(group)} repos have the same signature {sig}:\")\n",
    "        for i, _ in group:\n",
    "            print(f\"  {downloaded_repos[i].authorname()}\")\n",
    "        for i, _ in group[1:]:\n",
    "            duplicates.add(downloaded_repos[i].authorname())\n",
    "\n",
    "print(\"Totoal duplicates:\", len(duplicates))\n",
    "filtered_repos = [r for r in filtered_repos if r.authorname() not in duplicates]\n",
    "print(\"After filtering duplicates:\", len(filtered_repos))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_test=50, n_valid=50, n_train=1550\n"
     ]
    }
   ],
   "source": [
    "n_test = 50\n",
    "n_valid = 50\n",
    "n_train = len(filtered_repos) - n_test - n_valid\n",
    "print(f\"n_test={n_test}, n_valid={n_valid}, n_train={n_train}\")\n",
    "\n",
    "random.seed(42)\n",
    "filtered_repos.sort(key=lambda r: r.authorname())\n",
    "random.shuffle(filtered_repos)\n",
    "\n",
    "split = {\n",
    "    \"test\": filtered_repos[:n_test],\n",
    "    \"valid\": filtered_repos[n_test : n_test + n_valid],\n",
    "    \"train\": filtered_repos[n_test + n_valid :][:n_train],\n",
    "}\n",
    "\n",
    "pickle_dump(repos_dir / \"repos_split.pkl\", split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "moving test: 100%|██████████| 50/50 [00:00<00:00, 670.37it/s]\n",
      "moving valid: 100%|██████████| 50/50 [00:00<00:00, 716.50it/s]\n",
      "moving train: 100%|██████████| 1550/1550 [00:02<00:00, 686.52it/s]\n"
     ]
    }
   ],
   "source": [
    "# move downloaded repos to their split group\n",
    "for group, rs in split.items():\n",
    "    for repo in tqdm(rs, desc=f\"moving {group}\"):\n",
    "        dest = repos_dir / \"repos\" / group\n",
    "        dest.mkdir(exist_ok=True, parents=True)\n",
    "        shutil.move(repos_dir / \"downloaded\" / repo.authorname(), dest)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.venv': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6ffc72953da4dd16b2e00785be9c4013ef131f465a8658f3921b6634d4eeec8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
