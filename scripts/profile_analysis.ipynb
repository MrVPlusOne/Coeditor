{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext snakeviz\n",
    "%load_ext line_profiler\n",
    "\n",
    "\n",
    "from coeditor.common import *\n",
    "import os\n",
    "\n",
    "os.chdir(proj_root())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CommitInfo(hash='84cfd5206348ecc3f54d202b830f803d8a03f26f', parents=('a1e2b73ab836924d0b1f9ed88e4fd90e7a6f61e6',), msg='Add ablation: dense attention.')\n",
      "CommitInfo(hash='a1e2b73ab836924d0b1f9ed88e4fd90e7a6f61e6', parents=('ecdbdc3875e47887ff3c0320fd1367af28d0a491',), msg='Implement ablation: current_code_only.')\n",
      "CommitInfo(hash='ecdbdc3875e47887ff3c0320fd1367af28d0a491', parents=('ad918b35e2b8314f30a7f8ffc1e957c9f49956df',), msg='Exclude builtins defs in ctx by default.')\n"
     ]
    }
   ],
   "source": [
    "from coeditor.git import get_commit_history, CommitInfo\n",
    "\n",
    "repo_root = proj_root()\n",
    "# repo_root = get_dataset_dir(\"tiny\") / \"repos/test/gym\"\n",
    "\n",
    "commits = get_commit_history(repo_root, 5)\n",
    "# commits = [CommitInfo(\"9dea81b48a2e1d8f7e7a81211c0f09f627ee61a9\", (), \"omit\"),\n",
    "#            CommitInfo(\"6a4cefd875f6547fc0bb43e02b14bb21d70fda9a\", (), \"omit\")]\n",
    "for c in commits[:3]:\n",
    "    print(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "building initial project: 100%|██████████| 34/34 [00:00<00:00, 47.48it/s]\n",
      "processing commits: 100%|██████████| 4/4 [00:04<00:00,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(problems) = 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>count</th>\n",
       "      <th>avg_time</th>\n",
       "      <th>total_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>process_change</td>\n",
       "      <td>4</td>\n",
       "      <td>0.663314</td>\n",
       "      <td>2.653256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>parse_module</td>\n",
       "      <td>42</td>\n",
       "      <td>0.021610</td>\n",
       "      <td>0.907621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>checkout</td>\n",
       "      <td>13</td>\n",
       "      <td>0.008255</td>\n",
       "      <td>0.107321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JModuleChange.from_modules</td>\n",
       "      <td>8</td>\n",
       "      <td>0.013032</td>\n",
       "      <td>0.104256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pre_edit_analysis</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>post_edit_analysis</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         name  count  avg_time  total_time\n",
       "5              process_change      4  0.663314    2.653256\n",
       "1                parse_module     42  0.021610    0.907621\n",
       "0                    checkout     13  0.008255    0.107321\n",
       "2  JModuleChange.from_modules      8  0.013032    0.104256\n",
       "4           pre_edit_analysis      4  0.000002    0.000007\n",
       "3          post_edit_analysis      4  0.000001    0.000005"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from coeditor.scoped_changes import edits_from_commit_history, _tlogger\n",
    "from coeditor.c3problem import C3ProblemGenerator, C3ProblemTokenizer, JediUsageAnalyzer\n",
    "from coeditor.experiments.code_completion import FIMProblem, C3CompletionGenerator\n",
    "\n",
    "# generator = C3ProblemGenerator(JediUsageAnalyzer(include_builtins=True))\n",
    "generator = C3CompletionGenerator()\n",
    "\n",
    "workdir = proj_root() / \"../temp-1\"\n",
    "# subprocess.run([\"rm\", \"-r\", workdir])\n",
    "_tlogger.clear()\n",
    "problems = edits_from_commit_history(\n",
    "    repo_root, commits, workdir, change_processor=generator\n",
    ")\n",
    "print(f\"{len(problems) = }\")\n",
    "# for err, count in generator.analyzer.error_counts.items():\n",
    "#     print(f\"({count=}): {err}\")\n",
    "display(_tlogger.as_dataframe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "output:\n",
      "            save_steps=max(500, min(10000, epoch_steps // 5)),\n",
      "--------------------------------------------------------------------------------\n",
      "input:\n",
      "<s>    out_tks: TokenSeq\n",
      "    score: float\n",
      "    n_samples: int\n",
      "\n",
      "class RetrievalModelPrediction(TypedDict):\n",
      "    input_ids: TokenSeq\n",
      "    output_ids: TokenSeq\n",
      "    labels: TokenSeq\n",
      "    references: list[TokenSeq]\n",
      "\n",
      "class AttentionMode(enum.Enum):\n",
      "    basic = enum.auto()\n",
      "    query2ref = enum.auto()\n",
      "    bidirectional = enum.auto()\n",
      "\n",
      "class RetrievalEditorModel(T5PreTrainedModel):\n",
      "    is_parallelizable = False\n",
      "    supports_gradient_checkpointing = False\n",
      "\n",
      "    \"\"\"\n",
      "    A CodeT5 model that takes in multiple reference code snippets and a\n",
      "    query code snippet with multiple masked spans and perdicts the maksed spans.\n",
      "\n",
      "    While the computational cost of a normal CodeT5 encoder increases quadratically,\n",
      "    this model only increases linearly with the number of reference code snippets.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, config: T5Config):\n",
      "        super().__init__(config)\n",
      "        self.model_dim = config.d_model\n",
      "\n",
      "        self.shared = nn.Embedding(config.vocab_size, config.d_model)\n",
      "\n",
      "        encoder_config = copy.deepcopy(config)\n",
      "        encoder_config.is_decoder = False\n",
      "        encoder_config.use_cache = False\n",
      "        encoder_config.is_encoder_decoder = False\n",
      "        self.encoder = T5Stack(encoder_config, self.shared)\n",
      "\n",
      "        decoder_config = copy.deepcopy(config)\n",
      "        decoder_config.is_decoder = True\n",
      "        decoder_config.is_encoder_decoder = False\n",
      "        decoder_config.num_layers = config.num_decoder_layers\n",
      "        self.decoder = T5Stack(decoder_config, self.shared)\n",
      "\n",
      "        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
      "\n",
      "        # Initialize weights and apply final processing\n",
      "        self.post_init()\n",
      "\n",
      "        amode = getattr(config, \"attention_mode\", AttentionMode.bidirectional.name)\n",
      "        self.attention_mode = AttentionMode[amode]\n",
      "        self.tlogger = TimeLogger()\n",
      "\n",
      "    @attention_mode.setter\n",
      "    def attention_mode(self, mode: AttentionMode):\n",
      "        self._attention_mode = mode\n",
      "        self.config.attention_mode = mode.name\n",
      "\n",
      "    def train_on_data(\n",
      "        self,\n",
      "        training_name: str,\n",
      "        train_loader: \"C3DataLoader\",\n",
      "        eval_loader: \"C3DataLoader\",\n",
      "        train_args: \"TrainingArgs\",\n",
      "    ) -> None:\n",
      "        train_dir = get_model_dir(trained=False) / training_name\n",
      "        # eval_loader.tqdm_args = {\"disable\": True}\n",
      "\n",
      "        model = self\n",
      "        # model = torch.compile(self.to(\"cuda\"))  # pytorch doesn't support python 3.11 yet.\n",
      "\n",
      "        class DynamicTrainer(Seq2SeqTrainer):\n",
      "            def get_train_dataloader(self):\n",
      "                return train_loader\n",
      "\n",
      "            def get_eval_dataloader(self, eval_dataset):\n",
      "                return eval_loader\n",
      "\n",
      "            def evaluation_loop(\n",
      "                self,\n",
      "                dataloader,\n",
      "                description: str,\n",
      "                prediction_loss_only: Optional[bool] = None,\n",
      "                ignore_keys: Optional[List[str]] = None,\n",
      "                metric_key_prefix: str = \"eval\",\n",
      "            ) -> EvalLoopOutput:\n",
      "                metrics = model.eval_loss_on_loader(as_any(dataloader))\n",
      "                n_samples = metrics[\"loss_per_ex\"].weight\n",
      "                metrics = {\n",
      "                    f\"{metric_key_prefix}_{k}\": v.mean() for k, v in metrics.items()\n",
      "                }\n",
      "                return EvalLoopOutput(\n",
      "                    predictions=tuple(),\n",
      "                    label_ids=tuple(),\n",
      "                    metrics=metrics,\n",
      "                    num_samples=n_samples,\n",
      "                )\n",
      "\n",
      "        epoch_steps = len(train_loader)\n",
      "        cprint(\"blue\", \"Number of training batches (estimate):\", epoch_steps)\n",
      "        trainer_args = Seq2SeqTrainingArguments(\n",
      "            output_dir=str(train_dir),\n",
      "            overwrite_output_dir=True,\n",
      "            evaluation_strategy=\"epoch\",\n",
      "            save_strategy=\"steps\",\n",
      "<extra_id_0>\n",
      "            logging_steps=max(1, min(1000, epoch_steps // 10)),\n",
      "            num_train_epochs=train_args.max_train_epochs,\n",
      "            save_total_limit=3,\n",
      "            lr_scheduler_type=train_args.lr_scheduler_type,\n",
      "            learning_rate=train_args.learning_rate,\n",
      "            weight_decay=train_args.weight_decay,\n",
      "            metric_for_best_model=\"loss_per_tk\",\n",
      "            greater_is_better=False,\n",
      "            fp16=True,\n",
      "            # load_best_model_at_end=True,\n",
      "            push_to_hub=False,\n",
      "            report_to=[\"wandb\"],\n",
      "            disable_tqdm=True,\n",
      "            # torchdynamo=\"inductor\",  # use compiled model\n",
      "        )\n",
      "\n",
      "        trainer = DynamicTrainer(\n",
      "            self,\n",
      "            trainer_args,\n",
      "            # callbacks=[EarlyStoppingCallback(early_stopping_patience=1)],\n",
      "        )\n",
      "\n",
      "        trainer.train()\n",
      "        save_dir = get_model_dir(trained=True) / training_name\n",
      "        self.save(save_dir)\n",
      "        print(\"Model saved to:\", save_dir)\n",
      "\n",
      "    @torch.no_grad()\n",
      "    @torch.autocast(\"cuda\")\n",
      "    def eval_loss_on_loader(self, dataloader: \"C3DataLoader\"):\n",
      "        core = self\n",
      "        previous = core.training\n",
      "        core.eval()\n",
      "        metrics = dict[str, WeightedSum]()\n",
      "        for batch in dataloader.__iter__():\n",
      "            batch[\"input_ids\"] = batch[\"input_ids\"].to(core.device)\n",
      "            batch[\"labels\"] = batch[\"labels\"].to(core.device)\n",
      "            outputs = core.forward(**batch)\n",
      "            assert isinstance(outputs, Seq2SeqLMOutput)\n",
      "            if CheckNaN:\n",
      "                if outputs.logits.isnan().any():\n",
      "                    print(\"loss:\", not_none(outputs.loss).item())\n",
      "                    print(\"batch:\", batch)\n",
      "                    raise ValueError(\"NaN in logits\")\n",
      "            for k, v in compute_loss_metrics(outputs.logits, batch[\"labels\"]).items():\n",
      "                v = v + metrics.get(k, WeightedSum(0.0, 0))\n",
      "                metrics[k] = v\n",
      "        core.train(mode=previous)\n",
      "\n",
      "        return metrics\n",
      "\n",
      "    @torch.no_grad()\n",
      "    @torch.autocast(\"cuda\")\n",
      "    def eval_on_data(\n",
      "        self,\n",
      "        eval_problems: Sequence[C3Problem],\n",
      "        eval_loader: \"C3DataLoader\",\n",
      "        dec_args: DecodingArgs,\n",
      "        out_dir: Path,\n",
      "        probs_to_save: int = 300,\n",
      "    ):\n",
      "        shutil.rmtree(out_dir, ignore_errors=True)\n",
      "        (out_dir / \"correct\").mkdir(parents=True, exist_ok=True)\n",
      "        (out_dir / \"incorrect\").mkdir(parents=True, exist_ok=True)\n",
      "\n",
      "        save_ids = set(\n",
      "            random_subset(range(0, len(eval_problems)), probs_to_save, rng=42)\n",
      "        )\n",
      "\n",
      "        gen_args = dec_args.to_model_args()\n",
      "        exact_acc = CountedSum(0, 0)\n",
      "        ex_id = 0\n",
      "        for batch in eval_loader:  # type: ignore\n",
      "            out_tks = self.generate(\n",
      "                batch[\"input_ids\"].to(self.device),\n",
      "                references=batch[\"references\"],\n",
      "                query_ref_list=batch[\"query_ref_list\"],\n",
      "                **gen_args,\n",
      "            ).tolist()  # type: ignore\n",
      "            input_ids = batch[\"input_ids\"].tolist()\n",
      "            labels = batch[\"labels\"].tolist()\n",
      "            query_ref_list = batch[\"query_ref_list\"]\n",
      "            for i in range(len(input_ids)):\n",
      "                all_refs = batch[\"references\"]\n",
      "                references = [all_refs[j] for j in query_ref_list[i]]\n",
      "                mp = RetrievalModelPrediction(\n",
      "                    input_ids=remove_pad_ids(input_ids[i]),\n",
      "                    output_ids=remove_pad_ids(out_tks[i]),</s>\n"
     ]
    }
   ],
   "source": [
    "from coeditor.encoding import decode_tokens\n",
    "\n",
    "ex_id = 8\n",
    "input, output = problems[ex_id].to_codet5_format()\n",
    "print_sections(\n",
    "    (\"output\", decode_tokens(output)),\n",
    "    (\"input\", decode_tokens(input)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coeditor.encoding import encode_diffs, _BaseTokenizer, Add_id, decode_tokens\n",
    "from coeditor.c3problem import C3ProblemSimpleSplit, C3ProblemChangeInlining, C3ToCodeCompletion\n",
    "\n",
    "prob_trans = C3ProblemChangeInlining()\n",
    "# %lprun -T output/lprof.txt -f C3ProblemTokenizer.tokenize_problem edits = [e for p in problems for e in encoder.tokenize_problem(p)]\n",
    "new_probs = [p1 for p in problems for p1 in prob_trans.transform(p)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta: TkDelta(\n",
      "  2: ('<add>             len_in = rand.randint(64, 512)', '<add>             len_out = rand.randint(14, 256)', '<add>             input_ids = 5 * torch.ones(1, len_in, dtype=torch.long, device=self.device)', '<del>')\n",
      "  3: ('<del>',)\n",
      "  4: ('<del>',)\n",
      "  7: ('<add>             labels = 5 * torch.ones(1, len_out, dtype=torch.long, device=self.device)', '<del>')\n",
      ")\n",
      "edit range: range(0, 11)\n",
      "transformations: ('split',)\n",
      "--------------------------------------------------------------------------------\n",
      "path: coeditor.model/RetrievalEditorModel.profile_run\n",
      "n_references: 9\n",
      "total_reference_tks: 2957\n",
      "project: temp-1\n",
      "commit: CommitInfo(hash='31e1ac706f81baaf059aaee0787edab67bda6f3b', parents=('105bd32bd2520f951e8003a3e08c82d784029b2d',), msg='Improve C3DataLoader performance. - move _post_process into pmap. - support `disable_unchanged_refs`.')\n",
      "========Ground Truth========\n",
      " <2>:<add>             len_in = rand.randint(64, 512)\n",
      "     <add>             len_out = rand.randint(14, 256)\n",
      "     <add>             input_ids = 5 * torch.ones(1, len_in, dtype=torch.long, device=self.device)\n",
      "     <del>             input_ids = 5 * torch.ones(\n",
      " <3>:<del>                 1, rand.randint(64, 512), dtype=torch.long, device=self.device\n",
      " <4>:<del>             )\n",
      " <7>:<add>             labels = 5 * torch.ones(1, len_out, dtype=torch.long, device=self.device)\n",
      "     <del>             labels = 5 * torch.ones(1, 128, dtype=torch.long, device=self.device)\n",
      "\n",
      "========Main Code========\n",
      "      # module: coeditor.model\n",
      "      class RetrievalEditorModel(T5PreTrainedModel):\n",
      "    +     def profile_run(self, repeats: int = 10, max_refs: int = 20):\n",
      "    -     def profile_run(self, repeats: int = 10, max_refs: int = 10):\n",
      " <0>          rand = random.Random(42)\n",
      " <1>          for i in tqdm(range(repeats), \"test run\"):\n",
      " <2>              input_ids = 5 * torch.ones(\n",
      " <3>                  1, rand.randint(64, 512), dtype=torch.long, device=self.device\n",
      " <4>              )\n",
      " <5>              n_refs = rand.randint(max_refs // 2, max_refs)\n",
      " <6>              references = [[5] * rand.randint(64, 512) for _ in range(n_refs)]\n",
      " <7>              labels = 5 * torch.ones(1, 128, dtype=torch.long, device=self.device)\n",
      " <8>              with torch.autocast(\"cuda\"):\n",
      " <9>                  self.forward(as_any(input_ids), references, labels=as_any(labels))\n",
      "<10>  \n",
      "      \n",
      "===========unchanged ref 0===========\n",
      "    at: coeditor.common\n",
      "        TokenSeq = list[Token]\n",
      "    \n",
      "    at: torch.nn.modules.module.Module\n",
      "        dump_patches: bool = False\n",
      "    \n",
      "        _version: int = 1\n",
      "    \n",
      "        training: bool\n",
      "    \n",
      "        _parameters: Dict[str, Optional[Parameter]]\n",
      "    \n",
      "        _buffers: Dict[str, Optional[Tensor]]\n",
      "    \n",
      "        _non_persistent_buffers_set: Set[str]\n",
      "    \n",
      "        _backward_hooks: Dict[int, Callable]\n",
      "    \n",
      "        _is_full_backward_hook: Optional[bool]\n",
      "    \n",
      "        _forward_hooks: Dict[int, Callable]\n",
      "    \n",
      "        _forward_pre_hooks: Dict[int, Callable]\n",
      "    \n",
      "        _state_dict_hooks: Dict[int, Callable]\n",
      "    \n",
      "        _load_state_dict_pre_hooks: Dict[int, Callable]\n",
      "    \n",
      "        _load_state_dict_post_hooks: Dict[int, Callable]\n",
      "    \n",
      "        _modules: Dict[str, Optional['Module']]\n",
      "    \n",
      "        forward: Callable[..., Any] = _forward_unimplemented\n",
      "    \n",
      "        __call__ : Callable[..., Any] = _call_impl\n",
      "    \n",
      "        T_destination = TypeVar('T_destination', bound=Dict[str, Any])\n",
      "    \n",
      "    at: typing\n",
      "        Sequence = _alias(collections.abc.Sequence, 1)\n",
      "    \n",
      "    \n",
      "===========changed ref 0===========\n",
      "    # module: coeditor.model\n",
      "    class RetrievalEditorModel(T5PreTrainedModel):\n",
      "  -     def encode_token_seqs(\n",
      "  -         self, references: Sequence[TokenSeq] | Sequence[str], pad_id=None\n",
      "  -     ) -> LongTensor:\n",
      "  -         references = [\n",
      "  -             encode_lines_join(ref) if isinstance(ref, str) else ref\n",
      "  -             for ref in references\n",
      "  -         ]\n",
      "  -         out = pad_token_seqs(references, pad_id=pad_id)\n",
      "  -         out = out.to(self.device)\n",
      "  -         return cast(LongTensor, out)\n",
      "  - \n",
      "===========changed ref 1===========\n",
      "    # module: coeditor.c3problem\n",
      "    @dataclass\n",
      "    class C3ProblemTokenizer:\n",
      "        \"\"\"\n",
      "        ## Change log\n",
      "        - 2.6: increase max_ref_tks_sum from 512 * 12 to 512 * 16.\n",
      "        - 2.5: Sort used references by path.\n",
      "        - 2.4: Encode each changed reference individually. Encode signatures for unchanged.\n",
      "        \"\"\"\n",
      "    \n",
      "        VERSION = \"2.6\"\n",
      "        max_ref_tks: int = 512\n",
      "        max_query_tks: int = 512\n",
      "        max_output_tks: int = 256\n",
      "        max_scope_tks: int = 128\n",
      "        max_ref_tks_sum: int = 512 * 16\n",
      "        ref_chunk_overlap: int = 32\n",
      "  +     disable_unchanged_refs: bool = False\n",
      "    \n",
      "===========changed ref 2===========\n",
      "    # module: coeditor._utils\n",
      "    def pmap(\n",
      "        f: Callable[..., T1],\n",
      "        *f_args: Any,\n",
      "        desc: str | None = None,\n",
      "        key_args: Mapping[str, Any] | None = None,\n",
      "        max_workers: int | None = None,\n",
      "        chunksize: int | None = None,\n",
      "        tqdm_args: Mapping[str, Any] | None = None,\n",
      "    ) -> list[T1]:\n",
      "        \"\"\"\n",
      "        Parallel map with progress displaying.\n",
      "        \"\"\"\n",
      "        n = len(f_args[0])\n",
      "        assert_eq(n, *(len(xs) for xs in f_args))\n",
      "    \n",
      "        tqdm_args = dict(tqdm_args) if tqdm_args else {}\n",
      "        tqdm_args.setdefault(\"smoothing\", 0.0)\n",
      "    \n",
      "        if desc is None:\n",
      "            desc = \"pmap: \" + f.__name__\n",
      "    \n",
      "        if key_args is None:\n",
      "            key_args = {}\n",
      "    \n",
      "        if max_workers is None:\n",
      "            max_workers = DefaultWorkers\n",
      "        if max_workers <= 1:\n",
      "            outs = list[T1]()\n",
      "            for i in tqdm(range(n), desc=desc, **tqdm_args):\n",
      "                outs.append(f(*(a[i] for a in f_args), **key_args))\n",
      "            return outs\n",
      "    \n",
      "        if chunksize is None:\n",
      "  +         chunksize = n // (20 * max_workers)\n",
      "  -         chunksize = max(1, n // (20 * max_workers))\n",
      "  +     chunksize = max(1, chunksize)\n",
      "    \n",
      "        tag_f = _TaggedFunc(f, key_args)\n",
      "        arg_tuples = zip(range(n), *f_args)\n",
      "    \n",
      "        with (\n",
      "            multiprocessing.Pool(max_workers) as pool,\n",
      "            tqdm(total=n, desc=desc, **tqdm_args) as pbar,\n",
      "        ):\n",
      "            results = dict[int, T1]()\n",
      "            for i, r in pool.imap_unordered(tag_f, arg_tuples, chunksize=chunksize):\n",
      "                results[i] = r\n",
      "                pbar.update()\n",
      "        return [results[i] for i in range</s>\n",
      "===========changed ref 3===========\n",
      "    # module: coeditor._utils\n",
      "    def pmap(\n",
      "        f: Callable[..., T1],\n",
      "        *f_args: Any,\n",
      "        desc: str | None = None,\n",
      "        key_args: Mapping[str, Any] | None = None,\n",
      "        max_workers: int | None = None,\n",
      "        chunksize: int | None = None,\n",
      "        tqdm_args: Mapping[str, Any] | None = None,\n",
      "    ) -> list[T1]:\n",
      "    # offset: 1\n",
      "    <s>=chunksize):\n",
      "                results[i] = r\n",
      "                pbar.update()\n",
      "        return [results[i] for i in range(n)]\n",
      "    \n",
      "===========changed ref 4===========\n",
      "    # module: coeditor.c3problem\n",
      "    @dataclass\n",
      "    class C3ProblemTokenizer:\n",
      "        def tokenize_problem(\n",
      "            self,\n",
      "            problem: C3Problem,\n",
      "        ) -> TkC3Problem:\n",
      "            span = problem.span\n",
      "    \n",
      "            original: TokenSeq = span.original.tolist()\n",
      "            tk_delta: TkDelta = span.delta\n",
      "            origin_lines = tk_splitlines(original)\n",
      "            edit_start = problem.edit_line_ids[0]\n",
      "            scope_tks = self._encode_headers(span.headers, 0)\n",
      "            input_limit = self.max_query_tks - len(scope_tks)\n",
      "    \n",
      "            chunk_input = TokenSeq()\n",
      "            chunk_output = TokenSeq()\n",
      "            last_line = edit_start\n",
      "    \n",
      "            for i, l in enumerate(problem.edit_line_ids):\n",
      "                for line in origin_lines[last_line + 1 : l]:\n",
      "                    chunk_input.extend(line)\n",
      "                    chunk_input.append(Newline_id)\n",
      "    \n",
      "                chunk_input.append(get_extra_id(i))\n",
      "                if l < len(origin_lines):\n",
      "                    chunk_input.extend(origin_lines[l])\n",
      "                    chunk_input.append(Newline_id)\n",
      "                    last_line = l\n",
      "                line_change = join_list(tk_delta.get_line_change(l), Newline_id)\n",
      "                chunk_output.append(get_extra_id(i))\n",
      "                chunk_output.extend(line_change)\n",
      "                if line_change and line_change[-1]!= Del_id:\n",
      "                    chunk_output.append(Newline_id)\n",
      "                if len(chunk_input) > input_limit:\n",
      "                    break\n",
      "            edit_stop = last_line + 1\n",
      "    \n",
      "            # limit the input size if it's too long\n",
      "            chunk_input = truncate_section(\n",
      "                chunk_input, TruncateAt.Right, input_limit, inplace=True\n",
      "            )\n",
      "            chunk_output = truncate_output_tks(chunk_input, chunk_output)\n",
      "    \n",
      "            # try move some prev_change_tks into the input\n",
      "            above_tks = join_list(origin_lines[:edit_</s>\n",
      "===========changed ref 5===========\n",
      "    # module: coeditor.c3problem\n",
      "    @dataclass\n",
      "    class C3ProblemTokenizer:\n",
      "        def tokenize_problem(\n",
      "            self,\n",
      "            problem: C3Problem,\n",
      "        ) -> TkC3Problem:\n",
      "    # offset: 1\n",
      "    <s> # try move some prev_change_tks into the input\n",
      "            above_tks = join_list(origin_lines[:edit_start] + [TokenSeq()], Newline_id)\n",
      "            above_tks = tk_delta.for_input_range((0, edit_start)).apply_to_change(above_tks)\n",
      "            below_tks = join_list(origin_lines[edit_stop:] + [TokenSeq()], Newline_id)\n",
      "            chunk_input, above_tks, below_tks = self._inline_some_context(\n",
      "                chunk_input, above_tks, below_tks, input_limit\n",
      "            )\n",
      "    \n",
      "            chunk_output = truncate_section(\n",
      "                chunk_output,\n",
      "                TruncateAt.Right,\n",
      "                self.max_output_tks,\n",
      "                add_bos=False,\n",
      "                inplace=True,\n",
      "            )\n",
      "    \n",
      "            above_chunks = break_into_chunks(\n",
      "                above_tks,\n",
      "                lambda i: self._encode_headers(span.headers, -1 - i),\n",
      "                chunk_size=self.max_ref_tks,\n",
      "                overlap=self.ref_chunk_overlap,\n",
      "                right_to_left=True,\n",
      "            )\n",
      "            if not below_tks:\n",
      "                below_chunks = []\n",
      "            else:\n",
      "                below_chunks = break_into_chunks(\n",
      "                    below_tks,\n",
      "                    lambda i: self._encode_headers(span.headers, i + 1),\n",
      "                    chunk_size=self.max_ref_tks,\n",
      "                    overlap=self.ref_chunk_overlap,\n",
      "                )\n",
      "            above_chunks = [\n",
      "                (f\"above chunk {i}\", TkArray.new(chunk))\n",
      "                for i, chunk in enumerate(above_chunks)\n",
      "            ]\n",
      "            below_chunks =</s>\n",
      "===========changed ref 6===========\n",
      "    # module: coeditor.c3problem\n",
      "    @dataclass\n",
      "    class C3ProblemTokenizer:\n",
      "        def tokenize_problem(\n",
      "            self,\n",
      "            problem: C3Problem,\n",
      "        ) -> TkC3Problem:\n",
      "    # offset: 2\n",
      "    <s>            (f\"below chunk {i}\", TkArray.new(chunk))\n",
      "                for i, chunk in enumerate(below_chunks)\n",
      "            ]\n",
      "            all_refs = above_chunks + below_chunks\n",
      "            ref_size_sum = sum(len(ref) for _, ref in all_refs)\n",
      "    \n",
      "            truncated = False\n",
      "            if ref_size_sum < self.max_ref_tks_sum:\n",
      "  +             if not self.disable_unchanged_refs:\n",
      "  +                 unchanged = self._group_encode_unchanged_refs(\n",
      "  -             unchanged = self._group_encode_unchanged_refs(problem.relevant_unchanged)\n",
      "  +                     problem.relevant_unchanged\n",
      "  +                 )\n",
      "  +                 for i, chunk in enumerate(unchanged):\n",
      "  -             for i, chunk in enumerate(unchanged):\n",
      "  +                     all_refs.append((f\"unchanged ref {i}\", chunk))\n",
      "  -                 all_refs.append((f\"unchanged ref {i}\", chunk))\n",
      "            else:\n",
      "                truncated = True\n",
      "    \n",
      "            if ref_size_sum < self.max_ref_tks_sum:\n",
      "                changed = self._group_encode_changed_refs(problem.relevant_changes)\n",
      "                for i, chunk in enumerate(changed):\n",
      "                    all_refs.append((f\"changed ref {i}\", chunk))\n",
      "                ref_size_sum += sum(len(x) for x in changed)\n",
      "            else:\n",
      "                truncated = True\n",
      "    \n",
      "            # take until we hit the limit\n",
      "            ref_size_sum = 0\n",
      "            kept_refs = list[tuple[str, TkArray]]()\n",
      "            for name, ref in all_refs:\n",
      "                if ref_size_sum + len(ref) > self.max_ref_tks_sum:\n",
      "                    truncated = True\n",
      "                    break\n",
      "                ref_size_sum +=</s>\n",
      "===========changed ref 7===========\n",
      "    # module: coeditor.c3problem\n",
      "    @dataclass\n",
      "    class C3ProblemTokenizer:\n",
      "        def tokenize_problem(\n",
      "            self,\n",
      "            problem: C3Problem,\n",
      "        ) -> TkC3Problem:\n",
      "    # offset: 3\n",
      "    <s>ref)\n",
      "                kept_refs.append((name, ref))\n",
      "    \n",
      "            return TkC3Problem(\n",
      "                TkArray.new(chunk_input),\n",
      "                TkArray.new(scope_tks),\n",
      "                TkArray.new(chunk_output),\n",
      "                path=span.headers[-1].path,\n",
      "                change_type=problem.change_type,\n",
      "                named_references=kept_refs,\n",
      "                project=problem.src_info[\"project\"],\n",
      "                commit=problem.src_info[\"commit\"],\n",
      "                truncated=truncated,\n",
      "            )\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "ex_id = 3\n",
    "encoder = C3ProblemTokenizer(current_code_only=False)\n",
    "print(\"delta: \" + str(new_probs[ex_id].span.delta))\n",
    "print(\"edit range: \" + str(new_probs[ex_id].edit_line_ids))\n",
    "print(\"transformations: \" + str(new_probs[ex_id].transformations))\n",
    "edits = [encoder.tokenize_problem(p) for p in new_probs]\n",
    "print(edits[ex_id].show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta: TkDelta(\n",
      "  2: ('<add>             len_in = rand.randint(64, 512)', '<add>             len_out = rand.randint(14, 256)', '<add>             input_ids = 5 * torch.ones(1, len_in, dtype=torch.long, device=self.device)', '<del>')\n",
      "  3: ('<del>',)\n",
      "  4: ('<del>',)\n",
      "  7: ('<add>             labels = 5 * torch.ones(1, len_out, dtype=torch.long, device=self.device)', '<del>')\n",
      ")\n",
      "edit range: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "transformations: ('split',)\n",
      "--------------------------------------------------------------------------------\n",
      "path: coeditor.model/RetrievalEditorModel.profile_run\n",
      "n_references: 7\n",
      "total_reference_tks: 2585\n",
      "project: temp-1\n",
      "commit: CommitInfo(hash='31e1ac706f81baaf059aaee0787edab67bda6f3b', parents=('105bd32bd2520f951e8003a3e08c82d784029b2d',), msg='Improve C3DataLoader performance. - move _post_process into pmap. - support `disable_unchanged_refs`.')\n",
      "========Ground Truth========\n",
      " <2>:<add>             len_in = rand.randint(64, 512)\n",
      "     <add>             len_out = rand.randint(14, 256)\n",
      "     <add>             input_ids = 5 * torch.ones(1, len_in, dtype=torch.long, device=self.device)\n",
      "     <del>             input_ids = 5 * torch.ones(\n",
      " <3>:<del>                 1, rand.randint(64, 512), dtype=torch.long, device=self.device\n",
      " <4>:<del>             )\n",
      " <7>:<add>             labels = 5 * torch.ones(1, len_out, dtype=torch.long, device=self.device)\n",
      "     <del>             labels = 5 * torch.ones(1, 128, dtype=torch.long, device=self.device)\n",
      "\n",
      "========Main Code========\n",
      "      # module: coeditor.model\n",
      "      class RetrievalEditorModel(T5PreTrainedModel):\n",
      "          def profile_run(self, repeats: int = 10, max_refs: int = 20):\n",
      " <0>          rand = random.Random(42)\n",
      " <1>          for i in tqdm(range(repeats), \"test run\"):\n",
      " <2>              input_ids = 5 * torch.ones(\n",
      " <3>                  1, rand.randint(64, 512), dtype=torch.long, device=self.device\n",
      " <4>              )\n",
      " <5>              n_refs = rand.randint(max_refs // 2, max_refs)\n",
      " <6>              references = [[5] * rand.randint(64, 512) for _ in range(n_refs)]\n",
      " <7>              labels = 5 * torch.ones(1, 128, dtype=torch.long, device=self.device)\n",
      " <8>              with torch.autocast(\"cuda\"):\n",
      " <9>                  self.forward(as_any(input_ids), references, labels=as_any(labels))\n",
      "<10>  \n",
      "      \n",
      "===========unchanged ref 0===========\n",
      "    at: coeditor.common\n",
      "        TokenSeq = list[Token]\n",
      "    \n",
      "    at: torch.nn.modules.module.Module\n",
      "        dump_patches: bool = False\n",
      "    \n",
      "        _version: int = 1\n",
      "    \n",
      "        training: bool\n",
      "    \n",
      "        _parameters: Dict[str, Optional[Parameter]]\n",
      "    \n",
      "        _buffers: Dict[str, Optional[Tensor]]\n",
      "    \n",
      "        _non_persistent_buffers_set: Set[str]\n",
      "    \n",
      "        _backward_hooks: Dict[int, Callable]\n",
      "    \n",
      "        _is_full_backward_hook: Optional[bool]\n",
      "    \n",
      "        _forward_hooks: Dict[int, Callable]\n",
      "    \n",
      "        _forward_pre_hooks: Dict[int, Callable]\n",
      "    \n",
      "        _state_dict_hooks: Dict[int, Callable]\n",
      "    \n",
      "        _load_state_dict_pre_hooks: Dict[int, Callable]\n",
      "    \n",
      "        _load_state_dict_post_hooks: Dict[int, Callable]\n",
      "    \n",
      "        _modules: Dict[str, Optional['Module']]\n",
      "    \n",
      "        forward: Callable[..., Any] = _forward_unimplemented\n",
      "    \n",
      "        __call__ : Callable[..., Any] = _call_impl\n",
      "    \n",
      "        T_destination = TypeVar('T_destination', bound=Dict[str, Any])\n",
      "    \n",
      "    at: typing\n",
      "        Sequence = _alias(collections.abc.Sequence, 1)\n",
      "    \n",
      "    \n",
      "===========changed ref 0===========\n",
      "    # module: coeditor.c3problem\n",
      "    @dataclass\n",
      "    class C3ProblemTokenizer:\n",
      "        \"\"\"\n",
      "        ## Change log\n",
      "        - 2.6: increase max_ref_tks_sum from 512 * 12 to 512 * 16.\n",
      "        - 2.5: Sort used references by path.\n",
      "        - 2.4: Encode each changed reference individually. Encode signatures for unchanged.\n",
      "        \"\"\"\n",
      "    \n",
      "        VERSION = \"2.6\"\n",
      "        max_ref_tks: int = 512\n",
      "        max_query_tks: int = 512\n",
      "        max_output_tks: int = 256\n",
      "        max_scope_tks: int = 128\n",
      "        max_ref_tks_sum: int = 512 * 16\n",
      "        ref_chunk_overlap: int = 32\n",
      "        disable_unchanged_refs: bool = False\n",
      "    \n",
      "===========changed ref 1===========\n",
      "    # module: coeditor._utils\n",
      "    def pmap(\n",
      "        f: Callable[..., T1],\n",
      "        *f_args: Any,\n",
      "        desc: str | None = None,\n",
      "        key_args: Mapping[str, Any] | None = None,\n",
      "        max_workers: int | None = None,\n",
      "        chunksize: int | None = None,\n",
      "        tqdm_args: Mapping[str, Any] | None = None,\n",
      "    ) -> list[T1]:\n",
      "        \"\"\"\n",
      "        Parallel map with progress displaying.\n",
      "        \"\"\"\n",
      "        n = len(f_args[0])\n",
      "        assert_eq(n, *(len(xs) for xs in f_args))\n",
      "    \n",
      "        tqdm_args = dict(tqdm_args) if tqdm_args else {}\n",
      "        tqdm_args.setdefault(\"smoothing\", 0.0)\n",
      "    \n",
      "        if desc is None:\n",
      "            desc = \"pmap: \" + f.__name__\n",
      "    \n",
      "        if key_args is None:\n",
      "            key_args = {}\n",
      "    \n",
      "        if max_workers is None:\n",
      "            max_workers = DefaultWorkers\n",
      "        if max_workers <= 1:\n",
      "            outs = list[T1]()\n",
      "            for i in tqdm(range(n), desc=desc, **tqdm_args):\n",
      "                outs.append(f(*(a[i] for a in f_args), **key_args))\n",
      "            return outs\n",
      "    \n",
      "        if chunksize is None:\n",
      "            chunksize = n // (20 * max_workers)\n",
      "        chunksize = max(1, chunksize)\n",
      "    \n",
      "        tag_f = _TaggedFunc(f, key_args)\n",
      "        arg_tuples = zip(range(n), *f_args)\n",
      "    \n",
      "        with (\n",
      "            multiprocessing.Pool(max_workers) as pool,\n",
      "            tqdm(total=n, desc=desc, **tqdm_args) as pbar,\n",
      "        ):\n",
      "            results = dict[int, T1]()\n",
      "            for i, r in pool.imap_unordered(tag_f, arg_tuples, chunksize=chunksize):\n",
      "                results[i] = r\n",
      "                pbar.update()\n",
      "        return [results[i] for i in range(n)]\n",
      "    \n",
      "===========changed ref 2===========\n",
      "    # module: coeditor.c3problem\n",
      "    @dataclass\n",
      "    class C3ProblemTokenizer:\n",
      "        def tokenize_problem(\n",
      "            self,\n",
      "            problem: C3Problem,\n",
      "        ) -> TkC3Problem:\n",
      "            span = problem.span\n",
      "    \n",
      "            original: TokenSeq = span.original.tolist()\n",
      "            tk_delta: TkDelta = span.delta\n",
      "            origin_lines = tk_splitlines(original)\n",
      "            edit_start = problem.edit_line_ids[0]\n",
      "            scope_tks = self._encode_headers(span.headers, 0)\n",
      "            input_limit = self.max_query_tks - len(scope_tks)\n",
      "    \n",
      "            chunk_input = TokenSeq()\n",
      "            chunk_output = TokenSeq()\n",
      "            last_line = edit_start\n",
      "    \n",
      "            for i, l in enumerate(problem.edit_line_ids):\n",
      "                for line in origin_lines[last_line + 1 : l]:\n",
      "                    chunk_input.extend(line)\n",
      "                    chunk_input.append(Newline_id)\n",
      "    \n",
      "                chunk_input.append(get_extra_id(i))\n",
      "                if l < len(origin_lines):\n",
      "                    chunk_input.extend(origin_lines[l])\n",
      "                    chunk_input.append(Newline_id)\n",
      "                    last_line = l\n",
      "                line_change = join_list(tk_delta.get_line_change(l), Newline_id)\n",
      "                chunk_output.append(get_extra_id(i))\n",
      "                chunk_output.extend(line_change)\n",
      "                if line_change and line_change[-1]!= Del_id:\n",
      "                    chunk_output.append(Newline_id)\n",
      "                if len(chunk_input) > input_limit:\n",
      "                    break\n",
      "            edit_stop = last_line + 1\n",
      "    \n",
      "            # limit the input size if it's too long\n",
      "            chunk_input = truncate_section(\n",
      "                chunk_input, TruncateAt.Right, input_limit, inplace=True\n",
      "            )\n",
      "            chunk_output = truncate_output_tks(chunk_input, chunk_output)\n",
      "    \n",
      "            # try move some prev_change_tks into the input\n",
      "            above_tks = join_list(origin_lines[:edit_</s>\n",
      "===========changed ref 3===========\n",
      "    # module: coeditor.c3problem\n",
      "    @dataclass\n",
      "    class C3ProblemTokenizer:\n",
      "        def tokenize_problem(\n",
      "            self,\n",
      "            problem: C3Problem,\n",
      "        ) -> TkC3Problem:\n",
      "    # offset: 1\n",
      "    <s> # try move some prev_change_tks into the input\n",
      "            above_tks = join_list(origin_lines[:edit_start] + [TokenSeq()], Newline_id)\n",
      "            above_tks = tk_delta.for_input_range((0, edit_start)).apply_to_change(above_tks)\n",
      "            below_tks = join_list(origin_lines[edit_stop:] + [TokenSeq()], Newline_id)\n",
      "            chunk_input, above_tks, below_tks = self._inline_some_context(\n",
      "                chunk_input, above_tks, below_tks, input_limit\n",
      "            )\n",
      "    \n",
      "            chunk_output = truncate_section(\n",
      "                chunk_output,\n",
      "                TruncateAt.Right,\n",
      "                self.max_output_tks,\n",
      "                add_bos=False,\n",
      "                inplace=True,\n",
      "            )\n",
      "    \n",
      "            above_chunks = break_into_chunks(\n",
      "                above_tks,\n",
      "                lambda i: self._encode_headers(span.headers, -1 - i),\n",
      "                chunk_size=self.max_ref_tks,\n",
      "                overlap=self.ref_chunk_overlap,\n",
      "                right_to_left=True,\n",
      "            )\n",
      "            if not below_tks:\n",
      "                below_chunks = []\n",
      "            else:\n",
      "                below_chunks = break_into_chunks(\n",
      "                    below_tks,\n",
      "                    lambda i: self._encode_headers(span.headers, i + 1),\n",
      "                    chunk_size=self.max_ref_tks,\n",
      "                    overlap=self.ref_chunk_overlap,\n",
      "                )\n",
      "            above_chunks = [\n",
      "                (f\"above chunk {i}\", TkArray.new(chunk))\n",
      "                for i, chunk in enumerate(above_chunks)\n",
      "            ]\n",
      "            below_chunks =</s>\n",
      "===========changed ref 4===========\n",
      "    # module: coeditor.c3problem\n",
      "    @dataclass\n",
      "    class C3ProblemTokenizer:\n",
      "        def tokenize_problem(\n",
      "            self,\n",
      "            problem: C3Problem,\n",
      "        ) -> TkC3Problem:\n",
      "    # offset: 2\n",
      "    <s>            (f\"below chunk {i}\", TkArray.new(chunk))\n",
      "                for i, chunk in enumerate(below_chunks)\n",
      "            ]\n",
      "            all_refs = above_chunks + below_chunks\n",
      "            ref_size_sum = sum(len(ref) for _, ref in all_refs)\n",
      "    \n",
      "            truncated = False\n",
      "            if ref_size_sum < self.max_ref_tks_sum:\n",
      "                if not self.disable_unchanged_refs:\n",
      "                    unchanged = self._group_encode_unchanged_refs(\n",
      "                        problem.relevant_unchanged\n",
      "                    )\n",
      "                    for i, chunk in enumerate(unchanged):\n",
      "                        all_refs.append((f\"unchanged ref {i}\", chunk))\n",
      "            else:\n",
      "                truncated = True\n",
      "    \n",
      "            if ref_size_sum < self.max_ref_tks_sum:\n",
      "                changed = self._group_encode_changed_refs(problem.relevant_changes)\n",
      "                for i, chunk in enumerate(changed):\n",
      "                    all_refs.append((f\"changed ref {i}\", chunk))\n",
      "                ref_size_sum += sum(len(x) for x in changed)\n",
      "            else:\n",
      "                truncated = True\n",
      "    \n",
      "            # take until we hit the limit\n",
      "            ref_size_sum = 0\n",
      "            kept_refs = list[tuple[str, TkArray]]()\n",
      "            for name, ref in all_refs:\n",
      "                if ref_size_sum + len(ref) > self.max_ref_tks_sum:\n",
      "                    truncated = True\n",
      "                    break\n",
      "                ref_size_sum += len(ref)\n",
      "                kept_refs.append((name, ref))\n",
      "    \n",
      "            return TkC3Problem(\n",
      "                TkArray.new(chunk_input),\n",
      "                TkArray.new(scope_tks),\n",
      "                TkArray.new(chunk_output</s>\n",
      "===========changed ref 5===========\n",
      "    # module: coeditor.c3problem\n",
      "    @dataclass\n",
      "    class C3ProblemTokenizer:\n",
      "        def tokenize_problem(\n",
      "            self,\n",
      "            problem: C3Problem,\n",
      "        ) -> TkC3Problem:\n",
      "    # offset: 3\n",
      "    <s>            path=span.headers[-1].path,\n",
      "                change_type=problem.change_type,\n",
      "                named_references=kept_refs,\n",
      "                project=problem.src_info[\"project\"],\n",
      "                commit=problem.src_info[\"commit\"],\n",
      "                truncated=truncated,\n",
      "            )\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from coeditor.c3problem import _problem_to_current\n",
    "\n",
    "encoder = C3ProblemTokenizer(current_code_only=True)\n",
    "cprob = _problem_to_current(new_probs[ex_id])\n",
    "print(\"delta: \" + str(cprob.span.delta))\n",
    "print(\"edit range: \" + str(cprob.edit_line_ids))\n",
    "print(\"transformations: \" + str(cprob.transformations))\n",
    "print(encoder.tokenize_problem(new_probs[ex_id]).show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.82 s ± 26 ms per loop (mean ± std. dev. of 2 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 2\n",
    "analysis = UsageAnalysis(pyproj, add_implicit_rel_imports=True, record_type_usages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.79 s ± 21.6 ms per loop (mean ± std. dev. of 2 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 2\n",
    "PythonProject.from_root(proj_root())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6ffc72953da4dd16b2e00785be9c4013ef131f465a8658f3921b6634d4eeec8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
