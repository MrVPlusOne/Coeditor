{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from coeditor.common import *\n",
    "import os\n",
    "\n",
    "os.chdir(proj_root())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Modified: \n",
      " \n",
      "    - def apply_edit(edit: Edit, file: File):\n",
      "    + def apply_edit(edit: Edit, text: str) -> str:\n",
      "    -     \"Apply the edit to the content of given file\"\n",
      "    +     \"Apply the edit to the given string\"\n",
      "          # implementation ...\n",
      "     \n",
      "      def suggest_edit(\n",
      "          file: Path, \n",
      "          line: int, \n",
      "          apply: bool = False\n",
      "      ):\n",
      "          suggestion = ...\n",
      "          if apply:\n",
      "    -         apply_edit(suggestion, file)\n",
      "    +         new_code = apply_edit(suggestion, current_code)\n",
      "    +         file.write(new_code)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "from coeditor.history import *\n",
    "\n",
    "old_code = \"\"\"\n",
    "def apply_edit(edit: Edit, file: File):\n",
    "    \"Apply the edit to the content of given file\"\n",
    "    # implementation ...\n",
    "    \n",
    "def suggest_edit(\n",
    "    file: Path, \n",
    "    line: int, \n",
    "    apply: bool = False\n",
    "):\n",
    "    suggestion = ...\n",
    "    if apply:\n",
    "        apply_edit(suggestion, file)\n",
    "\"\"\"\n",
    "\n",
    "new_code = \"\"\"\n",
    "def apply_edit(edit: Edit, text: str) -> str:\n",
    "    \"Apply the edit to the given string\"\n",
    "    # implementation ...\n",
    "    \n",
    "def suggest_edit(\n",
    "    file: Path, \n",
    "    line: int, \n",
    "    apply: bool = False\n",
    "):\n",
    "    suggestion = ...\n",
    "    if apply:\n",
    "        new_code = apply_edit(suggestion, current_code)\n",
    "        file.write(new_code)\n",
    "\"\"\"\n",
    "\n",
    "code_change = Modified(old_code, new_code)\n",
    "print(show_change(code_change))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n",
      "\n",
      " <del> def apply_edit(edit: Edit, file: File):\n",
      " <add> def apply_edit(edit: Edit, text: str) -> str:\n",
      " <del>     \"Apply the edit to the content of given file\"\n",
      " <add>     \"Apply the edit to the given string\"\n",
      "    # implementation...\n",
      "    \n",
      "def suggest_edit(\n",
      "    file: Path, \n",
      "    line: int, \n",
      "    apply: bool = False\n",
      "):\n",
      "<mask_0>    suggestion =...\n",
      "<mask_1>    if apply:\n",
      "<mask_2>        apply_edit(suggestion, file)\n",
      "<mask_3>\n",
      "<mask_4>\n",
      "output:\n",
      "<mask_0><mask_1><mask_2> <del> <mask_3> <add>         new_code = apply_edit(suggestion, current_code)\n",
      " <add>         file.write(new_code)\n",
      "<mask_4>\n"
     ]
    }
   ],
   "source": [
    "from coeditor.encoders import change_tks_to_query_context, change_to_tokens\n",
    "from coeditor.encoding import *\n",
    "\n",
    "query, ctx = change_tks_to_query_context(change_to_tokens(code_change), 10)\n",
    "print(\"input:\")\n",
    "print(decode_tokens(ctx, prettify=True))\n",
    "print(decode_tokens(query[0], prettify=True))\n",
    "print(\"output:\")\n",
    "print(decode_tokens(query[1], prettify=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implement file-level dataset creation.\n",
      "Implement file-level edit encoder.\n",
      "Update installation instructions.\n",
      "Add DevGuide.md.\n",
      "Implement encoding format for CodeT5.\n",
      "Line-diff-based format for encoding edits.\n",
      "Implement EditSelectors.\n",
      "Improve diff visualization.\n",
      "Improve edit context construction.\n",
      "- Bugfix: `from_code_changes` mutates original copy. - Collect only usees in editing ctx. - Improve editing visualization.\n"
     ]
    }
   ],
   "source": [
    "from coeditor.history import *\n",
    "\n",
    "history = get_commit_history(proj_root(), 25)\n",
    "for cinfo in history[:10]:\n",
    "    print(cinfo.msg)\n",
    "\n",
    "# edits = edits_from_commit_history(proj_root(), history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retriving edits:   0%|          | 0/1 [00:00<?, ?it/s]Starting task: Retriving initial project from commit: 3c17c4ea794ce495edd62698a46aa696e384bed1\n",
    "(0.1s) Finished task: Retriving initial project from commit: 3c17c4ea794ce495edd62698a46aa696e384bed1\n",
    "Edits from commits: 100%|██████████| 215/215 [03:32<00:00,  1.01it/s]\n",
    "Retriving edits: 100%|██████████| 1/1 [05:24<00:00, 324.13s/it]\n",
    "Encoding edits: 100%|██████████| 215/215 [05:21<00:00,  1.50s/it]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Modified: \n",
      "    def preds_to_accuracies(\n",
      "        preds: Sequence[Sequence[PythonType]],\n",
      "        dataset: ChunkedDataset,\n",
      "        metric: AccuracyMetric,\n",
      "    ):\n",
      "            cats = [an.cat for info in dataset.chunks_info for an in info.annots_info]\n",
      "            labels = [ty for info in dataset.chunks_info for ty in info.types]\n",
      "    -       poses = [i for info in dataset.chunks_info for i in info.label_ids]\n",
      "            return type_accuracies(\n",
      "                list(seq_flatten(preds)),\n",
      "                labels,\n",
      "                cats,\n",
      "    -           poses,\n",
      "                metric=metric,\n",
      "            )\n"
     ]
    }
   ],
   "source": [
    "from coeditor.history import *\n",
    "from coeditor.encoding import *\n",
    "\n",
    "all_mods = [c for e in edits for c in e.all_elem_changes() if isinstance(c, Modified)]\n",
    "c = all_mods[0]\n",
    "print(show_change(c))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_commit: 49\n",
      "n_add: 339\n",
      "n_del: 246\n",
      "n_mod: 240\n"
     ]
    }
   ],
   "source": [
    "from coeditor.history import *\n",
    "\n",
    "n_add = n_del = n_mod = 0\n",
    "for e in edits:\n",
    "    for c in e.all_elem_changes():\n",
    "        if isinstance(c, Added):\n",
    "            n_add += 1\n",
    "        elif isinstance(c, Deleted):\n",
    "            n_del += 1\n",
    "        elif isinstance(c, Modified):\n",
    "            n_mod += 1\n",
    "        else:\n",
    "            raise ValueError(c)\n",
    "print(\"n_commit:\", len(edits))\n",
    "print(f\"n_add: {n_add}\")\n",
    "print(f\"n_del: {n_del}\")\n",
    "print(f\"n_mod: {n_mod}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting task: Performing intial module-level analysis...\n",
      "(6.4s) Finished task: Performing intial module-level analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing edits: 100%|██████████| 49/49 [03:52<00:00,  4.75s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>count</th>\n",
       "      <th>avg_time</th>\n",
       "      <th>total_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UsageAnalysis</td>\n",
       "      <td>98</td>\n",
       "      <td>1.579122</td>\n",
       "      <td>154.753982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ModuleAnlaysis/Incremental</td>\n",
       "      <td>182</td>\n",
       "      <td>0.412723</td>\n",
       "      <td>75.115604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ModuleAnlaysis/Initial</td>\n",
       "      <td>1</td>\n",
       "      <td>6.426628</td>\n",
       "      <td>6.426628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_select_change_ctx</td>\n",
       "      <td>240</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.001288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         name  count  avg_time  total_time\n",
       "1               UsageAnalysis     98  1.579122  154.753982\n",
       "2  ModuleAnlaysis/Incremental    182  0.412723   75.115604\n",
       "0      ModuleAnlaysis/Initial      1  6.426628    6.426628\n",
       "3          _select_change_ctx    240  0.000005    0.001288"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "analyzed_edits = analyze_edits(edits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modifications: 240\n",
      "User changes: 29\n",
      "Coverage: 12.1%\n"
     ]
    }
   ],
   "source": [
    "selected, all_cedits = select_edits(\n",
    "    analyzed_edits, EditSelectors.api_change_to_callsite\n",
    ")\n",
    "coverage = set[tuple[ProjectPath, str]]()\n",
    "\n",
    "out_file = Path(\"output/api_change_to_callsite.txt\")\n",
    "with open(out_file, \"w\") as f:\n",
    "    for ce in selected:\n",
    "        for c in ce.grouped_ctx_changes[\"users\"]:\n",
    "            coverage.add((get_change_path(c), not_none(ce.commit_info).hash))\n",
    "\n",
    "        ce.pprint(file=f)\n",
    "        print(\"~\" * 50, \"\\n\", file=f)\n",
    "\n",
    "print(\"All modifications:\", len(all_cedits))\n",
    "print(\"User changes:\", len(coverage))\n",
    "print(\"Coverage:\", f\"{len(coverage) / len(all_cedits):.1%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modifications: 240\n",
      "User changes: 156\n",
      "Coverage: 65.0%\n"
     ]
    }
   ],
   "source": [
    "selected2, all_cedits2 = select_edits(\n",
    "    analyzed_edits, EditSelectors.usee_changes_to_user\n",
    ")\n",
    "\n",
    "out_file = Path(\"output/pretrain.txt\")\n",
    "with open(out_file, \"w\") as f:\n",
    "    for ce in selected2:\n",
    "        ce.pprint(file=f)\n",
    "        print(\"~\" * 50, \"\\n\", file=f)\n",
    "\n",
    "print(\"All modifications:\", len(all_cedits2))\n",
    "print(\"User changes:\", len(selected2))\n",
    "print(\"Coverage:\", f\"{len(selected2) / len(all_cedits2):.1%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== End of new contents ====\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"ManyTypes4Py\"\n",
    "\n",
    "result_paths = {\n",
    "    \"CodeT5\": get_eval_dir(dataset, \"\"),\n",
    "    \"TypeT5\": get_eval_dir(\n",
    "        dataset,\n",
    "        \"(implicit_imports, new) model-v7--TrainingConfig(drop_env_types=False, add_implicit_rel_imports=True)\",\n",
    "    ),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_proj = PythonProject.from_root(Path(\"/home/jiayi/Projects/type4py\"))\n",
    "analysis = UsageAnalysis(\n",
    "    ex_proj, add_implicit_rel_imports=True, add_override_usages=True\n",
    ")\n",
    "pretty_print_dict(analysis.get_stats())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot.data import (\n",
    "    create_tokenized_srcsets,\n",
    "    get_tk_dataset_name,\n",
    "    load_tokenized_srcsets,\n",
    "    TypeCheckSettings,\n",
    ")\n",
    "from spot.tokenized_src import PreprocessArgs\n",
    "\n",
    "pre_args = PreprocessArgs()\n",
    "dataset = \"InferTypes4Py\"\n",
    "sdata_name = get_tk_dataset_name(dataset, pre_args, False)\n",
    "sdata_path = get_dataroot() / \"TokenizedSrcSets\" / sdata_name\n",
    "create_tokenized_srcsets(\n",
    "    dataset,\n",
    "    sdata_path,\n",
    "    func_only=False,\n",
    "    pre_args=pre_args,\n",
    ")\n",
    "tk_dataset = load_tokenized_srcsets(sdata_path)\n",
    "tk_dataset[\"test\"].print_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot import proj_root\n",
    "from spot.static_analysis import ProjectPath, UsageAnalysis, PythonProject\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "proj = PythonProject.from_root(proj_root())\n",
    "for caller, callees in UsageAnalysis(proj).user2used.items():\n",
    "    if caller.module == \"spot.static_analysis\":\n",
    "        print(caller)\n",
    "        for callee in callees:\n",
    "            print(\"\\t\", callee.used, \"\" if callee.is_certain else \"  (maybe)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import libcst as cst\n",
    "\n",
    "from spot.tokenized_src import TokenizedSrc, PreprocessArgs\n",
    "from spot.utils import Path, decode_tokens\n",
    "\n",
    "ex_code = '''# document comment 1\n",
    "  # document comment 2\n",
    "\"\"\"String document commnet\"\"\"\n",
    "import os; import spot;\n",
    "from sys import argv, exit\n",
    "# after import\n",
    "@wraps(function)\n",
    "def catch_permission_denied(function):\n",
    "    import some.inner.imports\n",
    "    \"\"\"\n",
    "    Decorator to catch :class:`psycopg2.ProgrammingError` exceptions with the\n",
    "    ``INSUFFICIENT_PRIVILEGE`` error code and rethrow them as\n",
    "    :class:`~werkzeug.exceptions.Forbidden` exceptions instead.\n",
    "    \"\"\"\n",
    "    @wraps(function)\n",
    "    def decorated(x: str, y: int) -> str:\n",
    "        try:\n",
    "            # comment 1\n",
    "            # comment 1 cont\n",
    "            return function(*args, **kwargs)\n",
    "\n",
    "        except InsufficientPrivilege as error:\n",
    "            LOG.error(\"Forbidden: %s\", error) # comment 2\n",
    "            raise Forbidden()\n",
    "\n",
    "    return decorated\n",
    "'''\n",
    "pre_args = PreprocessArgs(stub_in_preamble=True)\n",
    "ex_src = TokenizedSrc.parse(ex_code, Path(\"test_file\"), Path(\"test_repo\"), pre_args)\n",
    "print(decode_tokens(ex_src.tokenized_code))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot.data import src_to_chunks_, CtxArgs, PreprocessArgs\n",
    "from ipywidgets import interactive\n",
    "\n",
    "pre_args = PreprocessArgs(stub_in_preamble=True)\n",
    "ex_src = TokenizedSrc.parse(ex_code, Path(\"test_file\"), Path(\"test_repo\"), pre_args)\n",
    "\n",
    "\n",
    "def print_code(\n",
    "    preamble: int,\n",
    "    left: int,\n",
    "    right: int,\n",
    "    ctx_size: int,\n",
    "    max_labels: int,\n",
    "    chunk_id: int,\n",
    "    inline_prev: bool,\n",
    "):\n",
    "    chunks = []\n",
    "    args = CtxArgs(\n",
    "        ctx_size,\n",
    "        preamble,\n",
    "        left,\n",
    "        right,\n",
    "        max_labels=max_labels,\n",
    "        inline_prev_gold=inline_prev,\n",
    "    )\n",
    "    src_to_chunks_(chunks, [], ex_src, (0, len(ex_src.types)), args)\n",
    "    print(decode_tokens(chunks[chunk_id][\"input_ids\"]))\n",
    "\n",
    "\n",
    "interactive(\n",
    "    print_code,\n",
    "    preamble=(1, 100),\n",
    "    left=(1, 200),\n",
    "    right=(1, 100),\n",
    "    ctx_size=(1, 500),\n",
    "    max_labels=(1, 10),\n",
    "    chunk_id=(0, 1),\n",
    "    inline_prev=True,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f6ffc72953da4dd16b2e00785be9c4013ef131f465a8658f3921b6634d4eeec8"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.venv': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
