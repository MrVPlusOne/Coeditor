{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from spot.utils import *\n",
    "from spot.static_analysis import UsageAnalysis, PythonProject\n",
    "from spot.function_dataset import data_project_from_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "subprocess.run(\n",
    "    [\"git\", \"diff\", \"--name-status\", \"HEAD~1\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Construct edits incrementally from git history.\n",
      "Init commit for the coeditor branch.\n",
      "Rename ElemPath.\n",
      "Get rid of unsued files.\n",
      "Add readme template.\n",
      "Merge branch 'function-prediction'\n",
      "improve pushover.\n",
      "Add unit tests for running model.\n",
      "Update Pipfile and generate new lock file.\n",
      "More experiments.\n",
      "Implement oracle guided decoding.\n",
      "Add the type checking experiments.\n",
      "Ajust accuracy metrics. Store experiment settings.\n",
      "Add Typilus evaluation.\n",
      "Relocate `sigmap_from_file_predictions`.\n",
      "Resolve implicit relative imports.\n",
      "Update training parameters. Model version 6 -> 7.\n",
      "Make override usages implicit.\n",
      "Compute pub API accruacies from file-level preds.\n",
      "Add override usages.\n",
      "Add `remove_final` to the acc_metric.\n",
      "Generalize  implicit constructor usages.\n",
      "Update to model-v6.\n",
      "Improve data format:\n",
      "Update training and testing code.\n",
      "Resolve pytest fixture usages.\n",
      "Handle inner classes properly.\n",
      "Track usages in Python decorators.\n",
      "Add AccuracyMetric.\n",
      "Improve stub generation.\n",
      "Increase max_line_width to 400.\n",
      "Fix type normalization bug.\n",
      "Add infra to analyze incr decoding.\n",
      "Archive outdated scripts.\n",
      "Implement type4py accuracy evaluation code.\n",
      "Implement evaluation code for Type4Py.\n",
      "Update model training script.\n",
      "Modify model input prompt.\n",
      "Track source location info in PythonProject.\n",
      "Update `train_spot_model` and remove unused code.\n",
      "Improve pretty printting of ElemSignatures.\n",
      "Drop `is_quoted` in Python type.\n",
      "Fix test_static_analysis.\n",
      "remove `pyproject.toml` since it confuses vscode.\n",
      "type4py experiments: temp commit.\n",
      "Fix typo: VariableSignature.\n",
      "Make Evalresult store per-project results.\n",
      "1. Overhaul file paths configuration.\n",
      "Tidy up.\n",
      "Add wandb to eval script.\n",
      "Fix mask_types for self parameters.\n",
      "Add decoding evaluation script.\n",
      "Track the usages in argument default expression.\n",
      "Refactor decoding orders into class.\n",
      "Add incremental decoding evaluation.\n",
      "Improve attribute usage analysis.\n",
      "Initial implementation of incremental decoding.\n",
      "Put src data creation logic into functions.\n",
      "Use lightweight stub in functional dataset.\n",
      "Improve module topological sort.\n",
      "Separate model saving dir from datadir.\n",
      "Update accuracy computation:\n",
      "Refactor API for module-level analysis.\n",
      "Merge branch 'callers+signatures' into function-prediction\n",
      "Update input format to use callee signatures.\n",
      "Properly handle name shadowing in SA.\n",
      "Improve functional data format.\n",
      "Generate special usages for dataclasses.\n",
      "Improve functional data generation.\n",
      "Make stub generation keep rhs of globals.\n",
      "Improve functional dataset format.\n",
      "Simplify the handling of star imports.\n",
      "Resolve subclass relations during SA.\n",
      "Add variable usage tracking to SA.\n",
      "Properly handle star imports and class init.\n",
      "Rename some analysis attributes.\n",
      "Turn off early stopping.\n",
      "add drop_env_types as an option.\n",
      "Tweak training config param ordering.\n",
      "Keep typevar def in stub.\n",
      "Add more options to functional dataset gen.\n",
      "Fix model predicton export.\n",
      "Fix src file path.\n",
      "Ignore overload signatures during SA.\n",
      "Static analysis: better handle common methods.\n",
      "Add proper module resolution with ModuleNamespace.\n",
      "Improve usage analysis: avoid repeated usages.\n",
      "Generate function-level dataset.\n",
      "Resolve symlinks in imports.\n",
      "Only record the first usage of each callee.\n",
      "Exclude common methods in method fuzzy match.\n",
      "Add function usage analysis.\n",
      "Make CodePathManager not a trait.\n",
      "Remove macro argumetns in stub generation.\n",
      "Add eval_only in train_model.\n",
      "Fix output sequence length for max_labels=1.\n",
      "Add `inline_prev_gold` for interactive scenario.\n",
      "Implement `stub_in_preamble`.\n",
      "Implement stub generation.\n",
      "Switch to relative imports for consistency.\n"
     ]
    }
   ],
   "source": [
    "from coeditor.history import *\n",
    "\n",
    "history = get_commit_history(proj_root(), 100)\n",
    "for cinfo in history:\n",
    "    print(cinfo.msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriving initial project from commit 8fe4cd6dab0152bb7ab1604e683e82652ebbab0c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retriving commits: 0it [00:00, ?it/s]\rRetriving commits: 2it [00:01,  1.45it/s]\rRetriving commits: 3it [00:02,  1.05it/s]\rRetriving commits: 4it [00:02,  1.54it/s]\rRetriving commits: 5it [00:03,  1.80it/s]\rRetriving commits: 6it [00:03,  2.24it/s]\rRetriving commits: 7it [00:03,  2.12it/s]\rRetriving commits: 8it [00:04,  1.71it/s]\rRetriving commits: 9it [00:04,  2.16it/s]\rRetriving commits: 10it [00:05,  2.65it/s]\rRetriving commits: 11it [00:05,  3.10it/s]\rRetriving commits: 12it [00:06,  1.41it/s]\rRetriving commits: 13it [00:08,  1.00it/s]\rRetriving commits: 14it [00:09,  1.19it/s]\rRetriving commits: 15it [00:09,  1.47it/s]\rRetriving commits: 16it [00:10,  1.19it/s]\rRetriving commits: 17it [00:11,  1.26it/s]\rRetriving commits: 18it [00:12,  1.06it/s]\rRetriving commits: 19it [00:13,  1.02it/s]\rRetriving commits: 20it [00:13,  1.29it/s]\rRetriving commits: 21it [00:14,  1.56it/s]\rRetriving commits: 22it [00:15,  1.15it/s]\rRetriving commits: 23it [00:15,  1.56it/s]\rRetriving commits: 24it [00:17,  1.24it/s]\rRetriving commits: 25it [00:17,  1.24it/s]\rRetriving commits: 26it [00:18,  1.22it/s]\rRetriving commits: 27it [00:19,  1.17it/s]\rRetriving commits: 28it [00:20,  1.02it/s]\rRetriving commits: 29it [00:21,  1.11it/s]\rRetriving commits: 30it [00:22,  1.15it/s]\rRetriving commits: 31it [00:23,  1.01it/s]\rRetriving commits: 32it [00:24,  1.10it/s]\rRetriving commits: 33it [00:26,  1.40s/it]\rRetriving commits: 34it [00:28,  1.36s/it]\rRetriving commits: 35it [00:29,  1.23s/it]\rRetriving commits: 37it [00:30,  1.06s/it]\rRetriving commits: 38it [00:32,  1.21s/it]\rRetriving commits: 39it [00:33,  1.09s/it]\rRetriving commits: 40it [00:33,  1.03it/s]\rRetriving commits: 41it [00:35,  1.19s/it]\rRetriving commits: 42it [00:37,  1.34s/it]\rRetriving commits: 43it [00:38,  1.24s/it]\rRetriving commits: 44it [00:39,  1.11s/it]\rRetriving commits: 45it [00:41,  1.40s/it]\rRetriving commits: 46it [00:41,  1.04s/it]\rRetriving commits: 47it [00:42,  1.03it/s]\rRetriving commits: 48it [00:45,  1.56s/it]\rRetriving commits: 49it [00:45,  1.31s/it]\rRetriving commits: 51it [00:46,  1.07it/s]\rRetriving commits: 52it [00:48,  1.05s/it]\rRetriving commits: 53it [00:49,  1.11s/it]\rRetriving commits: 54it [00:51,  1.45s/it]\rRetriving commits: 55it [00:52,  1.15s/it]\rRetriving commits: 57it [00:52,  1.45it/s]\rRetriving commits: 58it [00:52,  1.74it/s]\rRetriving commits: 59it [00:53,  1.45it/s]\rRetriving commits: 60it [00:54,  1.78it/s]\rRetriving commits: 61it [00:55,  1.41it/s]\rRetriving commits: 62it [00:55,  1.64it/s]\rRetriving commits: 63it [00:58,  1.20s/it]\rRetriving commits: 64it [00:59,  1.20s/it]\rRetriving commits: 65it [01:02,  1.88s/it]\rRetriving commits: 66it [01:02,  1.35s/it]\rRetriving commits: 67it [01:05,  1.76s/it]\rRetriving commits: 68it [01:05,  1.32s/it]\rRetriving commits: 69it [01:06,  1.16s/it]\rRetriving commits: 70it [01:07,  1.03s/it]\rRetriving commits: 71it [01:08,  1.12s/it]\rRetriving commits: 72it [01:09,  1.06s/it]\rRetriving commits: 73it [01:10,  1.02s/it]\rRetriving commits: 74it [01:13,  1.60s/it]\rRetriving commits: 75it [01:15,  1.60s/it]\rRetriving commits: 76it [01:15,  1.34s/it]\rRetriving commits: 77it [01:17,  1.50s/it]\rRetriving commits: 78it [01:18,  1.33s/it]\rRetriving commits: 79it [01:19,  1.09s/it]\rRetriving commits: 80it [01:21,  1.56s/it]\rRetriving commits: 81it [01:23,  1.52s/it]\rRetriving commits: 82it [01:26,  2.00s/it]\rRetriving commits: 83it [01:27,  1.60s/it]\rRetriving commits: 84it [01:29,  1.94s/it]\rRetriving commits: 85it [01:30,  1.51s/it]\rRetriving commits: 86it [01:33,  1.98s/it]\rRetriving commits: 87it [01:35,  1.92s/it]\rRetriving commits: 88it [01:36,  1.69s/it]\rRetriving commits: 89it [01:37,  1.54s/it]\rRetriving commits: 90it [01:41,  2.36s/it]\rRetriving commits: 92it [01:42,  1.33s/it]\rRetriving commits: 93it [01:42,  1.10s/it]\rRetriving commits: 97it [01:43,  1.64it/s]\rRetriving commits: 98it [01:44,  1.38it/s]\rRetriving commits: 99it [01:45,  1.28it/s]\rRetriving commits: 99it [01:45,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "ProjectEdit(msg='Implement stub generation.', changed={'spot.stub_gen': ModuleEdit(added=['OMIT', 'gen_stub', 'remove_empty_lines', 'ClassNamespace.all_elems', 'ClassNamespace.declared_elems', 'StubGenerator.ns_stack', 'StubGenerator.__init__', 'StubGenerator.register_elem', 'StubGenerator.visit_ClassDef', 'StubGenerator.leave_ClassDef', 'StubGenerator.leave_FunctionDef', 'StubGenerator.leave_Annotation', 'StubGenerator.leave_Param', 'StubGenerator.leave_AnnAssign', 'StubGenerator.leave_Assign', 'StubGenerator.leave_Attribute', 'EmptyLineRemove.on_leave'], deleted=[], modified=[])})\n",
      "----------\n",
      "ProjectEdit(msg='Implement `stub_in_preamble`.', changed={'spot.data': ModuleEdit(added=[], deleted=[], modified=['SrcDataset.from_repos', 'get_dataset_name']), 'spot.stub_gen': ModuleEdit(added=[], deleted=['OMIT', 'gen_stub', 'remove_empty_lines', 'ClassNamespace.all_elems', 'ClassNamespace.declared_elems', 'StubGenerator.ns_stack', 'StubGenerator.__init__', 'StubGenerator.register_elem', 'StubGenerator.visit_ClassDef', 'StubGenerator.leave_ClassDef', 'StubGenerator.leave_FunctionDef', 'StubGenerator.leave_Annotation', 'StubGenerator.leave_Param', 'StubGenerator.leave_AnnAssign', 'StubGenerator.leave_Assign', 'StubGenerator.leave_Attribute', 'EmptyLineRemove.on_leave'], modified=[]), 'spot.tokenized_src': ModuleEdit(added=['stub_from_module', 'remove_empty_lines', 'PreprocessArgs.stub_in_preamble', 'ClassNamespace.all_elems', 'ClassNamespace.declared_elems', 'StubGenerator.OMIT', 'StubGenerator.ns_stack', 'StubGenerator.__init__', 'StubGenerator.register_elem', 'StubGenerator.visit_ClassDef', 'StubGenerator.leave_ClassDef', 'StubGenerator.leave_FunctionDef', 'StubGenerator.leave_Annotation', 'StubGenerator.leave_Param', 'StubGenerator.leave_AnnAssign', 'StubGenerator.leave_Assign', 'StubGenerator.leave_Attribute', 'EmptyLineRemove.on_leave'], deleted=[], modified=['preprocess_code', 'TokenizedSrc.parse']), 'spot.train': ModuleEdit(added=['TrainingConfig.stub_in_preamble', 'TrainingConfig.get_preprocess_args'], deleted=[], modified=['TrainingConfig.get_model_name'])})\n",
      "----------\n",
      "ProjectEdit(msg='Add `inline_prev_gold` for interactive scenario.', changed={'spot.data': ModuleEdit(added=['CtxArgs.inline_prev_gold'], deleted=[], modified=['_compute_ctx', 'preds_to_accuracies', 'src_to_chunks_']), 'spot.tokenized_src': ModuleEdit(added=[], deleted=[], modified=['TokenizedSrc.inline_prev_predictions']), 'spot.train': ModuleEdit(added=['TrainingConfig.inline_prev_gold', 'TrainModelWrapper.avg_loss', 'TrainModelWrapper.labels_trained'], deleted=[], modified=['TrainModelWrapper.training_step', 'TrainingConfig.train_ctx_args', 'TrainModelWrapper.__init__'])})\n",
      "----------\n",
      "ProjectEdit(msg='Fix output sequence length for max_labels=1.', changed={'spot.model': ModuleEdit(added=['DecodingArgs.tokens_per_type', 'DecodingArgs.slack_tokens'], deleted=['DecodingArgs.max_tokens_per_type'], modified=['ModelWrapper.predict_on_batch', 'ModelWrapper.predict'])})\n",
      "----------\n",
      "ProjectEdit(msg='Add eval_only in train_model.', changed={'scripts.train_model': ModuleEdit(added=['eval_only', 'wrapper'], deleted=[], modified=[]), 'spot.visualization': ModuleEdit(added=[], deleted=[], modified=['export_preds_on_code'])})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "edits = edits_from_commit_history(proj_root(), history)\n",
    "for e in edits[:5]:\n",
    "    print(\"-\" * 10)\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"ManyTypes4Py\"\n",
    "\n",
    "result_paths = {\n",
    "    \"CodeT5\": get_eval_dir(dataset, \"\"),\n",
    "    \"TypeT5\": get_eval_dir(dataset, \"(implicit_imports, new) model-v7--TrainingConfig(drop_env_types=False, add_implicit_rel_imports=True)\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_proj = PythonProject.from_root(Path(\"/home/jiayi/Projects/type4py\"))\n",
    "analysis = UsageAnalysis(\n",
    "    ex_proj, add_implicit_rel_imports=True, add_override_usages=True\n",
    ")\n",
    "pretty_print_dict(analysis.get_stats())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot.data import (\n",
    "    create_tokenized_srcsets,\n",
    "    get_tk_dataset_name,\n",
    "    load_tokenized_srcsets,\n",
    "    TypeCheckSettings,\n",
    ")\n",
    "from spot.tokenized_src import PreprocessArgs\n",
    "\n",
    "pre_args = PreprocessArgs()\n",
    "dataset = \"InferTypes4Py\"\n",
    "sdata_name = get_tk_dataset_name(dataset, pre_args, False)\n",
    "sdata_path = get_dataroot() / \"TokenizedSrcSets\" / sdata_name\n",
    "create_tokenized_srcsets(\n",
    "    dataset,\n",
    "    sdata_path,\n",
    "    func_only=False,\n",
    "    pre_args=pre_args,\n",
    ")\n",
    "tk_dataset = load_tokenized_srcsets(sdata_path)\n",
    "tk_dataset[\"test\"].print_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot.static_analysis import ProjectPath\n",
    "\n",
    "analysis.user2used[ProjectPath.from_str(\"type4py.type_check/MypyManager._build_tc_cmd\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot.utils import *\n",
    "\n",
    "def show_subtokens(ids: list[int]):\n",
    "    tks = [f\"⦗{x}⦘\" for x in DefaultTokenizer.convert_ids_to_tokens(ids)]\n",
    "    print(\" \".join(tks))\n",
    "\n",
    "ex_code = \"\"\"\n",
    "def eval_on_dataset(\n",
    "    model: <extra_id_0>,\n",
    "    data: <extra_id_1>,\n",
    "    window_size: <extra_id_2> = None,\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "encoding = DefaultTokenizer.encode(ex_code)\n",
    "print(encoding)\n",
    "show_subtokens(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_output = \"\"\"\n",
    "<extra_id_0>ModelWrapper<extra_id_1>TokenizedSrcSet<extra_id_2>Optional[int]\n",
    "\"\"\"\n",
    "print(decoding := DefaultTokenizer.encode(ex_output))\n",
    "show_subtokens(decoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot import proj_root\n",
    "from spot.static_analysis import ProjectPath, UsageAnalysis, PythonProject\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "proj = PythonProject.from_root(proj_root())\n",
    "for caller, callees in UsageAnalysis(proj).user2used.items():\n",
    "    if caller.module == \"spot.static_analysis\":\n",
    "        print(caller)\n",
    "        for callee in callees:\n",
    "            print(\"\\t\", callee.used, \"\" if callee.is_certain else \"  (maybe)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from spot.tokenized_src import PreprocessArgs, proj_root\n",
    "from spot.function_dataset import repo_to_tk_srcs, dataset_from_repos\n",
    "\n",
    "srcs = repo_to_tk_srcs(proj_root(), PreprocessArgs(drop_env_types=True))\n",
    "\n",
    "from spot.data import TokenizedSrcSet, CtxArgs\n",
    "\n",
    "sdata = TokenizedSrcSet(proj_root(), srcs)\n",
    "ctx_args = CtxArgs(1024, 128, 256, 512)\n",
    "cdata = sdata.to_chunks(ctx_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot.utils import *\n",
    "\n",
    "print(decode_tokens(cdata.data[\"input_ids\"][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for src in [s for s in srcs if \"static_analysis/PythonFunction\" in str(s.file)][:10]:\n",
    "    print(f\"======= file: {src.file} ========\")\n",
    "    src.print_code(max_lines=250)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot.data import PreprocessArgs, repr_modified_args\n",
    "\n",
    "repr_modified_args(\n",
    "    TrainingConfig(pre_args=PreprocessArgs(imports_in_preamble=False)), flatten=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot.static_analysis import cst, PythonModule, compute_module_usages, PythonProject\n",
    "\n",
    "code1 = \"\"\"\n",
    "# root.file1\n",
    "\n",
    "# global function\n",
    "def gf(x):\n",
    "    return x * x\n",
    "\n",
    "# with inner function\n",
    "def gf_with_inner(x):\n",
    "    def inner(y):\n",
    "        return y * y\n",
    "    return inner(x)\n",
    "\n",
    "# class\n",
    "class C:\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "    \n",
    "    def foo(self, y):\n",
    "        return self.x + y\n",
    "\n",
    "    @staticmethod\n",
    "    def s_method(x):\n",
    "        return x + 1\n",
    "    \n",
    "\"\"\"\n",
    "code2 = \"\"\"\n",
    "# root.file2\n",
    "from .file1 import gf\n",
    "from root.file1 import gf_with_inner\n",
    "import root.file1\n",
    "import root.file1 as f1\n",
    "\n",
    "def usage1(x):\n",
    "    gf(x) + root.file1.C(5)\n",
    "    foo(5)\n",
    "\n",
    "def usage2(x):\n",
    "    def inner():\n",
    "        1 + gf_with_inner(x)\n",
    "    return inner()\n",
    "\n",
    "def usage_method1(x):\n",
    "    x = f1.C(5)\n",
    "    1 + x.foo(3)\n",
    "\n",
    "def usage_method2(x):\n",
    "    (1 + f1.C(5)).foo(3)\n",
    "\n",
    "def usage_local():\n",
    "    usage1(3)\n",
    "    UsageClass(4)\n",
    "\n",
    "@f1.C(1)\n",
    "def usage_dec():\n",
    "    pass\n",
    "\n",
    "class UsageClass:\n",
    "    def __init__(self, x):\n",
    "        self.x = gf_with_inner(x)\n",
    "        self.y = self.foo(5)\n",
    "\n",
    "    def foo(self, y):\n",
    "        return usage_local(f1.gf(y))\n",
    "\n",
    "    @staticmethod\n",
    "    def s_method(x):\n",
    "        return x\n",
    "\n",
    "class SubClass(UsageClass):\n",
    "    def use(self):\n",
    "        self.foo(5)\n",
    "        f1.C.s_method(5)\n",
    "\"\"\"\n",
    "\n",
    "code3 = \"\"\"\n",
    "# root.file3\n",
    "from .file1 import *\n",
    "\n",
    "def usage1(x):\n",
    "    gf(5)\n",
    "    C(5)\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "project = PythonProject.from_modules(\n",
    "    [\n",
    "        PythonModule.from_cst(cst.parse_module(code1), \"root.file1\"),\n",
    "        PythonModule.from_cst(cst.parse_module(code2), \"root.file2\"),\n",
    "        PythonModule.from_cst(cst.parse_module(code3), \"root.file3\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "for u in compute_module_usages(project.modules[\"root.file3\"]):\n",
    "    print(str(u))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot.static_analysis import build_project_namespaces\n",
    "\n",
    "build_project_namespaces(project)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot.static_analysis import compute_module_usages\n",
    "\n",
    "compute_module_usages(project.modules[\"root.file3\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot.static_analysis import UsageAnalysis, build_project_namespaces\n",
    "\n",
    "build_project_namespaces(project)\n",
    "\n",
    "analysis = UsageAnalysis(project)\n",
    "analysis.caller2callees[ProjectPath(\"root.file2\", \"SubClass.use\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import libcst as cst\n",
    "\n",
    "from spot.tokenized_src import TokenizedSrc, PreprocessArgs\n",
    "from spot.utils import Path, decode_tokens\n",
    "\n",
    "ex_code = '''# document comment 1\n",
    "  # document comment 2\n",
    "\"\"\"String document commnet\"\"\"\n",
    "import os; import spot;\n",
    "from sys import argv, exit\n",
    "# after import\n",
    "@wraps(function)\n",
    "def catch_permission_denied(function):\n",
    "    import some.inner.imports\n",
    "    \"\"\"\n",
    "    Decorator to catch :class:`psycopg2.ProgrammingError` exceptions with the\n",
    "    ``INSUFFICIENT_PRIVILEGE`` error code and rethrow them as\n",
    "    :class:`~werkzeug.exceptions.Forbidden` exceptions instead.\n",
    "    \"\"\"\n",
    "    @wraps(function)\n",
    "    def decorated(x: str, y: int) -> str:\n",
    "        try:\n",
    "            # comment 1\n",
    "            # comment 1 cont\n",
    "            return function(*args, **kwargs)\n",
    "\n",
    "        except InsufficientPrivilege as error:\n",
    "            LOG.error(\"Forbidden: %s\", error) # comment 2\n",
    "            raise Forbidden()\n",
    "\n",
    "    return decorated\n",
    "'''\n",
    "pre_args = PreprocessArgs(stub_in_preamble=True)\n",
    "ex_src = TokenizedSrc.parse(ex_code, Path(\"test_file\"), Path(\"test_repo\"), pre_args)\n",
    "print(decode_tokens(ex_src.tokenized_code))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot.data import src_to_chunks_, CtxArgs, PreprocessArgs\n",
    "from ipywidgets import interactive\n",
    "\n",
    "pre_args = PreprocessArgs(stub_in_preamble=True)\n",
    "ex_src = TokenizedSrc.parse(ex_code, Path(\"test_file\"), Path(\"test_repo\"), pre_args)\n",
    "\n",
    "\n",
    "def print_code(\n",
    "    preamble: int,\n",
    "    left: int,\n",
    "    right: int,\n",
    "    ctx_size: int,\n",
    "    max_labels: int,\n",
    "    chunk_id: int,\n",
    "    inline_prev: bool,\n",
    "):\n",
    "    chunks = []\n",
    "    args = CtxArgs(\n",
    "        ctx_size,\n",
    "        preamble,\n",
    "        left,\n",
    "        right,\n",
    "        max_labels=max_labels,\n",
    "        inline_prev_gold=inline_prev,\n",
    "    )\n",
    "    src_to_chunks_(chunks, [], ex_src, (0, len(ex_src.types)), args)\n",
    "    print(decode_tokens(chunks[chunk_id][\"input_ids\"]))\n",
    "\n",
    "\n",
    "interactive(\n",
    "    print_code,\n",
    "    preamble=(1, 100),\n",
    "    left=(1, 200),\n",
    "    right=(1, 100),\n",
    "    ctx_size=(1, 500),\n",
    "    max_labels=(1, 10),\n",
    "    chunk_id=(0, 1),\n",
    "    inline_prev=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "from spot.data import GitRepo, ModuleRemapUnpickler\n",
    "from spot.type_env import (\n",
    "    AnnotPath,\n",
    "    MypyChecker,\n",
    "    SelectAnnotations,\n",
    "    TypeInfAction,\n",
    "    TypeInfEnv,\n",
    "    TypeInfState,\n",
    "    mypy_checker,\n",
    ")\n",
    "from spot.utils import cst, proj_root, read_file, seq_flatten, tqdm, write_file\n",
    "\n",
    "os.chdir(proj_root())\n",
    "\n",
    "datadir = Path(os.getenv(\"datadir\"))\n",
    "repos_dir = datadir / \"SPOT-data/repos\"\n",
    "\n",
    "useful_repos_path = proj_root() / \"scripts\" / \"useful_repos.pkl\"\n",
    "rename_module = lambda n: \"spot.data\" if n == \"spot.data_prepare\" else n\n",
    "with useful_repos_path.open(\"rb\") as f:\n",
    "    useful_repos: list[GitRepo] = ModuleRemapUnpickler(f, rename_module).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading pre-trained model and tokenizer\n",
    "from spot.utils import get_data_dir\n",
    "\n",
    "model_dir = \"Salesforce/codet5-base\"\n",
    "# model_dir = datadir / \"checkpoints/saved/SPOT-CodeT5-no_margin/\"\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    DataCollatorForSeq2Seq,\n",
    "    RobertaTokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    ")\n",
    "from transformers.models.t5 import T5ForConditionalGeneration\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer: RobertaTokenizer = RobertaTokenizer.from_pretrained(model_dir)\n",
    "model: T5ForConditionalGeneration = T5ForConditionalGeneration.from_pretrained(\n",
    "    model_dir\n",
    ").to(device)\n",
    "max_target_length = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot.data import mask_type_annots, output_ids_as_types, tokenize_masked\n",
    "\n",
    "test_code = \"\"\"\n",
    "@dataclass\n",
    "class GitRepo:\n",
    "    author: str\n",
    "    name: str\n",
    "    url: str\n",
    "    stars: int\n",
    "    forks: int\n",
    "\n",
    "    def authorname(self):\n",
    "        return self.author + \"__\" + self.name\n",
    "\n",
    "    def repo_dir(self, repos_dir: Path) -> Path:\n",
    "        return repos_dir / \"downloaded\" / self.authorname()\n",
    "\n",
    "    def download(self, repos_dir: Path, timeout=None) -> bool:\n",
    "        pass\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def run_model(code: str, num_beams=16):\n",
    "    masked = mask_type_annots((Path(\"no_source\"), code))\n",
    "    tks = tokenize_masked(masked, tokenizer, device)\n",
    "    input_ids = tks[\"input_ids\"]\n",
    "    with torch.no_grad():\n",
    "        loss = model.forward(**tks).loss\n",
    "        dec = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_target_length,\n",
    "            num_beams=num_beams,\n",
    "            # do_sample=True,\n",
    "        )[0]\n",
    "    return {\n",
    "        \"loss\": loss,\n",
    "        \"predicted_types\": output_ids_as_types(dec, tokenizer),\n",
    "        \"labels\": output_ids_as_types(tks[\"labels\"][0], tokenizer),\n",
    "        \"generation\": tokenizer.decode(dec),\n",
    "        \"input_ids\": input_ids[0],\n",
    "        \"output_ids\": dec,\n",
    "        \"annots_info\": masked[\"annots_info\"],\n",
    "    }\n",
    "\n",
    "\n",
    "result = run_model(test_code, num_beams=10)\n",
    "result[\"loss\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot import PythonType\n",
    "from spot.type_env import apply_annotations\n",
    "\n",
    "\n",
    "def type_to_annot(ty: PythonType) -> str:\n",
    "    return cst.Annotation(cst.parse_expression(str(ty)))\n",
    "\n",
    "\n",
    "def run_aug_model(src: Path, cwd: Path):\n",
    "    result = run_model(read_file(src), num_beams=10)\n",
    "    pred_annots = {\n",
    "        info.path: type_to_annot(t)\n",
    "        for info, t in zip(result[\"annots_info\"], result[\"predicted_types\"])\n",
    "    }\n",
    "    m1 = apply_annotations(cst.parse_module(read_file(src)), pred_annots)\n",
    "    write_file(src, m1.code)\n",
    "    checker_r = MypyChecker.check_project(src, cwd)\n",
    "    pos_to_preds = {\n",
    "        info.annot_range: str(ty)\n",
    "        for info, ty in zip(result[\"annots_info\"], result[\"predicted_types\"])\n",
    "    }\n",
    "    return {\n",
    "        \"model_result\": result,\n",
    "        \"module\": m1,\n",
    "        \"checker_feedback\": checker_r,\n",
    "        \"pos_to_preds\": pos_to_preds,\n",
    "    }\n",
    "\n",
    "\n",
    "aug_r = run_aug_model(inference_dir / \"env_code_2.py\", inference_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot.utils import patch_code_with_extra\n",
    "\n",
    "print(\"---- predicted types ----\")\n",
    "print(aug_r[\"model_result\"][\"predicted_types\"])\n",
    "print(\"---- model output ----\")\n",
    "print(tokenizer.decode(aug_r[\"model_result\"][\"output_ids\"], skip_special_tokens=False))\n",
    "print(\"---- checker_feedback ----\")\n",
    "print(aug_r[\"checker_feedback\"].output_str)\n",
    "\n",
    "print(\"---- new input ----\")\n",
    "new_input = patch_code_with_extra(\n",
    "    aug_r[\"module\"].code,\n",
    "    aug_r[\"pos_to_preds\"],\n",
    "    aug_r[\"checker_feedback\"].error_dict[\"env_code_2.py\"],\n",
    ")\n",
    "print(new_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from spot.utils import Path, run_long_task, DefaultTokenizer, not_none, CountedAcc\n",
    "from spot import proj_root\n",
    "from spot.function_dataset import guess_src_root\n",
    "\n",
    "datadir = Path(not_none(os.getenv(\"datadir\")))\n",
    "repos_dir = datadir / \"SPOT-data/repos/\"\n",
    "\n",
    "repos_split_path = proj_root() / \"data/repos_split.pkl\"\n",
    "with repos_split_path.open(\"rb\") as f:\n",
    "    repos_split = pickle.load(f)\n",
    "\n",
    "root_is_src = list[bool]()\n",
    "for repo in repos_split[\"train\"]:\n",
    "    rd = repo.repo_dir(repos_dir)\n",
    "    root_is_src.append(guess_src_root(rd).name == \"src\")\n",
    "\n",
    "CountedAcc(sum(root_is_src), len(root_is_src))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_in_root = 0\n",
    "package_in_root = 0\n",
    "setup_in_root = 0\n",
    "n_proj = 0\n",
    "\n",
    "weird_repos = []\n",
    "setup_files = []\n",
    "\n",
    "for repo in repos_split[\"train\"]:\n",
    "    rd: Path = repo.repo_dir(repos_dir)\n",
    "    n_proj += 1\n",
    "    files = list(rd.iterdir())\n",
    "    if rd / \"src\" in files:\n",
    "        src_in_root += 1\n",
    "    elif rd / (pname := rd.name.split(\"__\")[-1]) in files:\n",
    "        package_in_root += 1\n",
    "    elif rd / \"setup.cfg\" in files:\n",
    "        setup_in_root += 1\n",
    "        setup_files.append(rd / \"setup.cfg\")\n",
    "    else:\n",
    "        weird_repos.append(repo)\n",
    "\n",
    "print(\"n_projects:\", n_proj)\n",
    "print(\"src_in_root:\", src_in_root)\n",
    "print(\"package_in_root:\", package_in_root)\n",
    "print(\"setup_in_root:\", setup_in_root)\n",
    "print(\"weird_repos:\", len(weird_repos))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for repo in weird_repos[:10]:\n",
    "    rd: Path = repo.repo_dir(repos_dir)\n",
    "    print(\"Repo:\", rd.relative_to(repos_dir))\n",
    "    for f in rd.iterdir():\n",
    "        print(f.relative_to(rd))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f6ffc72953da4dd16b2e00785be9c4013ef131f465a8658f3921b6634d4eeec8"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.venv': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
