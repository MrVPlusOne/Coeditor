{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from coeditor.common import *\n",
    "import os\n",
    "\n",
    "os.chdir(proj_root())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implement file-level dataset creation.\n",
      "Implement file-level edit encoder.\n",
      "Update installation instructions.\n",
      "Add DevGuide.md.\n",
      "Implement encoding format for CodeT5.\n",
      "Line-diff-based format for encoding edits.\n",
      "Implement EditSelectors.\n",
      "Improve diff visualization.\n",
      "Improve edit context construction.\n",
      "- Bugfix: `from_code_changes` mutates original copy. - Collect only usees in editing ctx. - Improve editing visualization.\n"
     ]
    }
   ],
   "source": [
    "from coeditor.history import *\n",
    "\n",
    "history = get_commit_history(proj_root(), 25)\n",
    "for cinfo in history[:10]:\n",
    "    print(cinfo.msg)\n",
    "\n",
    "# edits = edits_from_commit_history(proj_root(), history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coeditor.dataset import dataset_from_projects\n",
    "\n",
    "dataset = dataset_from_projects([proj_root()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Training Example: spot.type_checking/collect_annotations --------------------\n",
      "Input:\n",
      "    from dataclasses import dataclass\n",
      "    from distutils.log import error\n",
      "     <add> from logging import warn\n",
      "    from posixpath import dirname, realpath\n",
      "    import re\n",
      "    from typing import Iterable\n",
      "    from.utils import *\n",
      "    import subprocess\n",
      "\n",
      "\n",
      "     <add> @dataclass(order=True, unsafe_hash=True)\n",
      "     <del> @dataclass(frozen=True)\n",
      "    class AnnotPath:\n",
      "\n",
      "        value: Tuple[str,...]\n",
      "\n",
      "     <add>     def __repr__(self):\n",
      "     <add>         return f\"AnnotPath('{'.'.join(self.value)}')\"\n",
      "     <add> \n",
      "     <add>     def __str__(self):\n",
      "     <add>         return f\"'{'.'.join(self.value)}'\"\n",
      "     <add> \n",
      "     <add> \n",
      "    def annot_path(*segs: str) -> AnnotPath:\n",
      "        return AnnotPath(tuple(segs))\n",
      "     <add> \n",
      "\n",
      "    <extra_id_0>def collect_annotations(code: cst.CSTNode) -> Dict[AnnotPath, Optional[cst.Annotation]]:\n",
      "    <extra_id_1>    collector = AnnotCollector()\n",
      "    <extra_id_2>    code.visit(collector)\n",
      "    <extra_id_3>\n",
      "\n",
      "\n",
      "     <add> def apply_annotations(code: cst.CSTNode, annots: dict[AnnotPath, cst.Annotation]):\n",
      "     <del> def apply_annotations(code: cst.CSTNode, annots: Dict[AnnotPath, cst.Annotation]):\n",
      "        applier = AnnotApplier(annots)\n",
      "        return code.visit(applier)\n",
      "\n",
      "\n",
      "    class AnnotCollector(cst.CSTVisitor):\n",
      "        def __init__(self):\n",
      "            self.stack: List[str] = []\n",
      "     <add>         self.annot_paths: List[AnnotPath] = []\n",
      "     <add>         self.annotations: Dict[AnnotPath, cst.Annotation] = {}\n",
      "     <del>         self.annotations: Dict[AnnotPath, Optional[cst.Annotation]] = {}\n",
      "\n",
      "        def on_visit(self, node):\n",
      "            if (\n",
      "                isinstance(node, cst.FunctionDef)\n",
      "                or isinstance(node, cst.ClassDef)\n",
      "                or isinstance(node, cst.Param)\n",
      "            ):\n",
      "                self.stack.append(node.name.value)\n",
      "            elif isinstance(node, cst.AnnAssign):\n",
      "                self.stack.append(node.target.value)\n",
      "            return super().on_visit(node)\n",
      "\n",
      "        def on_leave(self, node):\n",
      "            r = super().on_leave(node)\n",
      "            if (\n",
      "                isinstance(node, cst.FunctionDef)\n",
      "                or isinstance(node, cst.ClassDef)\n",
      "                or isinstance(node, cst.Param)\n",
      "                or isinstance(node, cst.AnnAssign)\n",
      "            ):\n",
      "                self.stack.pop()\n",
      "            return r\n",
      "\n",
      "        def _current_path(self):\n",
      "            return AnnotPath(tuple(self.stack))\n",
      "\n",
      "     <add>     def _record_annot(self, annot: Union[cst.Annotation, None]):\n",
      "     <add>         path = self._current_path()\n",
      "     <add>         self.annot_paths.append(path)\n",
      "     <add>         if annot is not None:\n",
      "     <add>             self.annotations[path] = annot\n",
      "     <add> \n",
      "        def visit_FunctionDef(self, node: cst.FunctionDef):\n",
      "            self.stack.append(SpecialNames.Return)\n",
      "     <add>         self._record_annot(node.returns)\n",
      "     <del>         self.annotations[self._current_path()] = node.returns\n",
      "            self.stack.pop()\n",
      "\n",
      "        def visit_Param(self, node: cst.Param):\n",
      "     <add>         self._record_annot(node.annotation)\n",
      "     <del>         self.annotations[self._current_path()] = node.annotation\n",
      "\n",
      "     <add>     def visit_AnnAssign(self, node: cst.AnnAssign):\n",
      "     <add>         self._record_annot(node.annotation)\n",
      "     <del>     def visit_AnnAssign(self, ndoe: cst.AnnAssign):\n",
      "     <del>         self.annotations[self._current_path()] = ndoe.annotation\n",
      "\n",
      "\n",
      "    class AnnotApplier(cst.CSTTransformer):\n",
      "        def __init__(self, annots: Dict[AnnotPath, cst.Annotation]):\n",
      "            self.annots = annots\n",
      "            self.stack: List[str] = []\n",
      "            self.prefixes: Set[Tuple[str,...]] = set()\n",
      "            for path in annots.keys():\n",
      "                self.prefixes.update(path.value[0:i] for i in range(len(path.value) + 1))\n",
      "\n",
      "        def _current_path(self):\n",
      "            return AnnotPath(tuple(self.stack))\n",
      "\n",
      "        def on_visit(self, node):\n",
      "            if (\n",
      "                isinstance(node, cst.FunctionDef)\n",
      "                or isinstance(node, cst.ClassDef)\n",
      "                or isinstance(node, cst.Param)\n",
      "            ):\n",
      "                self.stack.append(node.name.value)\n",
      "            elif isinstance(node, cst.AnnAssign):\n",
      "                self.stack.append(node.target.value)\n",
      "            if tuple(self.stack) not in self.prefixes:\n",
      "                return False\n",
      "            return super().on_visit(node)\n",
      "\n",
      "        def on_leave(self, node, updated):\n",
      "            r = super().on_leave(node, updated)\n",
      "            if (\n",
      "                isinstance(node, cst.FunctionDef)\n",
      "                or isinstance(node, cst.ClassDef)\n",
      "                or isinstance(node, cst.Param)\n",
      "                or isinstance(node, cst.AnnAssign)\n",
      "            ):\n",
      "                self.stack.pop()\n",
      "            return r\n",
      "\n",
      "        def leave_FunctionDef(\n",
      "            self, node: cst.FunctionDef, updated: cst.FunctionDef\n",
      "        ) -> cst.FunctionDef:\n",
      "            self.stack.append(SpecialNames.Return)\n",
      "            patch = self.annots.get(self._current_path())\n",
      "            self.stack.pop()\n",
      "            return updated if patch is None else updated.with_changes(returns=patch)\n",
      "\n",
      "        def leave_Param(self, node: cst.Param, updated: cst.Param) -> cst.Param:\n",
      "            patch = self.annots.get(self._current_path())\n",
      "            return updated if patch is None else updated.with_changes(annotation=patch)\n",
      "\n",
      "\n",
      "     <add> @dataclass\n",
      "     <add> class MypyResult:\n",
      "     <add>     num_errors: int\n",
      "     <add>     num_error_dict: Dict[str, int]\n",
      "     <add>     output_str: str\n",
      "     <add> \n",
      "     <add> \n",
      "    class MypyChecker:\n",
      "\n",
      "        def __init__(self, dmypy_path, code_dir) -> None:\n",
      "            self.code_dir = realpath(code_dir)\n",
      "            self.dmypy_path = realpath(dmypy_path)\n",
      "     <add>         subprocess.run(\n",
      "     <add>             [\n",
      "     <add>                 \"python\",\n",
      "     <add>                 self.dmypy_path,\n",
      "     <add>                 \"start\",\n",
      "     <add>                 \"--\",\n",
      "     <add>                 \"--follow-imports=skip\",\n",
      "     <add>             ],\n",
      "     <add>             cwd=self.code_dir,\n",
      "     <add>         )\n",
      "     <add>         subprocess.run(\n",
      "     <add>             [\"python\", self.dmypy_path, \"check\", self.code_dir],\n",
      "     <add>             cwd=self.code_dir,\n",
      "     <del>         self.mypy_version = subprocess.run(\n",
      "     <del>             [\"python\", self.dmypy_path, \"-V\"],\n",
      "                capture_output=True,\n",
      "     <add>         )\n",
      "     <del>             text=True,\n",
      "     <del>             cwd=self.code_dir,\n",
      "     <del>         ).stdout\n",
      "\n",
      "     <add>     def close(self):\n",
      "     <del>     def stop_daemon(self):\n",
      "            subprocess.run(\n",
      "                [\"python\", self.dmypy_path, \"stop\"],\n",
      "                cwd=self.code_dir,\n",
      "            )\n",
      "\n",
      "     <add>     def check_code_dir(self) -> MypyResult:\n",
      "     <add>         return self._run_mypy([\"python\", self.dmypy_path, \"check\", self.code_dir])\n",
      "     <del>     def __del__(self):\n",
      "     <del>         self.stop_daemon()\n",
      "\n",
      "     <add> \n",
      "     <add>     def recheck_files(self, *updated_files: str) -> MypyResult:\n",
      "     <add>         return self._run_mypy([\"python\", self.dmypy_path, \"recheck\", \"--update\", *updated_files])\n",
      "     <add>         \n",
      "     <add> \n",
      "     <add>     def _run_mypy(self, cmd: list[str]) -> MypyResult:\n",
      "     <del>     def check_file(self, fpath):\n",
      "     <del>         print(\"chekcing!\")\n",
      "     <del>         cmd = [\"python\", self.dmypy_path, \"run\", \"--\", fpath]\n",
      "            result = subprocess.run(\n",
      "                cmd,\n",
      "                capture_output=True,\n",
      "                text=True,\n",
      "                cwd=self.code_dir,\n",
      "            )\n",
      "            lines = result.stdout.splitlines()\n",
      "     <add>         assert (\n",
      "     <add>             len(lines) > 0\n",
      "     <add>         ), f\"mypy failed. Command: `{' '.join(cmd)}`\\nError: {result.stderr}\"\n",
      "     <add>         num_error_dict: dict[str, int] = {}\n",
      "     <del>         assert len(lines) > 0, f\"mypy failed. Error: {result.stderr}\"\n",
      "     <del>         num_error_dict = {}\n",
      "            for l in lines:\n",
      "                m = re.match(r\"(.*\\.py):\\d+: error:.+\", l)\n",
      "                if m is not None:\n",
      "                    num_error_dict[m.group(1)] = num_error_dict.get(m.group(1), 0) + 1\n",
      "\n",
      "            m = re.match(r\"Found (\\d+) errors? in\", lines[-1])\n",
      "            if m is None:\n",
      "                num_errors = 0\n",
      "            else:\n",
      "                num_errors = int(m.group(1))\n",
      "\n",
      "            total_errors = sum(num_error_dict.values())\n",
      "            assert (\n",
      "                num_errors == total_errors\n",
      "            ), f\"{num_errors}!= {total_errors}. mypy output: {result.stdout}\"\n",
      "            return MypyResult(num_errors, num_error_dict, result.stdout)\n",
      "\n",
      "\n",
      "    @dataclass\n",
      "     <add> class mypy_checker:\n",
      "     <add> \n",
      "     <add>     dmypy_path: str\n",
      "     <add>     code_dir: str\n",
      "     <add> \n",
      "     <add>     def __enter__(self):\n",
      "     <add>         self.checker = MypyChecker(self.dmypy_path, self.code_dir)\n",
      "     <add>         return self.checker\n",
      "     <add> \n",
      "     <add>     def __exit__(self, exc_type, exc_val, exc_tb):\n",
      "     <add>         self.checker.close() <del> class MypyResult:\n",
      "     <del>     num_errors: int\n",
      "     <del>     num_error_dict: Dict[str, int]\n",
      "     <del>     output_str: str\n",
      "\n",
      "Output:\n",
      "    <extra_id_0> <add> def collect_annotations(\n",
      "     <add>     code: cst.CSTNode,\n",
      "     <add> ) -> Tuple[list[AnnotPath], dict[AnnotPath, cst.Annotation]]:\n",
      "     <del> <extra_id_1><extra_id_2><extra_id_3> <add>     return collector.annot_paths, collector.annotations <del>     return collector.annotations\n"
     ]
    }
   ],
   "source": [
    "dataset.edits[0].print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retriving edits:   0%|          | 0/1 [00:00<?, ?it/s]Starting task: Retriving initial project from commit: 3c17c4ea794ce495edd62698a46aa696e384bed1\n",
    "(0.1s) Finished task: Retriving initial project from commit: 3c17c4ea794ce495edd62698a46aa696e384bed1\n",
    "Edits from commits: 100%|██████████| 215/215 [03:32<00:00,  1.01it/s]\n",
    "Retriving edits: 100%|██████████| 1/1 [05:24<00:00, 324.13s/it]\n",
    "Encoding edits: 100%|██████████| 215/215 [05:21<00:00,  1.50s/it]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Training Example: spot.data/GitRepo.collect_annotations --------------------\n",
      "Input:\n",
      "    import copy\n",
      "    import multiprocessing\n",
      "    import pickle\n",
      "    import shutil\n",
      "    import subprocess\n",
      "    import warnings\n",
      "    from dataclasses import dataclass\n",
      "    from datetime import datetime\n",
      "    from typing import *\n",
      "\n",
      "    import dateparser\n",
      "    from datasets import Dataset\n",
      "\n",
      "    from.tokenized_src import *\n",
      "    from.type_check import TypeCheckArgs\n",
      "    from.type_env import (\n",
      "        AccuracyMetric,\n",
      "        AnnotCat,\n",
      "        AnnotInfo,\n",
      "        AnnotPath,\n",
      "        MypyChecker,\n",
      "        MypyFeedback,\n",
      "        PythonType,\n",
      "        collect_annots_info,\n",
      "        normalize_type,\n",
      "        parse_type_expr,\n",
      "        parse_type_from_ast,\n",
      "        type_accuracies,\n",
      "    )\n",
      "    from.utils import *\n",
      "\n",
      "    warnings.filterwarnings(\n",
      "        \"ignore\",\n",
      "        message=\"The localize method is no longer necessary, as this time zone supports the fold attribute\",\n",
      "    )\n",
      "\n",
      "\n",
      "    class TypeCheckSettings:\n",
      "        temp_path: str = \"Default\"\n",
      "\n",
      "\n",
      "    @dataclass\n",
      "    class GitRepo:\n",
      "        author: str\n",
      "        name: str\n",
      "        url: str\n",
      "        stars: int\n",
      "        forks: int\n",
      "        description: str = \"\"\n",
      "        lines_of_code: Optional[int] = None\n",
      "        last_update: Optional[datetime] = None\n",
      "        n_type_annots: Optional[int] = None\n",
      "        n_type_places: Optional[int] = None\n",
      "\n",
      "        def authorname(self):\n",
      "            return self.author + \"__\" + self.name\n",
      "\n",
      "        def repo_dir(self, repos_dir: Path) -> Path:\n",
      "            return repos_dir / \"downloaded\" / self.authorname()\n",
      "\n",
      "        def download(self, repos_dir: Path, timeout=None) -> bool:\n",
      "            subprocess.run(\n",
      "                [\"git\", \"clone\", \"--depth\", \"1\", self.url, self.authorname()],\n",
      "                cwd=(repos_dir / \"downloading\"),\n",
      "                timeout=timeout,\n",
      "                capture_output=True,\n",
      "            )\n",
      "            if not (repos_dir / \"downloading\" / self.authorname()).is_dir():\n",
      "                return False\n",
      "            subprocess.run(\n",
      "                [\"mv\", self.authorname(), (repos_dir / \"downloaded\")],\n",
      "                cwd=(repos_dir / \"downloading\"),\n",
      "                capture_output=True,\n",
      "            )\n",
      "            return True\n",
      "\n",
      "        def read_last_update(self, repos_dir):\n",
      "            d = self.repo_dir(repos_dir)\n",
      "            s = subprocess.run(\n",
      "                [\"git\", \"log\", \"-1\", \"--format=%cd\"], cwd=d, capture_output=True, text=True\n",
      "            ).stdout\n",
      "            lu = dateparser.parse(s.split(\"+\")[0])\n",
      "            assert lu is not None\n",
      "            self.last_update = lu.replace(tzinfo=None)\n",
      "            return self.last_update\n",
      "\n",
      "        def src_files(self, repos_dir):\n",
      "            for fpath in self.repo_dir(repos_dir).glob(\"**/*.py\"):\n",
      "                yield (fpath, read_file(fpath))\n",
      "\n",
      "        def count_lines_of_code(self, repos_dir):\n",
      "            n_lines = 0\n",
      "            for src in self.repo_dir(repos_dir).glob(\"**/*.py\"):\n",
      "                with open(src, \"r\") as fp:\n",
      "                    n_lines += sum(1 for line in fp if line.rstrip())\n",
      "            self.lines_of_code = n_lines\n",
      "            return n_lines\n",
      "\n",
      "    <extra_id_0>def collect_annotations(\n",
      "    <extra_id_1>    self, repos_dir, silent=True\n",
      "    <extra_id_2>) -> dict[Path, dict[AnnotPath, tuple[Optional[PythonType], AnnotCat]]]:\n",
      "    <extra_id_3>    n_paths, n_annots = 0, 0\n",
      "    <extra_id_4>    file_to_annots = dict[\n",
      "    <extra_id_5>        Path, dict[AnnotPath, tuple[Optional[PythonType], AnnotCat]]\n",
      "    <extra_id_6>    ]()\n",
      "    <extra_id_7>    for src in self.repo_dir(repos_dir).glob(\"**/*.py\"):\n",
      "    <extra_id_8>        rpath = src.relative_to(self.repo_dir(repos_dir))\n",
      "    <extra_id_9>        m = cst.parse_module(read_file(src))\n",
      "    <extra_id_10>        paths = collect_annots_info(m)\n",
      "    <extra_id_11>        path_to_cat = {pinfo.path: pinfo.cat for pinfo in paths}\n",
      "    <extra_id_12>        n_paths += len(paths)\n",
      "    <extra_id_13>        annots = (info for info in paths if info.annot is not None)\n",
      "    <extra_id_14>        n_annots += sum(1 for _ in annots)\n",
      "    <extra_id_15>        file_to_annots[rpath] = {\n",
      "    <extra_id_16>            (k := info.path): (\n",
      "    <extra_id_17>                parse_type_expr(\n",
      "    <extra_id_18>                    m, cast(cst.Annotation, info.annot).annotation, silent\n",
      "    <extra_id_19>                ),\n",
      "    <extra_id_20>                path_to_cat[k],\n",
      "    <extra_id_21>            )\n",
      "    <extra_id_22>            for info in annots\n",
      "    <extra_id_23>        }\n",
      "    <extra_id_24>    self.n_type_annots = n_annots\n",
      "    <extra_id_25>    self.n_type_places = n_paths\n",
      "    <extra_id_26>    return file_to_annots<extra_id_27>\n",
      "\n",
      "        def revert_changes(self, repos_dir):\n",
      "            rd = self.repo_dir(repos_dir)\n",
      "            result = subprocess.run(\n",
      "                [\"git\", \"diff\", \"--name-only\"], cwd=rd, capture_output=True, text=True\n",
      "            )\n",
      "            if result.returncode == 0 and result.stdout.strip()!= \"\":\n",
      "                print(\"Reverting changes in\", rd)\n",
      "                subprocess.run(\n",
      "                    [\"git\", \"checkout\", \".\"],\n",
      "                    cwd=rd,\n",
      "                )\n",
      "\n",
      "        @staticmethod\n",
      "        def from_json(json):\n",
      "            return GitRepo(\n",
      "                author=json[\"author\"],\n",
      "                name=json[\"repo\"],\n",
      "                url=json[\"repoUrl\"],\n",
      "                stars=json[\"stars\"],\n",
      "                forks=json[\"forks\"],\n",
      "            )\n",
      "\n",
      "\n",
      "    @dataclass\n",
      "    class CtxArgs:\n",
      "        ctx_size: int\n",
      "        preamble_size: int\n",
      "        left_margin: int\n",
      "        right_margin: int\n",
      "        max_labels: int = 16\n",
      "        inline_prev_gold: bool = False\n",
      "\n",
      "        def __post_init__(self):\n",
      "            assert self.preamble_size > 0\n",
      "            assert (\n",
      "                self.preamble_size < self.left_margin\n",
      "            ), \"Preamble bigger than left_margin.(Preamble is allcoated from the left margin.)\"\n",
      "            assert (\n",
      "                self.left_margin + self.right_margin < self.ctx_size\n",
      "            ), \"No window size left.\"\n",
      "\n",
      "        @property\n",
      "        def window_size(self) -> int:\n",
      "            return self.ctx_size - self.left_margin - self.right_margin\n",
      "\n",
      "        def __repr__(self):\n",
      "            return repr_modified_args(self)\n",
      "\n",
      "\n",
      "    def _compute_ctx(\n",
      "        src: TokenizedSrc, label_range: tuple[int, int], ctx_args: CtxArgs\n",
      "    ) -> tuple[list[int], tuple[int, int]]:\n",
      "        src_len = len(src.tokenized_code)\n",
      "        assert label_range[0] < len(\n",
      "            src.types_pos\n",
      "        ), f\"label_range={label_range}, len(types_pos)={len(src.types_pos)}\"\n",
      "        window_start = src.types_pos[label_range[0]]\n",
      "        left_margin_start = max(0, window_start - ctx_args.left_margin)\n",
      "        left_margin_size = window_start - left_margin_start\n",
      "\n",
      "        max_window_size = ctx_args.window_size\n",
      "        right_margin_end = left_margin_start + ctx_args.ctx_size\n",
      "        if right_margin_end >= src_len:\n",
      "            right_margin_end = src_len\n",
      "            max_window_size = right_margin_end - window_start\n",
      "            assert max_window_size > 0\n",
      "\n",
      "        label_pos = 0\n",
      "        label_ids = list[int]()\n",
      "        assert len(src.types) > 0\n",
      "        for i in range(label_range[0], label_range[1]):\n",
      "            label_pos = src.types_pos[i] - window_start\n",
      "            if 0 <= label_pos < max_window_size:\n",
      "                label_ids.append(i)\n",
      "            if len(label_ids) >= ctx_args.max_labels:\n",
      "                break\n",
      "\n",
      "        window_size = label_pos + 1\n",
      "        right_margin_end = min(\n",
      "            right_margin_end, window_start + window_size + ctx_args.right_margin\n",
      "        )\n",
      "\n",
      "        assert right_margin_end - left_margin_start <= ctx_args.ctx_size\n",
      "        assert left_margin_size <= ctx_args.left_margin\n",
      "        assert len(label_ids) <= ctx_args.max_labels\n",
      "\n",
      "        return label_ids, (left_margin_start, right_margin_end)\n",
      "\n",
      "\n",
      "    def chunk_from_src(\n",
      "        src: TokenizedSrc, label_id: int, ctx_args: CtxArgs\n",
      "    ) -> tuple[dict, \"SrcChunkInfo\"]:\n",
      "        chunks = list[dict]()\n",
      "        chunks_info = list[SrcChunkInfo]()\n",
      "        src_to_chunks_(\n",
      "            chunks,\n",
      "            chunks_info,\n",
      "            src,\n",
      "            (label_id, label_id + 1),\n",
      "            ctx_args,\n",
      "        )\n",
      "        assert_eq(len(chunks), len(chunks_info), 1)\n",
      "        return chunks[0], chunks_info[0]\n",
      "\n",
      "\n",
      "    def src_to_chunks(\n",
      "        src: TokenizedSrc,\n",
      "        label_range: tuple[int, int],\n",
      "        ctx_args: CtxArgs,\n",
      "    ) -> tuple[list[dict], list[\"SrcChunkInfo\"]]:\n",
      "        chunks = list[dict]()\n",
      "        chunks_info = list[SrcChunkInfo]()\n",
      "        src_to_chunks_(chunks, chunks_info, src, label_range, ctx_args)\n",
      "        return chunks, chunks_info\n",
      "\n",
      "\n",
      "    def src_to_chunks_(\n",
      "        chunks: list[dict],\n",
      "        chunks_info: list[\"SrcChunkInfo\"],\n",
      "        src: TokenizedSrc,\n",
      "        label_range: tuple[int, int],\n",
      "        ctx_args: CtxArgs,\n",
      "    ) -> None:\n",
      "        assert 0 <= label_range[0]\n",
      "        assert label_range[1] <= len(\n",
      "            src.types\n",
      "        ), f\"label_range: {label_range}, len(types): {len(src.types)}\"\n",
      "\n",
      "        tokenizer = DefaultTokenizer\n",
      "        special_tks = [tokenizer.additional_special_tokens_ids[99 - i] for i in range(100)]\n",
      "        bos_id, eos_id = not_none(tokenizer.bos_token_id), not_none(tokenizer.eos_token_id)\n",
      "\n",
      "        if len(src.tokenized_preamble) > ctx_args.preamble_size:\n",
      "            preamble = src.tokenized_preamble[-ctx_args.preamble_size :]\n",
      "            preamble[0] = bos_id\n",
      "        else:\n",
      "            preamble = src.tokenized_preamble\n",
      "        new_ctx_args = copy.deepcopy(ctx_args)\n",
      "        new_ctx_args.ctx_size -= len(preamble)\n",
      "        new_ctx_args.left_margin -= len(preamble)\n",
      "        new_ctx_args.preamble_size = 0\n",
      "\n",
      "        label_ids, (ctx_start, ctx_end) = _compute_ctx(src, label_range, new_ctx_args)\n",
      "        tks = src.tokenized_code[ctx_start:ctx_end]\n",
      "        if ctx_start > 0:\n",
      "            tks[0] = bos_id\n",
      "        if ctx_end < len(src.tokenized_code) - 1:\n",
      "            tks[-1] = eos_id\n",
      "        label_tkns = [bos_id]\n",
      "        types = list[PythonType]()\n",
      "        types_info = list[AnnotInfo]()\n",
      "        prev_types = list[PythonType]() if src.prev_types is not None else None\n",
      "        inlined_spans = list[slice]() if src.inlined_spans is not None else None\n",
      "        for i, l_id in enumerate(label_ids):\n",
      "            label_pos = src.types_pos[l_id] - ctx_start\n",
      "            tks[label_pos] = special_tks[i]\n",
      "            label_tkns.append(special_tks[i])\n",
      "            label_tkns.extend(src.types_tks[l_id])\n",
      "            types.append(src.types[l_id])\n",
      "            types_info.append(src.types_info[l_id])\n",
      "            if prev_types is not None and l_id in as_any(src.prev_types):\n",
      "                prev_types.append(as_any(src.prev_types)[l_id])\n",
      "            if</s>\n",
      "Output:\n",
      "    <extra_id_0><extra_id_1><extra_id_2><extra_id_3><extra_id_4><extra_id_5><extra_id_6><extra_id_7><extra_id_8><extra_id_9><extra_id_10><extra_id_11><extra_id_12><extra_id_13><extra_id_14><extra_id_15><extra_id_16><extra_id_17><extra_id_18> <add>                     cast(cst.Annotation, info.annot).annotation, silent\n",
      "     <del> <extra_id_19><extra_id_20><extra_id_21><extra_id_22><extra_id_23><extra_id_24><extra_id_25><extra_id_26><extra_id_27>\n"
     ]
    }
   ],
   "source": [
    "exs[1].truncate_ctx(WindowArgs.Default()).print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Modified: \n",
      "    def preds_to_accuracies(\n",
      "        preds: Sequence[Sequence[PythonType]],\n",
      "        dataset: ChunkedDataset,\n",
      "        metric: AccuracyMetric,\n",
      "    ):\n",
      "            cats = [an.cat for info in dataset.chunks_info for an in info.annots_info]\n",
      "            labels = [ty for info in dataset.chunks_info for ty in info.types]\n",
      "    -       poses = [i for info in dataset.chunks_info for i in info.label_ids]\n",
      "            return type_accuracies(\n",
      "                list(seq_flatten(preds)),\n",
      "                labels,\n",
      "                cats,\n",
      "    -           poses,\n",
      "                metric=metric,\n",
      "            )\n"
     ]
    }
   ],
   "source": [
    "from coeditor.history import *\n",
    "from coeditor.encoding import *\n",
    "\n",
    "all_mods = [c for e in edits for c in e.all_elem_changes() if isinstance(c, Modified)]\n",
    "c = all_mods[0]\n",
    "print(show_change(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32100, 768)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spot.model import load_model_spot\n",
    "from coeditor.encoding import _Tokenizer\n",
    "\n",
    "model = load_model_spot(\"Salesforce/codet5-base\")\n",
    "model.resize_token_embeddings(_Tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from coeditor.common import split_list\n",
    "\n",
    "\"a\\nb\\n\".splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- \n",
      "\n",
      "+++ \n",
      "\n",
      "@@ -1,2 +1,3 @@\n",
      "\n",
      " a\n",
      "\n",
      " b\n",
      "\n",
      "+c\n"
     ]
    }
   ],
   "source": [
    "from coeditor.encoding import Modified, change_to_tokens, decode_tokens, show_string_diff\n",
    "import difflib\n",
    "\n",
    "print(\"\\n\".join(difflib.unified_diff(\"a\\nb\\n\".splitlines(True), \"a\\nb\\nc\".splitlines(True))))\n",
    "\n",
    "# print(decode_tokens(change_to_tokens(Modified(\"a\", \"a\\n\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_commit: 49\n",
      "n_add: 339\n",
      "n_del: 246\n",
      "n_mod: 240\n"
     ]
    }
   ],
   "source": [
    "from coeditor.history import *\n",
    "\n",
    "n_add = n_del = n_mod = 0\n",
    "for e in edits:\n",
    "    for c in e.all_elem_changes():\n",
    "        if isinstance(c, Added):\n",
    "            n_add += 1\n",
    "        elif isinstance(c, Deleted):\n",
    "            n_del += 1\n",
    "        elif isinstance(c, Modified):\n",
    "            n_mod += 1\n",
    "        else:\n",
    "            raise ValueError(c)\n",
    "print(\"n_commit:\", len(edits))\n",
    "print(f\"n_add: {n_add}\")\n",
    "print(f\"n_del: {n_del}\")\n",
    "print(f\"n_mod: {n_mod}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting task: Performing intial module-level analysis...\n",
      "(6.4s) Finished task: Performing intial module-level analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing edits: 100%|██████████| 49/49 [03:52<00:00,  4.75s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>count</th>\n",
       "      <th>avg_time</th>\n",
       "      <th>total_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UsageAnalysis</td>\n",
       "      <td>98</td>\n",
       "      <td>1.579122</td>\n",
       "      <td>154.753982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ModuleAnlaysis/Incremental</td>\n",
       "      <td>182</td>\n",
       "      <td>0.412723</td>\n",
       "      <td>75.115604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ModuleAnlaysis/Initial</td>\n",
       "      <td>1</td>\n",
       "      <td>6.426628</td>\n",
       "      <td>6.426628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_select_change_ctx</td>\n",
       "      <td>240</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.001288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         name  count  avg_time  total_time\n",
       "1               UsageAnalysis     98  1.579122  154.753982\n",
       "2  ModuleAnlaysis/Incremental    182  0.412723   75.115604\n",
       "0      ModuleAnlaysis/Initial      1  6.426628    6.426628\n",
       "3          _select_change_ctx    240  0.000005    0.001288"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "analyzed_edits = analyze_edits(edits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modifications: 240\n",
      "User changes: 29\n",
      "Coverage: 12.1%\n"
     ]
    }
   ],
   "source": [
    "selected, all_cedits = select_edits(analyzed_edits, EditSelectors.api_change_to_callsite)\n",
    "coverage = set[tuple[ProjectPath, str]]()\n",
    "\n",
    "out_file = Path(\"output/api_change_to_callsite.txt\")\n",
    "with open(out_file, \"w\") as f:\n",
    "    for ce in selected:\n",
    "        for c in ce.grouped_ctx_changes[\"users\"]:\n",
    "            coverage.add((get_change_path(c), not_none(ce.commit_info).hash))\n",
    "\n",
    "        ce.pprint(file=f)\n",
    "        print(\"~\" * 50, \"\\n\", file=f)\n",
    "\n",
    "print(\"All modifications:\", len(all_cedits))\n",
    "print(\"User changes:\", len(coverage))\n",
    "print(\"Coverage:\", f\"{len(coverage) / len(all_cedits):.1%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modifications: 240\n",
      "User changes: 156\n",
      "Coverage: 65.0%\n"
     ]
    }
   ],
   "source": [
    "selected2, all_cedits2 = select_edits(analyzed_edits, EditSelectors.usee_changes_to_user)\n",
    "\n",
    "out_file = Path(\"output/pretrain.txt\")\n",
    "with open(out_file, \"w\") as f:\n",
    "    for ce in selected2:\n",
    "        ce.pprint(file=f)\n",
    "        print(\"~\" * 50, \"\\n\", file=f)\n",
    "\n",
    "print(\"All modifications:\", len(all_cedits2))\n",
    "print(\"User changes:\", len(selected2))\n",
    "print(\"Coverage:\", f\"{len(selected2) / len(all_cedits2):.1%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== End of new contents ====\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"ManyTypes4Py\"\n",
    "\n",
    "result_paths = {\n",
    "    \"CodeT5\": get_eval_dir(dataset, \"\"),\n",
    "    \"TypeT5\": get_eval_dir(\n",
    "        dataset,\n",
    "        \"(implicit_imports, new) model-v7--TrainingConfig(drop_env_types=False, add_implicit_rel_imports=True)\",\n",
    "    ),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_proj = PythonProject.from_root(Path(\"/home/jiayi/Projects/type4py\"))\n",
    "analysis = UsageAnalysis(\n",
    "    ex_proj, add_implicit_rel_imports=True, add_override_usages=True\n",
    ")\n",
    "pretty_print_dict(analysis.get_stats())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot.data import (\n",
    "    create_tokenized_srcsets,\n",
    "    get_tk_dataset_name,\n",
    "    load_tokenized_srcsets,\n",
    "    TypeCheckSettings,\n",
    ")\n",
    "from spot.tokenized_src import PreprocessArgs\n",
    "\n",
    "pre_args = PreprocessArgs()\n",
    "dataset = \"InferTypes4Py\"\n",
    "sdata_name = get_tk_dataset_name(dataset, pre_args, False)\n",
    "sdata_path = get_dataroot() / \"TokenizedSrcSets\" / sdata_name\n",
    "create_tokenized_srcsets(\n",
    "    dataset,\n",
    "    sdata_path,\n",
    "    func_only=False,\n",
    "    pre_args=pre_args,\n",
    ")\n",
    "tk_dataset = load_tokenized_srcsets(sdata_path)\n",
    "tk_dataset[\"test\"].print_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot import proj_root\n",
    "from spot.static_analysis import ProjectPath, UsageAnalysis, PythonProject\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "proj = PythonProject.from_root(proj_root())\n",
    "for caller, callees in UsageAnalysis(proj).user2used.items():\n",
    "    if caller.module == \"spot.static_analysis\":\n",
    "        print(caller)\n",
    "        for callee in callees:\n",
    "            print(\"\\t\", callee.used, \"\" if callee.is_certain else \"  (maybe)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import libcst as cst\n",
    "\n",
    "from spot.tokenized_src import TokenizedSrc, PreprocessArgs\n",
    "from spot.utils import Path, decode_tokens\n",
    "\n",
    "ex_code = '''# document comment 1\n",
    "  # document comment 2\n",
    "\"\"\"String document commnet\"\"\"\n",
    "import os; import spot;\n",
    "from sys import argv, exit\n",
    "# after import\n",
    "@wraps(function)\n",
    "def catch_permission_denied(function):\n",
    "    import some.inner.imports\n",
    "    \"\"\"\n",
    "    Decorator to catch :class:`psycopg2.ProgrammingError` exceptions with the\n",
    "    ``INSUFFICIENT_PRIVILEGE`` error code and rethrow them as\n",
    "    :class:`~werkzeug.exceptions.Forbidden` exceptions instead.\n",
    "    \"\"\"\n",
    "    @wraps(function)\n",
    "    def decorated(x: str, y: int) -> str:\n",
    "        try:\n",
    "            # comment 1\n",
    "            # comment 1 cont\n",
    "            return function(*args, **kwargs)\n",
    "\n",
    "        except InsufficientPrivilege as error:\n",
    "            LOG.error(\"Forbidden: %s\", error) # comment 2\n",
    "            raise Forbidden()\n",
    "\n",
    "    return decorated\n",
    "'''\n",
    "pre_args = PreprocessArgs(stub_in_preamble=True)\n",
    "ex_src = TokenizedSrc.parse(ex_code, Path(\"test_file\"), Path(\"test_repo\"), pre_args)\n",
    "print(decode_tokens(ex_src.tokenized_code))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot.data import src_to_chunks_, CtxArgs, PreprocessArgs\n",
    "from ipywidgets import interactive\n",
    "\n",
    "pre_args = PreprocessArgs(stub_in_preamble=True)\n",
    "ex_src = TokenizedSrc.parse(ex_code, Path(\"test_file\"), Path(\"test_repo\"), pre_args)\n",
    "\n",
    "\n",
    "def print_code(\n",
    "    preamble: int,\n",
    "    left: int,\n",
    "    right: int,\n",
    "    ctx_size: int,\n",
    "    max_labels: int,\n",
    "    chunk_id: int,\n",
    "    inline_prev: bool,\n",
    "):\n",
    "    chunks = []\n",
    "    args = CtxArgs(\n",
    "        ctx_size,\n",
    "        preamble,\n",
    "        left,\n",
    "        right,\n",
    "        max_labels=max_labels,\n",
    "        inline_prev_gold=inline_prev,\n",
    "    )\n",
    "    src_to_chunks_(chunks, [], ex_src, (0, len(ex_src.types)), args)\n",
    "    print(decode_tokens(chunks[chunk_id][\"input_ids\"]))\n",
    "\n",
    "\n",
    "interactive(\n",
    "    print_code,\n",
    "    preamble=(1, 100),\n",
    "    left=(1, 200),\n",
    "    right=(1, 100),\n",
    "    ctx_size=(1, 500),\n",
    "    max_labels=(1, 10),\n",
    "    chunk_id=(0, 1),\n",
    "    inline_prev=True,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f6ffc72953da4dd16b2e00785be9c4013ef131f465a8658f3921b6634d4eeec8"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.venv': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
