{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from coeditor.common import *\n",
    "import os\n",
    "\n",
    "os.chdir(proj_root())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coeditor.encoding import encode_basic, decode_tokens\n",
    "import torch\n",
    "from coeditor.retrieval_model import (\n",
    "    RetrievalEditorModel,\n",
    "    T5LayerSelfAttention,\n",
    "    t5_cross_attention,\n",
    "    T5Stack,\n",
    "    encode_query_stack,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_parameters: 222.88M\n"
     ]
    }
   ],
   "source": [
    "model = RetrievalEditorModel.from_code_t5(\"base\")\n",
    "device = torch.device(\"cuda:2\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(f\"num_parameters: {model.num_parameters()/1e6:.2f}M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:2')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test self attention\n",
    "config = model.config\n",
    "sa = T5LayerSelfAttention(config, has_relative_attention_bias=True)\n",
    "sa = sa.to(device)\n",
    "sa.eval()\n",
    "\n",
    "hidden_states = torch.randn(2, 3, config.d_model).to(device)\n",
    "out1 = sa.forward(hidden_states)[0]\n",
    "out2 = t5_cross_attention(sa, hidden_states, key_value_states=hidden_states)[0]\n",
    "\n",
    "torch.all(out1 == out2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack = model.encoder\n",
    "stack.eval()\n",
    "\n",
    "query_ids = torch.LongTensor([[1, 2, 5], [8,3,0]]).to(device)\n",
    "query_mask = query_ids.ne(0)\n",
    "ref_states = tuple(torch.randn(2, 5, config.d_model).to(device) for _ in stack.block)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out1.dtype=torch.float32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:2')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_mask = torch.zeros(2, 5, dtype=torch.bool).to(device)\n",
    "out1 = stack.forward(query_ids, attention_mask=query_mask)[0]\n",
    "out2 = encode_query_stack(\n",
    "    stack,\n",
    "    query_ids,\n",
    "    ref_states,\n",
    "    ref_attention_mask=ref_mask,\n",
    ").last_hidden_state\n",
    "\n",
    "print(f\"{out1.dtype=}\")\n",
    "torch.max(torch.abs((out1 - out2) * query_mask.unsqueeze(-1))) < 1e-5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_id=0\n",
      "query_attened_ref=False\n",
      "Loss with good ref: 2.984902858734131\n",
      "Loss with reversed good ref: 2.9848129749298096\n",
      "Loss with bad ref: 4.678882122039795\n",
      "Loss with no ref: 5.392837047576904\n",
      "query_attened_ref=True\n",
      "Loss with good ref: 2.831233263015747\n",
      "Loss with reversed good ref: 2.8314459323883057\n",
      "Loss with bad ref: 4.534156799316406\n",
      "Loss with no ref: 5.392837047576904\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = [\n",
    "    \"<s>assert weather == <extra_id_0>\\n</s>\",\n",
    "    \"<s>assert time == <extra_id_0> # make this longer\\n</s>\",\n",
    "    \"<s>assert name == <extra_id_0>\\n</s>\",\n",
    "]\n",
    "good_refs = [\"<s>weather = 'Icey'\\n</s>\", \"<s>time = '1:25AM'\\n</s>\", \"<s>name = 'Tako'\\n</s>\"]\n",
    "bad_refs = [\"<s>weather = 'Sunny'\\n</s>\", \"<s>time = '5:21PM'\\n</s>\", \"<s>name = 'Shmi'\\n</s>\"]\n",
    "answer = [\n",
    "    \"<pad><s><extra_id_0>'Icey'\",\n",
    "    \"<pad><s><extra_id_0>'1:25AM'\",\n",
    "    \"<pad><s><extra_id_0>'Tako'\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for q_id in [0]:\n",
    "    q_ids = slice(0, 2)\n",
    "    print(f\"{q_id=}\")\n",
    "    for query_attened_ref in [False, True]:\n",
    "        print(f\"{query_attened_ref=}\")\n",
    "        model.query_attened_ref = query_attened_ref\n",
    "\n",
    "        out = model.forward(\n",
    "            model.encode_token_seqs(query[q_ids]),\n",
    "            references=[encode_basic(x) for x in good_refs],\n",
    "            labels=model.encode_token_seqs(answer[q_ids], -100),\n",
    "        )\n",
    "        print(\"Loss with good ref:\", out.loss.item())\n",
    "        \n",
    "        with torch.autocast(\"cuda\"):\n",
    "            out = model.forward(\n",
    "                model.encode_token_seqs(query[q_ids]),\n",
    "                references=[encode_basic(x) for x in reversed(good_refs)],\n",
    "                # query_ref_list=[[1, 0], []],\n",
    "                labels=model.encode_token_seqs(answer[q_ids], -100),\n",
    "            )\n",
    "            print(\"Loss with reversed good ref:\", out.loss.item())\n",
    "\n",
    "        out = model.forward(\n",
    "            model.encode_token_seqs(query[q_ids]),\n",
    "            references=[encode_basic(x) for x in bad_refs],\n",
    "            labels=model.encode_token_seqs(answer[q_ids], -100),\n",
    "        )\n",
    "        print(\"Loss with bad ref:\", out.loss.item())\n",
    "\n",
    "        out = model.forward(\n",
    "            model.encode_token_seqs(query[q_ids]),\n",
    "            references=None,\n",
    "            labels=model.encode_token_seqs(answer[q_ids], -100),\n",
    "        )\n",
    "        print(\"Loss with no ref:\", out.loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single input:\n",
      "<s>weather = 'Icey'\n",
      "</s><s>time = '1:25AM'\n",
      "</s><s>name = 'Tako'\n",
      "</s><s>assert weather == <extra_id_0>\n",
      "</s>\n",
      "-------\n",
      "<s>weather = 'Icey'\n",
      "</s><s>time = '1:25AM'\n",
      "</s><s>name = 'Tako'\n",
      "</s><s>assert time == <extra_id_0> # make this longer\n",
      "</s>\n",
      "-------\n",
      "<s>weather = 'Icey'\n",
      "</s><s>time = '1:25AM'\n",
      "</s><s>name = 'Tako'\n",
      "</s><s>assert name == <extra_id_0>\n",
      "</s>\n",
      "Loss of CodeT5: tensor(3.4302, device='cuda:2', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from coeditor.model import CodeT5Model\n",
    "\n",
    "single_inputs = [\"\".join([*good_refs, q]) for q in query]\n",
    "print(\"Single input:\")\n",
    "print(\"\\n-------\\n\".join(single_inputs))\n",
    "\n",
    "codet5 = cast(CodeT5Model, CodeT5Model.from_pretrained(\"Salesforce/codet5-base\"))\n",
    "codet5.to(device)\n",
    "codet5.eval()\n",
    "\n",
    "out = codet5.forward(\n",
    "    model.encode_token_seqs(single_inputs),\n",
    "    labels=model.encode_token_seqs(answer),\n",
    ")\n",
    "print(\"Loss of CodeT5:\", out.loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output 0:\n",
      "<pad><s><extra_id_0>Tako <s> public class TakoWeather {</s>\n",
      "Output 1:\n",
      "<pad><s><extra_id_0>'1:25AM' name = 'Tako'</s>\n",
      "Output 2:\n",
      "<pad><s><extra_id_0>Tako <s> public class TakoWeather</s><pad>\n"
     ]
    }
   ],
   "source": [
    "codet5_seq = codet5.generate(\n",
    "    model.encode_token_seqs(single_inputs),\n",
    "    max_length=50,\n",
    "    num_beams=8,\n",
    ")\n",
    "for i, y in enumerate(codet5_seq):\n",
    "    print(f\"Output {i}:\")\n",
    "    print(decode_tokens(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output 0:\n",
      "<pad><s><extra_id_0>'Tako' unction( c ount)</s><pad><pad><pad><pad><pad>\n",
      "Output 1:\n",
      "<pad><s><extra_id_0>'1:25AM' ata = '1:25AM' ata</s>\n",
      "Output 2:\n",
      "<pad><s><extra_id_0>'Tako' unction s tarts=0 unction</s><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "model.query_attened_ref = True\n",
    "out_seq = model.generate(\n",
    "    model.encode_token_seqs(query),\n",
    "    references=[encode_basic(x) for x in reversed(good_refs)],\n",
    "    # num_beams=8,\n",
    "    max_length=50,\n",
    ")\n",
    "for i, y in enumerate(out_seq):\n",
    "    print(f\"Output {i}:\")\n",
    "    print(decode_tokens(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implement file-level dataset creation.\n",
      "Implement file-level edit encoder.\n",
      "Update installation instructions.\n",
      "Add DevGuide.md.\n",
      "Implement encoding format for CodeT5.\n",
      "Line-diff-based format for encoding edits.\n",
      "Implement EditSelectors.\n",
      "Improve diff visualization.\n",
      "Improve edit context construction.\n",
      "- Bugfix: `from_code_changes` mutates original copy. - Collect only usees in editing ctx. - Improve editing visualization.\n"
     ]
    }
   ],
   "source": [
    "from coeditor.history import *\n",
    "\n",
    "history = get_commit_history(proj_root(), 25)\n",
    "for cinfo in history[:10]:\n",
    "    print(cinfo.msg)\n",
    "\n",
    "# edits = edits_from_commit_history(proj_root(), history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coeditor.dataset import dataset_from_projects\n",
    "\n",
    "dataset = dataset_from_projects([proj_root()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Training Example: spot.type_checking/collect_annotations --------------------\n",
      "Input:\n",
      "    from dataclasses import dataclass\n",
      "    from distutils.log import error\n",
      "     <add> from logging import warn\n",
      "    from posixpath import dirname, realpath\n",
      "    import re\n",
      "    from typing import Iterable\n",
      "    from.utils import *\n",
      "    import subprocess\n",
      "\n",
      "\n",
      "     <add> @dataclass(order=True, unsafe_hash=True)\n",
      "     <del> @dataclass(frozen=True)\n",
      "    class AnnotPath:\n",
      "\n",
      "        value: Tuple[str,...]\n",
      "\n",
      "     <add>     def __repr__(self):\n",
      "     <add>         return f\"AnnotPath('{'.'.join(self.value)}')\"\n",
      "     <add> \n",
      "     <add>     def __str__(self):\n",
      "     <add>         return f\"'{'.'.join(self.value)}'\"\n",
      "     <add> \n",
      "     <add> \n",
      "    def annot_path(*segs: str) -> AnnotPath:\n",
      "        return AnnotPath(tuple(segs))\n",
      "     <add> \n",
      "\n",
      "    <extra_id_0>def collect_annotations(code: cst.CSTNode) -> Dict[AnnotPath, Optional[cst.Annotation]]:\n",
      "    <extra_id_1>    collector = AnnotCollector()\n",
      "    <extra_id_2>    code.visit(collector)\n",
      "    <extra_id_3>\n",
      "\n",
      "\n",
      "     <add> def apply_annotations(code: cst.CSTNode, annots: dict[AnnotPath, cst.Annotation]):\n",
      "     <del> def apply_annotations(code: cst.CSTNode, annots: Dict[AnnotPath, cst.Annotation]):\n",
      "        applier = AnnotApplier(annots)\n",
      "        return code.visit(applier)\n",
      "\n",
      "\n",
      "    class AnnotCollector(cst.CSTVisitor):\n",
      "        def __init__(self):\n",
      "            self.stack: List[str] = []\n",
      "     <add>         self.annot_paths: List[AnnotPath] = []\n",
      "     <add>         self.annotations: Dict[AnnotPath, cst.Annotation] = {}\n",
      "     <del>         self.annotations: Dict[AnnotPath, Optional[cst.Annotation]] = {}\n",
      "\n",
      "        def on_visit(self, node):\n",
      "            if (\n",
      "                isinstance(node, cst.FunctionDef)\n",
      "                or isinstance(node, cst.ClassDef)\n",
      "                or isinstance(node, cst.Param)\n",
      "            ):\n",
      "                self.stack.append(node.name.value)\n",
      "            elif isinstance(node, cst.AnnAssign):\n",
      "                self.stack.append(node.target.value)\n",
      "            return super().on_visit(node)\n",
      "\n",
      "        def on_leave(self, node):\n",
      "            r = super().on_leave(node)\n",
      "            if (\n",
      "                isinstance(node, cst.FunctionDef)\n",
      "                or isinstance(node, cst.ClassDef)\n",
      "                or isinstance(node, cst.Param)\n",
      "                or isinstance(node, cst.AnnAssign)\n",
      "            ):\n",
      "                self.stack.pop()\n",
      "            return r\n",
      "\n",
      "        def _current_path(self):\n",
      "            return AnnotPath(tuple(self.stack))\n",
      "\n",
      "     <add>     def _record_annot(self, annot: Union[cst.Annotation, None]):\n",
      "     <add>         path = self._current_path()\n",
      "     <add>         self.annot_paths.append(path)\n",
      "     <add>         if annot is not None:\n",
      "     <add>             self.annotations[path] = annot\n",
      "     <add> \n",
      "        def visit_FunctionDef(self, node: cst.FunctionDef):\n",
      "            self.stack.append(SpecialNames.Return)\n",
      "     <add>         self._record_annot(node.returns)\n",
      "     <del>         self.annotations[self._current_path()] = node.returns\n",
      "            self.stack.pop()\n",
      "\n",
      "        def visit_Param(self, node: cst.Param):\n",
      "     <add>         self._record_annot(node.annotation)\n",
      "     <del>         self.annotations[self._current_path()] = node.annotation\n",
      "\n",
      "     <add>     def visit_AnnAssign(self, node: cst.AnnAssign):\n",
      "     <add>         self._record_annot(node.annotation)\n",
      "     <del>     def visit_AnnAssign(self, ndoe: cst.AnnAssign):\n",
      "     <del>         self.annotations[self._current_path()] = ndoe.annotation\n",
      "\n",
      "\n",
      "    class AnnotApplier(cst.CSTTransformer):\n",
      "        def __init__(self, annots: Dict[AnnotPath, cst.Annotation]):\n",
      "            self.annots = annots\n",
      "            self.stack: List[str] = []\n",
      "            self.prefixes: Set[Tuple[str,...]] = set()\n",
      "            for path in annots.keys():\n",
      "                self.prefixes.update(path.value[0:i] for i in range(len(path.value) + 1))\n",
      "\n",
      "        def _current_path(self):\n",
      "            return AnnotPath(tuple(self.stack))\n",
      "\n",
      "        def on_visit(self, node):\n",
      "            if (\n",
      "                isinstance(node, cst.FunctionDef)\n",
      "                or isinstance(node, cst.ClassDef)\n",
      "                or isinstance(node, cst.Param)\n",
      "            ):\n",
      "                self.stack.append(node.name.value)\n",
      "            elif isinstance(node, cst.AnnAssign):\n",
      "                self.stack.append(node.target.value)\n",
      "            if tuple(self.stack) not in self.prefixes:\n",
      "                return False\n",
      "            return super().on_visit(node)\n",
      "\n",
      "        def on_leave(self, node, updated):\n",
      "            r = super().on_leave(node, updated)\n",
      "            if (\n",
      "                isinstance(node, cst.FunctionDef)\n",
      "                or isinstance(node, cst.ClassDef)\n",
      "                or isinstance(node, cst.Param)\n",
      "                or isinstance(node, cst.AnnAssign)\n",
      "            ):\n",
      "                self.stack.pop()\n",
      "            return r\n",
      "\n",
      "        def leave_FunctionDef(\n",
      "            self, node: cst.FunctionDef, updated: cst.FunctionDef\n",
      "        ) -> cst.FunctionDef:\n",
      "            self.stack.append(SpecialNames.Return)\n",
      "            patch = self.annots.get(self._current_path())\n",
      "            self.stack.pop()\n",
      "            return updated if patch is None else updated.with_changes(returns=patch)\n",
      "\n",
      "        def leave_Param(self, node: cst.Param, updated: cst.Param) -> cst.Param:\n",
      "            patch = self.annots.get(self._current_path())\n",
      "            return updated if patch is None else updated.with_changes(annotation=patch)\n",
      "\n",
      "\n",
      "     <add> @dataclass\n",
      "     <add> class MypyResult:\n",
      "     <add>     num_errors: int\n",
      "     <add>     num_error_dict: Dict[str, int]\n",
      "     <add>     output_str: str\n",
      "     <add> \n",
      "     <add> \n",
      "    class MypyChecker:\n",
      "\n",
      "        def __init__(self, dmypy_path, code_dir) -> None:\n",
      "            self.code_dir = realpath(code_dir)\n",
      "            self.dmypy_path = realpath(dmypy_path)\n",
      "     <add>         subprocess.run(\n",
      "     <add>             [\n",
      "     <add>                 \"python\",\n",
      "     <add>                 self.dmypy_path,\n",
      "     <add>                 \"start\",\n",
      "     <add>                 \"--\",\n",
      "     <add>                 \"--follow-imports=skip\",\n",
      "     <add>             ],\n",
      "     <add>             cwd=self.code_dir,\n",
      "     <add>         )\n",
      "     <add>         subprocess.run(\n",
      "     <add>             [\"python\", self.dmypy_path, \"check\", self.code_dir],\n",
      "     <add>             cwd=self.code_dir,\n",
      "     <del>         self.mypy_version = subprocess.run(\n",
      "     <del>             [\"python\", self.dmypy_path, \"-V\"],\n",
      "                capture_output=True,\n",
      "     <add>         )\n",
      "     <del>             text=True,\n",
      "     <del>             cwd=self.code_dir,\n",
      "     <del>         ).stdout\n",
      "\n",
      "     <add>     def close(self):\n",
      "     <del>     def stop_daemon(self):\n",
      "            subprocess.run(\n",
      "                [\"python\", self.dmypy_path, \"stop\"],\n",
      "                cwd=self.code_dir,\n",
      "            )\n",
      "\n",
      "     <add>     def check_code_dir(self) -> MypyResult:\n",
      "     <add>         return self._run_mypy([\"python\", self.dmypy_path, \"check\", self.code_dir])\n",
      "     <del>     def __del__(self):\n",
      "     <del>         self.stop_daemon()\n",
      "\n",
      "     <add> \n",
      "     <add>     def recheck_files(self, *updated_files: str) -> MypyResult:\n",
      "     <add>         return self._run_mypy([\"python\", self.dmypy_path, \"recheck\", \"--update\", *updated_files])\n",
      "     <add>         \n",
      "     <add> \n",
      "     <add>     def _run_mypy(self, cmd: list[str]) -> MypyResult:\n",
      "     <del>     def check_file(self, fpath):\n",
      "     <del>         print(\"chekcing!\")\n",
      "     <del>         cmd = [\"python\", self.dmypy_path, \"run\", \"--\", fpath]\n",
      "            result = subprocess.run(\n",
      "                cmd,\n",
      "                capture_output=True,\n",
      "                text=True,\n",
      "                cwd=self.code_dir,\n",
      "            )\n",
      "            lines = result.stdout.splitlines()\n",
      "     <add>         assert (\n",
      "     <add>             len(lines) > 0\n",
      "     <add>         ), f\"mypy failed. Command: `{' '.join(cmd)}`\\nError: {result.stderr}\"\n",
      "     <add>         num_error_dict: dict[str, int] = {}\n",
      "     <del>         assert len(lines) > 0, f\"mypy failed. Error: {result.stderr}\"\n",
      "     <del>         num_error_dict = {}\n",
      "            for l in lines:\n",
      "                m = re.match(r\"(.*\\.py):\\d+: error:.+\", l)\n",
      "                if m is not None:\n",
      "                    num_error_dict[m.group(1)] = num_error_dict.get(m.group(1), 0) + 1\n",
      "\n",
      "            m = re.match(r\"Found (\\d+) errors? in\", lines[-1])\n",
      "            if m is None:\n",
      "                num_errors = 0\n",
      "            else:\n",
      "                num_errors = int(m.group(1))\n",
      "\n",
      "            total_errors = sum(num_error_dict.values())\n",
      "            assert (\n",
      "                num_errors == total_errors\n",
      "            ), f\"{num_errors}!= {total_errors}. mypy output: {result.stdout}\"\n",
      "            return MypyResult(num_errors, num_error_dict, result.stdout)\n",
      "\n",
      "\n",
      "    @dataclass\n",
      "     <add> class mypy_checker:\n",
      "     <add> \n",
      "     <add>     dmypy_path: str\n",
      "     <add>     code_dir: str\n",
      "     <add> \n",
      "     <add>     def __enter__(self):\n",
      "     <add>         self.checker = MypyChecker(self.dmypy_path, self.code_dir)\n",
      "     <add>         return self.checker\n",
      "     <add> \n",
      "     <add>     def __exit__(self, exc_type, exc_val, exc_tb):\n",
      "     <add>         self.checker.close() <del> class MypyResult:\n",
      "     <del>     num_errors: int\n",
      "     <del>     num_error_dict: Dict[str, int]\n",
      "     <del>     output_str: str\n",
      "\n",
      "Output:\n",
      "    <extra_id_0> <add> def collect_annotations(\n",
      "     <add>     code: cst.CSTNode,\n",
      "     <add> ) -> Tuple[list[AnnotPath], dict[AnnotPath, cst.Annotation]]:\n",
      "     <del> <extra_id_1><extra_id_2><extra_id_3> <add>     return collector.annot_paths, collector.annotations <del>     return collector.annotations\n"
     ]
    }
   ],
   "source": [
    "dataset.edits[0].print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retriving edits:   0%|          | 0/1 [00:00<?, ?it/s]Starting task: Retriving initial project from commit: 3c17c4ea794ce495edd62698a46aa696e384bed1\n",
    "(0.1s) Finished task: Retriving initial project from commit: 3c17c4ea794ce495edd62698a46aa696e384bed1\n",
    "Edits from commits: 100%|██████████| 215/215 [03:32<00:00,  1.01it/s]\n",
    "Retriving edits: 100%|██████████| 1/1 [05:24<00:00, 324.13s/it]\n",
    "Encoding edits: 100%|██████████| 215/215 [05:21<00:00,  1.50s/it]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Modified: \n",
      "    def preds_to_accuracies(\n",
      "        preds: Sequence[Sequence[PythonType]],\n",
      "        dataset: ChunkedDataset,\n",
      "        metric: AccuracyMetric,\n",
      "    ):\n",
      "            cats = [an.cat for info in dataset.chunks_info for an in info.annots_info]\n",
      "            labels = [ty for info in dataset.chunks_info for ty in info.types]\n",
      "    -       poses = [i for info in dataset.chunks_info for i in info.label_ids]\n",
      "            return type_accuracies(\n",
      "                list(seq_flatten(preds)),\n",
      "                labels,\n",
      "                cats,\n",
      "    -           poses,\n",
      "                metric=metric,\n",
      "            )\n"
     ]
    }
   ],
   "source": [
    "from coeditor.history import *\n",
    "from coeditor.encoding import *\n",
    "\n",
    "all_mods = [c for e in edits for c in e.all_elem_changes() if isinstance(c, Modified)]\n",
    "c = all_mods[0]\n",
    "print(show_change(c))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_commit: 49\n",
      "n_add: 339\n",
      "n_del: 246\n",
      "n_mod: 240\n"
     ]
    }
   ],
   "source": [
    "from coeditor.history import *\n",
    "\n",
    "n_add = n_del = n_mod = 0\n",
    "for e in edits:\n",
    "    for c in e.all_elem_changes():\n",
    "        if isinstance(c, Added):\n",
    "            n_add += 1\n",
    "        elif isinstance(c, Deleted):\n",
    "            n_del += 1\n",
    "        elif isinstance(c, Modified):\n",
    "            n_mod += 1\n",
    "        else:\n",
    "            raise ValueError(c)\n",
    "print(\"n_commit:\", len(edits))\n",
    "print(f\"n_add: {n_add}\")\n",
    "print(f\"n_del: {n_del}\")\n",
    "print(f\"n_mod: {n_mod}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting task: Performing intial module-level analysis...\n",
      "(6.4s) Finished task: Performing intial module-level analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing edits: 100%|██████████| 49/49 [03:52<00:00,  4.75s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>count</th>\n",
       "      <th>avg_time</th>\n",
       "      <th>total_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UsageAnalysis</td>\n",
       "      <td>98</td>\n",
       "      <td>1.579122</td>\n",
       "      <td>154.753982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ModuleAnlaysis/Incremental</td>\n",
       "      <td>182</td>\n",
       "      <td>0.412723</td>\n",
       "      <td>75.115604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ModuleAnlaysis/Initial</td>\n",
       "      <td>1</td>\n",
       "      <td>6.426628</td>\n",
       "      <td>6.426628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_select_change_ctx</td>\n",
       "      <td>240</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.001288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         name  count  avg_time  total_time\n",
       "1               UsageAnalysis     98  1.579122  154.753982\n",
       "2  ModuleAnlaysis/Incremental    182  0.412723   75.115604\n",
       "0      ModuleAnlaysis/Initial      1  6.426628    6.426628\n",
       "3          _select_change_ctx    240  0.000005    0.001288"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "analyzed_edits = analyze_edits(edits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modifications: 240\n",
      "User changes: 29\n",
      "Coverage: 12.1%\n"
     ]
    }
   ],
   "source": [
    "selected, all_cedits = select_edits(\n",
    "    analyzed_edits, EditSelectors.api_change_to_callsite\n",
    ")\n",
    "coverage = set[tuple[ProjectPath, str]]()\n",
    "\n",
    "out_file = Path(\"output/api_change_to_callsite.txt\")\n",
    "with open(out_file, \"w\") as f:\n",
    "    for ce in selected:\n",
    "        for c in ce.grouped_ctx_changes[\"users\"]:\n",
    "            coverage.add((get_change_path(c), not_none(ce.commit_info).hash))\n",
    "\n",
    "        ce.pprint(file=f)\n",
    "        print(\"~\" * 50, \"\\n\", file=f)\n",
    "\n",
    "print(\"All modifications:\", len(all_cedits))\n",
    "print(\"User changes:\", len(coverage))\n",
    "print(\"Coverage:\", f\"{len(coverage) / len(all_cedits):.1%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modifications: 240\n",
      "User changes: 156\n",
      "Coverage: 65.0%\n"
     ]
    }
   ],
   "source": [
    "selected2, all_cedits2 = select_edits(\n",
    "    analyzed_edits, EditSelectors.usee_changes_to_user\n",
    ")\n",
    "\n",
    "out_file = Path(\"output/pretrain.txt\")\n",
    "with open(out_file, \"w\") as f:\n",
    "    for ce in selected2:\n",
    "        ce.pprint(file=f)\n",
    "        print(\"~\" * 50, \"\\n\", file=f)\n",
    "\n",
    "print(\"All modifications:\", len(all_cedits2))\n",
    "print(\"User changes:\", len(selected2))\n",
    "print(\"Coverage:\", f\"{len(selected2) / len(all_cedits2):.1%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== End of new contents ====\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"ManyTypes4Py\"\n",
    "\n",
    "result_paths = {\n",
    "    \"CodeT5\": get_eval_dir(dataset, \"\"),\n",
    "    \"TypeT5\": get_eval_dir(\n",
    "        dataset,\n",
    "        \"(implicit_imports, new) model-v7--TrainingConfig(drop_env_types=False, add_implicit_rel_imports=True)\",\n",
    "    ),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_proj = PythonProject.from_root(Path(\"/home/jiayi/Projects/type4py\"))\n",
    "analysis = UsageAnalysis(\n",
    "    ex_proj, add_implicit_rel_imports=True, add_override_usages=True\n",
    ")\n",
    "pretty_print_dict(analysis.get_stats())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot.data import (\n",
    "    create_tokenized_srcsets,\n",
    "    get_tk_dataset_name,\n",
    "    load_tokenized_srcsets,\n",
    "    TypeCheckSettings,\n",
    ")\n",
    "from spot.tokenized_src import PreprocessArgs\n",
    "\n",
    "pre_args = PreprocessArgs()\n",
    "dataset = \"InferTypes4Py\"\n",
    "sdata_name = get_tk_dataset_name(dataset, pre_args, False)\n",
    "sdata_path = get_dataroot() / \"TokenizedSrcSets\" / sdata_name\n",
    "create_tokenized_srcsets(\n",
    "    dataset,\n",
    "    sdata_path,\n",
    "    func_only=False,\n",
    "    pre_args=pre_args,\n",
    ")\n",
    "tk_dataset = load_tokenized_srcsets(sdata_path)\n",
    "tk_dataset[\"test\"].print_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot import proj_root\n",
    "from spot.static_analysis import ProjectPath, UsageAnalysis, PythonProject\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "proj = PythonProject.from_root(proj_root())\n",
    "for caller, callees in UsageAnalysis(proj).user2used.items():\n",
    "    if caller.module == \"spot.static_analysis\":\n",
    "        print(caller)\n",
    "        for callee in callees:\n",
    "            print(\"\\t\", callee.used, \"\" if callee.is_certain else \"  (maybe)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import libcst as cst\n",
    "\n",
    "from spot.tokenized_src import TokenizedSrc, PreprocessArgs\n",
    "from spot.utils import Path, decode_tokens\n",
    "\n",
    "ex_code = '''# document comment 1\n",
    "  # document comment 2\n",
    "\"\"\"String document commnet\"\"\"\n",
    "import os; import spot;\n",
    "from sys import argv, exit\n",
    "# after import\n",
    "@wraps(function)\n",
    "def catch_permission_denied(function):\n",
    "    import some.inner.imports\n",
    "    \"\"\"\n",
    "    Decorator to catch :class:`psycopg2.ProgrammingError` exceptions with the\n",
    "    ``INSUFFICIENT_PRIVILEGE`` error code and rethrow them as\n",
    "    :class:`~werkzeug.exceptions.Forbidden` exceptions instead.\n",
    "    \"\"\"\n",
    "    @wraps(function)\n",
    "    def decorated(x: str, y: int) -> str:\n",
    "        try:\n",
    "            # comment 1\n",
    "            # comment 1 cont\n",
    "            return function(*args, **kwargs)\n",
    "\n",
    "        except InsufficientPrivilege as error:\n",
    "            LOG.error(\"Forbidden: %s\", error) # comment 2\n",
    "            raise Forbidden()\n",
    "\n",
    "    return decorated\n",
    "'''\n",
    "pre_args = PreprocessArgs(stub_in_preamble=True)\n",
    "ex_src = TokenizedSrc.parse(ex_code, Path(\"test_file\"), Path(\"test_repo\"), pre_args)\n",
    "print(decode_tokens(ex_src.tokenized_code))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot.data import src_to_chunks_, CtxArgs, PreprocessArgs\n",
    "from ipywidgets import interactive\n",
    "\n",
    "pre_args = PreprocessArgs(stub_in_preamble=True)\n",
    "ex_src = TokenizedSrc.parse(ex_code, Path(\"test_file\"), Path(\"test_repo\"), pre_args)\n",
    "\n",
    "\n",
    "def print_code(\n",
    "    preamble: int,\n",
    "    left: int,\n",
    "    right: int,\n",
    "    ctx_size: int,\n",
    "    max_labels: int,\n",
    "    chunk_id: int,\n",
    "    inline_prev: bool,\n",
    "):\n",
    "    chunks = []\n",
    "    args = CtxArgs(\n",
    "        ctx_size,\n",
    "        preamble,\n",
    "        left,\n",
    "        right,\n",
    "        max_labels=max_labels,\n",
    "        inline_prev_gold=inline_prev,\n",
    "    )\n",
    "    src_to_chunks_(chunks, [], ex_src, (0, len(ex_src.types)), args)\n",
    "    print(decode_tokens(chunks[chunk_id][\"input_ids\"]))\n",
    "\n",
    "\n",
    "interactive(\n",
    "    print_code,\n",
    "    preamble=(1, 100),\n",
    "    left=(1, 200),\n",
    "    right=(1, 100),\n",
    "    ctx_size=(1, 500),\n",
    "    max_labels=(1, 10),\n",
    "    chunk_id=(0, 1),\n",
    "    inline_prev=True,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f6ffc72953da4dd16b2e00785be9c4013ef131f465a8658f3921b6634d4eeec8"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.venv': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
