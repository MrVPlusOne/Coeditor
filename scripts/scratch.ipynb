{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from spot.utils import *\n",
    "from spot.static_analysis import UsageAnalysis, PythonProject\n",
    "from spot.function_dataset import data_project_from_dir\n",
    "import os\n",
    "\n",
    "os.chdir(proj_root())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implement EditSelectors.\n",
      "Improve diff visualization.\n",
      "Improve edit context construction.\n",
      "- Bugfix: `from_code_changes` mutates original copy. - Collect only usees in editing ctx. - Improve editing visualization.\n",
      "Generate contextual edits using static analysis.\n",
      "Properly handle merge commits.\n",
      "Construct edits incrementally from git history.\n",
      "Init commit for the coeditor branch.\n",
      "Rename ElemPath.\n",
      "Get rid of unsued files.\n",
      "Starting task: Retriving initial project from commit: c7f133ac3d92b45840424e29a099c1590004acbd\n",
      "(4.8s) Finished task: Retriving initial project from commit: c7f133ac3d92b45840424e29a099c1590004acbd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Edits from commits: 100%|██████████| 49/49 [00:58<00:00,  1.19s/it]\n"
     ]
    }
   ],
   "source": [
    "from coeditor.history import *\n",
    "\n",
    "history = get_commit_history(proj_root(), 50)\n",
    "for cinfo in history[:10]:\n",
    "    print(cinfo.msg)\n",
    "\n",
    "edits = edits_from_commit_history(proj_root(), history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Modified: \n",
      "    def test_usage_analysis():\n",
      "    ... 112 lines omitted ...\n",
      "            usage1(5)  # from file2, which shadows file3\n",
      "            C(5)  # from file1\n",
      "            nonexist()\n",
      " \n",
      "        \"\"\"\n",
      " \n",
      "    -       project = PythonProject.from_modules(\n",
      "    -           [\n",
      "    -               PythonModule.from_cst(cst.parse_module(code1), \"root.file1\"),\n",
      "    -               PythonModule.from_cst(cst.parse_module(code2), \"root.file2\"),\n",
      "    -               PythonModule.from_cst(cst.parse_module(code3), \"root.file3\"),\n",
      "    -               PythonModule.from_cst(cst.parse_module(code4), \"root.file4\"),\n",
      "    -           ]\n",
      "    +       project = project_from_code(\n",
      "    +           {\n",
      "    +               \"root.file1\": code1,\n",
      "    +               \"root.file2\": code2,\n",
      "    +               \"root.file3\": code3,\n",
      "    +               \"root.file4\": code4,\n",
      "    +           }\n",
      "            )\n",
      "            analysis = UsageAnalysis(project)\n",
      " \n",
      "            analysis.assert_usages(\n",
      "                \"root.file2/usage1\",\n",
      "                (\"root.file1/gf\", True),\n",
      "    ... 78 lines omitted ...\n",
      "        from . import file1\n",
      " \n",
      "        def usage5():\n",
      "            file1.gf(5)\n",
      "     \n",
      "        \"\"\"\n",
      "    -       project = PythonProject.from_modules(\n",
      "    -           [\n",
      "    -               PythonModule.from_cst(cst.parse_module(code1), \"root.file1\"),\n",
      "    -               PythonModule.from_cst(cst.parse_module(code5), \"root.file5\"),\n",
      "    -           ]\n",
      "    +\n",
      "    +       project = project_from_code(\n",
      "    +           {\n",
      "    +               \"root.file1\": code1,\n",
      "    +               \"root.file5\": code5,\n",
      "    +           }\n",
      "            )\n",
      "            analysis = UsageAnalysis(project)\n",
      "            analysis.assert_usages(\"root.file5/usage5\", (\"root.file1/gf\", True))\n",
      " \n",
      "            code6 = \"\"\"\n",
      "        # root.file6\n",
      " \n",
      "        Count = 1\n",
      " \n",
      "        def inc(x=Count):\n",
      "            return x + 1\n",
      "        \"\"\"\n",
      "    -       project = PythonProject.from_modules(\n",
      "    -           [PythonModule.from_cst(cst.parse_module(code6), \"root.file6\")]\n",
      "    +\n",
      "    +       project = project_from_code(\n",
      "    +           {\n",
      "    +               \"root.file6\": code6,\n",
      "    +           }\n",
      "            )\n",
      "            analysis = UsageAnalysis(project)\n",
      "            analysis.assert_usages(\"root.file6/inc\", (\"root.file6/Count\", True))\n"
     ]
    }
   ],
   "source": [
    "from coeditor.history import *\n",
    "from coeditor.encoding import *\n",
    "\n",
    "all_mods = [c for e in edits for c in e.all_elem_changes() if isinstance(c, Modified)]\n",
    "c = all_mods[0]\n",
    "print(show_change(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_commit: 49\n",
      "n_add: 339\n",
      "n_del: 246\n",
      "n_mod: 240\n"
     ]
    }
   ],
   "source": [
    "from coeditor.history import *\n",
    "\n",
    "n_add = n_del = n_mod = 0\n",
    "for e in edits:\n",
    "    for c in e.all_elem_changes():\n",
    "        if isinstance(c, Added):\n",
    "            n_add += 1\n",
    "        elif isinstance(c, Deleted):\n",
    "            n_del += 1\n",
    "        elif isinstance(c, Modified):\n",
    "            n_mod += 1\n",
    "        else:\n",
    "            raise ValueError(c)\n",
    "print(\"n_commit:\", len(edits))\n",
    "print(f\"n_add: {n_add}\")\n",
    "print(f\"n_del: {n_del}\")\n",
    "print(f\"n_mod: {n_mod}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting task: Performing intial module-level analysis...\n",
      "(6.4s) Finished task: Performing intial module-level analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing edits: 100%|██████████| 49/49 [03:52<00:00,  4.75s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>count</th>\n",
       "      <th>avg_time</th>\n",
       "      <th>total_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UsageAnalysis</td>\n",
       "      <td>98</td>\n",
       "      <td>1.579122</td>\n",
       "      <td>154.753982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ModuleAnlaysis/Incremental</td>\n",
       "      <td>182</td>\n",
       "      <td>0.412723</td>\n",
       "      <td>75.115604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ModuleAnlaysis/Initial</td>\n",
       "      <td>1</td>\n",
       "      <td>6.426628</td>\n",
       "      <td>6.426628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_select_change_ctx</td>\n",
       "      <td>240</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.001288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         name  count  avg_time  total_time\n",
       "1               UsageAnalysis     98  1.579122  154.753982\n",
       "2  ModuleAnlaysis/Incremental    182  0.412723   75.115604\n",
       "0      ModuleAnlaysis/Initial      1  6.426628    6.426628\n",
       "3          _select_change_ctx    240  0.000005    0.001288"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "analyzed_edits = analyze_edits(edits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modifications: 240\n",
      "User changes: 29\n",
      "Coverage: 12.1%\n"
     ]
    }
   ],
   "source": [
    "selected, all_cedits = select_edits(analyzed_edits, EditSelectors.api_change_to_callsite)\n",
    "coverage = set[tuple[ProjectPath, str]]()\n",
    "\n",
    "out_file = Path(\"output/api_change_to_callsite.txt\")\n",
    "with open(out_file, \"w\") as f:\n",
    "    for ce in selected:\n",
    "        for c in ce.grouped_ctx_changes[\"users\"]:\n",
    "            coverage.add((get_change_path(c), not_none(ce.commit_info).hash))\n",
    "\n",
    "        ce.pprint(file=f)\n",
    "        print(\"~\" * 50, \"\\n\", file=f)\n",
    "\n",
    "print(\"All modifications:\", len(all_cedits))\n",
    "print(\"User changes:\", len(coverage))\n",
    "print(\"Coverage:\", f\"{len(coverage) / len(all_cedits):.1%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modifications: 240\n",
      "User changes: 156\n",
      "Coverage: 65.0%\n"
     ]
    }
   ],
   "source": [
    "selected2, all_cedits2 = select_edits(analyzed_edits, EditSelectors.usee_changes_to_user)\n",
    "\n",
    "out_file = Path(\"output/pretrain.txt\")\n",
    "with open(out_file, \"w\") as f:\n",
    "    for ce in selected2:\n",
    "        ce.pprint(file=f)\n",
    "        print(\"~\" * 50, \"\\n\", file=f)\n",
    "\n",
    "print(\"All modifications:\", len(all_cedits2))\n",
    "print(\"User changes:\", len(selected2))\n",
    "print(\"Coverage:\", f\"{len(selected2) / len(all_cedits2):.1%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== End of new contents ====\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"ManyTypes4Py\"\n",
    "\n",
    "result_paths = {\n",
    "    \"CodeT5\": get_eval_dir(dataset, \"\"),\n",
    "    \"TypeT5\": get_eval_dir(\n",
    "        dataset,\n",
    "        \"(implicit_imports, new) model-v7--TrainingConfig(drop_env_types=False, add_implicit_rel_imports=True)\",\n",
    "    ),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_proj = PythonProject.from_root(Path(\"/home/jiayi/Projects/type4py\"))\n",
    "analysis = UsageAnalysis(\n",
    "    ex_proj, add_implicit_rel_imports=True, add_override_usages=True\n",
    ")\n",
    "pretty_print_dict(analysis.get_stats())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot.data import (\n",
    "    create_tokenized_srcsets,\n",
    "    get_tk_dataset_name,\n",
    "    load_tokenized_srcsets,\n",
    "    TypeCheckSettings,\n",
    ")\n",
    "from spot.tokenized_src import PreprocessArgs\n",
    "\n",
    "pre_args = PreprocessArgs()\n",
    "dataset = \"InferTypes4Py\"\n",
    "sdata_name = get_tk_dataset_name(dataset, pre_args, False)\n",
    "sdata_path = get_dataroot() / \"TokenizedSrcSets\" / sdata_name\n",
    "create_tokenized_srcsets(\n",
    "    dataset,\n",
    "    sdata_path,\n",
    "    func_only=False,\n",
    "    pre_args=pre_args,\n",
    ")\n",
    "tk_dataset = load_tokenized_srcsets(sdata_path)\n",
    "tk_dataset[\"test\"].print_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot.static_analysis import ProjectPath\n",
    "\n",
    "analysis.user2used[ProjectPath.from_str(\"type4py.type_check/MypyManager._build_tc_cmd\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot.utils import *\n",
    "\n",
    "\n",
    "def show_subtokens(ids: list[int]):\n",
    "    tks = [f\"⦗{x}⦘\" for x in DefaultTokenizer.convert_ids_to_tokens(ids)]\n",
    "    print(\" \".join(tks))\n",
    "\n",
    "\n",
    "ex_code = \"\"\"\n",
    "def eval_on_dataset(\n",
    "    model: <extra_id_0>,\n",
    "    data: <extra_id_1>,\n",
    "    window_size: <extra_id_2> = None,\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "encoding = DefaultTokenizer.encode(ex_code)\n",
    "print(encoding)\n",
    "show_subtokens(encoding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_output = \"\"\"\n",
    "<extra_id_0>ModelWrapper<extra_id_1>TokenizedSrcSet<extra_id_2>Optional[int]\n",
    "\"\"\"\n",
    "print(decoding := DefaultTokenizer.encode(ex_output))\n",
    "show_subtokens(decoding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot import proj_root\n",
    "from spot.static_analysis import ProjectPath, UsageAnalysis, PythonProject\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "proj = PythonProject.from_root(proj_root())\n",
    "for caller, callees in UsageAnalysis(proj).user2used.items():\n",
    "    if caller.module == \"spot.static_analysis\":\n",
    "        print(caller)\n",
    "        for callee in callees:\n",
    "            print(\"\\t\", callee.used, \"\" if callee.is_certain else \"  (maybe)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from spot.tokenized_src import PreprocessArgs, proj_root\n",
    "from spot.function_dataset import repo_to_tk_srcs, dataset_from_repos\n",
    "\n",
    "srcs = repo_to_tk_srcs(proj_root(), PreprocessArgs(drop_env_types=True))\n",
    "\n",
    "from spot.data import TokenizedSrcSet, CtxArgs\n",
    "\n",
    "sdata = TokenizedSrcSet(proj_root(), srcs)\n",
    "ctx_args = CtxArgs(1024, 128, 256, 512)\n",
    "cdata = sdata.to_chunks(ctx_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot.utils import *\n",
    "\n",
    "print(decode_tokens(cdata.data[\"input_ids\"][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for src in [s for s in srcs if \"static_analysis/PythonFunction\" in str(s.file)][:10]:\n",
    "    print(f\"======= file: {src.file} ========\")\n",
    "    src.print_code(max_lines=250)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot.data import PreprocessArgs, repr_modified_args\n",
    "\n",
    "repr_modified_args(\n",
    "    TrainingConfig(pre_args=PreprocessArgs(imports_in_preamble=False)), flatten=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot.static_analysis import cst, PythonModule, compute_module_usages, PythonProject\n",
    "\n",
    "code1 = \"\"\"\n",
    "# root.file1\n",
    "\n",
    "# global function\n",
    "def gf(x):\n",
    "    return x * x\n",
    "\n",
    "# with inner function\n",
    "def gf_with_inner(x):\n",
    "    def inner(y):\n",
    "        return y * y\n",
    "    return inner(x)\n",
    "\n",
    "# class\n",
    "class C:\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "    \n",
    "    def foo(self, y):\n",
    "        return self.x + y\n",
    "\n",
    "    @staticmethod\n",
    "    def s_method(x):\n",
    "        return x + 1\n",
    "    \n",
    "\"\"\"\n",
    "code2 = \"\"\"\n",
    "# root.file2\n",
    "from .file1 import gf\n",
    "from root.file1 import gf_with_inner\n",
    "import root.file1\n",
    "import root.file1 as f1\n",
    "\n",
    "def usage1(x):\n",
    "    gf(x) + root.file1.C(5)\n",
    "    foo(5)\n",
    "\n",
    "def usage2(x):\n",
    "    def inner():\n",
    "        1 + gf_with_inner(x)\n",
    "    return inner()\n",
    "\n",
    "def usage_method1(x):\n",
    "    x = f1.C(5)\n",
    "    1 + x.foo(3)\n",
    "\n",
    "def usage_method2(x):\n",
    "    (1 + f1.C(5)).foo(3)\n",
    "\n",
    "def usage_local():\n",
    "    usage1(3)\n",
    "    UsageClass(4)\n",
    "\n",
    "@f1.C(1)\n",
    "def usage_dec():\n",
    "    pass\n",
    "\n",
    "class UsageClass:\n",
    "    def __init__(self, x):\n",
    "        self.x = gf_with_inner(x)\n",
    "        self.y = self.foo(5)\n",
    "\n",
    "    def foo(self, y):\n",
    "        return usage_local(f1.gf(y))\n",
    "\n",
    "    @staticmethod\n",
    "    def s_method(x):\n",
    "        return x\n",
    "\n",
    "class SubClass(UsageClass):\n",
    "    def use(self):\n",
    "        self.foo(5)\n",
    "        f1.C.s_method(5)\n",
    "\"\"\"\n",
    "\n",
    "code3 = \"\"\"\n",
    "# root.file3\n",
    "from .file1 import *\n",
    "\n",
    "def usage1(x):\n",
    "    gf(5)\n",
    "    C(5)\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "project = PythonProject.from_modules(\n",
    "    [\n",
    "        PythonModule.from_cst(cst.parse_module(code1), \"root.file1\"),\n",
    "        PythonModule.from_cst(cst.parse_module(code2), \"root.file2\"),\n",
    "        PythonModule.from_cst(cst.parse_module(code3), \"root.file3\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "for u in compute_module_usages(project.modules[\"root.file3\"]):\n",
    "    print(str(u))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot.static_analysis import build_project_namespaces\n",
    "\n",
    "build_project_namespaces(project)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot.static_analysis import compute_module_usages\n",
    "\n",
    "compute_module_usages(project.modules[\"root.file3\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot.static_analysis import UsageAnalysis, build_project_namespaces\n",
    "\n",
    "build_project_namespaces(project)\n",
    "\n",
    "analysis = UsageAnalysis(project)\n",
    "analysis.caller2callees[ProjectPath(\"root.file2\", \"SubClass.use\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import libcst as cst\n",
    "\n",
    "from spot.tokenized_src import TokenizedSrc, PreprocessArgs\n",
    "from spot.utils import Path, decode_tokens\n",
    "\n",
    "ex_code = '''# document comment 1\n",
    "  # document comment 2\n",
    "\"\"\"String document commnet\"\"\"\n",
    "import os; import spot;\n",
    "from sys import argv, exit\n",
    "# after import\n",
    "@wraps(function)\n",
    "def catch_permission_denied(function):\n",
    "    import some.inner.imports\n",
    "    \"\"\"\n",
    "    Decorator to catch :class:`psycopg2.ProgrammingError` exceptions with the\n",
    "    ``INSUFFICIENT_PRIVILEGE`` error code and rethrow them as\n",
    "    :class:`~werkzeug.exceptions.Forbidden` exceptions instead.\n",
    "    \"\"\"\n",
    "    @wraps(function)\n",
    "    def decorated(x: str, y: int) -> str:\n",
    "        try:\n",
    "            # comment 1\n",
    "            # comment 1 cont\n",
    "            return function(*args, **kwargs)\n",
    "\n",
    "        except InsufficientPrivilege as error:\n",
    "            LOG.error(\"Forbidden: %s\", error) # comment 2\n",
    "            raise Forbidden()\n",
    "\n",
    "    return decorated\n",
    "'''\n",
    "pre_args = PreprocessArgs(stub_in_preamble=True)\n",
    "ex_src = TokenizedSrc.parse(ex_code, Path(\"test_file\"), Path(\"test_repo\"), pre_args)\n",
    "print(decode_tokens(ex_src.tokenized_code))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot.data import src_to_chunks_, CtxArgs, PreprocessArgs\n",
    "from ipywidgets import interactive\n",
    "\n",
    "pre_args = PreprocessArgs(stub_in_preamble=True)\n",
    "ex_src = TokenizedSrc.parse(ex_code, Path(\"test_file\"), Path(\"test_repo\"), pre_args)\n",
    "\n",
    "\n",
    "def print_code(\n",
    "    preamble: int,\n",
    "    left: int,\n",
    "    right: int,\n",
    "    ctx_size: int,\n",
    "    max_labels: int,\n",
    "    chunk_id: int,\n",
    "    inline_prev: bool,\n",
    "):\n",
    "    chunks = []\n",
    "    args = CtxArgs(\n",
    "        ctx_size,\n",
    "        preamble,\n",
    "        left,\n",
    "        right,\n",
    "        max_labels=max_labels,\n",
    "        inline_prev_gold=inline_prev,\n",
    "    )\n",
    "    src_to_chunks_(chunks, [], ex_src, (0, len(ex_src.types)), args)\n",
    "    print(decode_tokens(chunks[chunk_id][\"input_ids\"]))\n",
    "\n",
    "\n",
    "interactive(\n",
    "    print_code,\n",
    "    preamble=(1, 100),\n",
    "    left=(1, 200),\n",
    "    right=(1, 100),\n",
    "    ctx_size=(1, 500),\n",
    "    max_labels=(1, 10),\n",
    "    chunk_id=(0, 1),\n",
    "    inline_prev=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "from spot.data import GitRepo, ModuleRemapUnpickler\n",
    "from spot.type_env import (\n",
    "    AnnotPath,\n",
    "    MypyChecker,\n",
    "    SelectAnnotations,\n",
    "    TypeInfAction,\n",
    "    TypeInfEnv,\n",
    "    TypeInfState,\n",
    "    mypy_checker,\n",
    ")\n",
    "from spot.utils import cst, proj_root, read_file, seq_flatten, tqdm, write_file\n",
    "\n",
    "os.chdir(proj_root())\n",
    "\n",
    "datadir = Path(os.getenv(\"datadir\"))\n",
    "repos_dir = datadir / \"SPOT-data/repos\"\n",
    "\n",
    "useful_repos_path = proj_root() / \"scripts\" / \"useful_repos.pkl\"\n",
    "rename_module = lambda n: \"spot.data\" if n == \"spot.data_prepare\" else n\n",
    "with useful_repos_path.open(\"rb\") as f:\n",
    "    useful_repos: list[GitRepo] = ModuleRemapUnpickler(f, rename_module).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading pre-trained model and tokenizer\n",
    "from spot.utils import get_data_dir\n",
    "\n",
    "model_dir = \"Salesforce/codet5-base\"\n",
    "# model_dir = datadir / \"checkpoints/saved/SPOT-CodeT5-no_margin/\"\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    DataCollatorForSeq2Seq,\n",
    "    RobertaTokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    ")\n",
    "from transformers.models.t5 import T5ForConditionalGeneration\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer: RobertaTokenizer = RobertaTokenizer.from_pretrained(model_dir)\n",
    "model: T5ForConditionalGeneration = T5ForConditionalGeneration.from_pretrained(\n",
    "    model_dir\n",
    ").to(device)\n",
    "max_target_length = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot.data import mask_type_annots, output_ids_as_types, tokenize_masked\n",
    "\n",
    "test_code = \"\"\"\n",
    "@dataclass\n",
    "class GitRepo:\n",
    "    author: str\n",
    "    name: str\n",
    "    url: str\n",
    "    stars: int\n",
    "    forks: int\n",
    "\n",
    "    def authorname(self):\n",
    "        return self.author + \"__\" + self.name\n",
    "\n",
    "    def repo_dir(self, repos_dir: Path) -> Path:\n",
    "        return repos_dir / \"downloaded\" / self.authorname()\n",
    "\n",
    "    def download(self, repos_dir: Path, timeout=None) -> bool:\n",
    "        pass\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def run_model(code: str, num_beams=16):\n",
    "    masked = mask_type_annots((Path(\"no_source\"), code))\n",
    "    tks = tokenize_masked(masked, tokenizer, device)\n",
    "    input_ids = tks[\"input_ids\"]\n",
    "    with torch.no_grad():\n",
    "        loss = model.forward(**tks).loss\n",
    "        dec = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_target_length,\n",
    "            num_beams=num_beams,\n",
    "            # do_sample=True,\n",
    "        )[0]\n",
    "    return {\n",
    "        \"loss\": loss,\n",
    "        \"predicted_types\": output_ids_as_types(dec, tokenizer),\n",
    "        \"labels\": output_ids_as_types(tks[\"labels\"][0], tokenizer),\n",
    "        \"generation\": tokenizer.decode(dec),\n",
    "        \"input_ids\": input_ids[0],\n",
    "        \"output_ids\": dec,\n",
    "        \"annots_info\": masked[\"annots_info\"],\n",
    "    }\n",
    "\n",
    "\n",
    "result = run_model(test_code, num_beams=10)\n",
    "result[\"loss\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot import PythonType\n",
    "from spot.type_env import apply_annotations\n",
    "\n",
    "\n",
    "def type_to_annot(ty: PythonType) -> str:\n",
    "    return cst.Annotation(cst.parse_expression(str(ty)))\n",
    "\n",
    "\n",
    "def run_aug_model(src: Path, cwd: Path):\n",
    "    result = run_model(read_file(src), num_beams=10)\n",
    "    pred_annots = {\n",
    "        info.path: type_to_annot(t)\n",
    "        for info, t in zip(result[\"annots_info\"], result[\"predicted_types\"])\n",
    "    }\n",
    "    m1 = apply_annotations(cst.parse_module(read_file(src)), pred_annots)\n",
    "    write_file(src, m1.code)\n",
    "    checker_r = MypyChecker.check_project(src, cwd)\n",
    "    pos_to_preds = {\n",
    "        info.annot_range: str(ty)\n",
    "        for info, ty in zip(result[\"annots_info\"], result[\"predicted_types\"])\n",
    "    }\n",
    "    return {\n",
    "        \"model_result\": result,\n",
    "        \"module\": m1,\n",
    "        \"checker_feedback\": checker_r,\n",
    "        \"pos_to_preds\": pos_to_preds,\n",
    "    }\n",
    "\n",
    "\n",
    "aug_r = run_aug_model(inference_dir / \"env_code_2.py\", inference_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot.utils import patch_code_with_extra\n",
    "\n",
    "print(\"---- predicted types ----\")\n",
    "print(aug_r[\"model_result\"][\"predicted_types\"])\n",
    "print(\"---- model output ----\")\n",
    "print(tokenizer.decode(aug_r[\"model_result\"][\"output_ids\"], skip_special_tokens=False))\n",
    "print(\"---- checker_feedback ----\")\n",
    "print(aug_r[\"checker_feedback\"].output_str)\n",
    "\n",
    "print(\"---- new input ----\")\n",
    "new_input = patch_code_with_extra(\n",
    "    aug_r[\"module\"].code,\n",
    "    aug_r[\"pos_to_preds\"],\n",
    "    aug_r[\"checker_feedback\"].error_dict[\"env_code_2.py\"],\n",
    ")\n",
    "print(new_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from spot.utils import Path, run_long_task, DefaultTokenizer, not_none, CountedAcc\n",
    "from spot import proj_root\n",
    "from spot.function_dataset import guess_src_root\n",
    "\n",
    "datadir = Path(not_none(os.getenv(\"datadir\")))\n",
    "repos_dir = datadir / \"SPOT-data/repos/\"\n",
    "\n",
    "repos_split_path = proj_root() / \"data/repos_split.pkl\"\n",
    "with repos_split_path.open(\"rb\") as f:\n",
    "    repos_split = pickle.load(f)\n",
    "\n",
    "root_is_src = list[bool]()\n",
    "for repo in repos_split[\"train\"]:\n",
    "    rd = repo.repo_dir(repos_dir)\n",
    "    root_is_src.append(guess_src_root(rd).name == \"src\")\n",
    "\n",
    "CountedAcc(sum(root_is_src), len(root_is_src))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_in_root = 0\n",
    "package_in_root = 0\n",
    "setup_in_root = 0\n",
    "n_proj = 0\n",
    "\n",
    "weird_repos = []\n",
    "setup_files = []\n",
    "\n",
    "for repo in repos_split[\"train\"]:\n",
    "    rd: Path = repo.repo_dir(repos_dir)\n",
    "    n_proj += 1\n",
    "    files = list(rd.iterdir())\n",
    "    if rd / \"src\" in files:\n",
    "        src_in_root += 1\n",
    "    elif rd / (pname := rd.name.split(\"__\")[-1]) in files:\n",
    "        package_in_root += 1\n",
    "    elif rd / \"setup.cfg\" in files:\n",
    "        setup_in_root += 1\n",
    "        setup_files.append(rd / \"setup.cfg\")\n",
    "    else:\n",
    "        weird_repos.append(repo)\n",
    "\n",
    "print(\"n_projects:\", n_proj)\n",
    "print(\"src_in_root:\", src_in_root)\n",
    "print(\"package_in_root:\", package_in_root)\n",
    "print(\"setup_in_root:\", setup_in_root)\n",
    "print(\"weird_repos:\", len(weird_repos))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for repo in weird_repos[:10]:\n",
    "    rd: Path = repo.repo_dir(repos_dir)\n",
    "    print(\"Repo:\", rd.relative_to(repos_dir))\n",
    "    for f in rd.iterdir():\n",
    "        print(f.relative_to(rd))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f6ffc72953da4dd16b2e00785be9c4013ef131f465a8658f3921b6634d4eeec8"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.venv': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
