{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from spot.utils import *\n",
    "from spot.static_analysis import UsageAnalysis, PythonProject\n",
    "from spot.function_dataset import data_project_from_dir\n",
    "import os\n",
    "\n",
    "os.chdir(proj_root())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Properly handle merge commits.\n",
      "Construct edits incrementally from git history.\n",
      "Init commit for the coeditor branch.\n",
      "Rename ElemPath.\n",
      "Get rid of unsued files.\n",
      "Add readme template.\n",
      "Merge branch 'function-prediction'\n",
      "improve pushover.\n",
      "Add unit tests for running model.\n",
      "Update Pipfile and generate new lock file.\n",
      "More experiments.\n",
      "Implement oracle guided decoding.\n",
      "Add the type checking experiments.\n",
      "Ajust accuracy metrics. Store experiment settings.\n",
      "Add Typilus evaluation.\n",
      "Relocate `sigmap_from_file_predictions`.\n",
      "Resolve implicit relative imports.\n",
      "Update training parameters. Model version 6 -> 7.\n",
      "Make override usages implicit.\n",
      "Compute pub API accruacies from file-level preds.\n",
      "Add override usages.\n",
      "Add `remove_final` to the acc_metric.\n",
      "Generalize  implicit constructor usages.\n",
      "Update to model-v6.\n",
      "Improve data format:\n",
      "Update training and testing code.\n",
      "Resolve pytest fixture usages.\n",
      "Handle inner classes properly.\n",
      "Track usages in Python decorators.\n",
      "Add AccuracyMetric.\n"
     ]
    }
   ],
   "source": [
    "from coeditor.history import *\n",
    "\n",
    "history = get_commit_history(proj_root(), 30)\n",
    "for cinfo in history:\n",
    "    print(cinfo.msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting task: Retriving initial project from commit b040ef2c1b3a7c800d7a103b1c8738b8860e8210\n",
      "(5.5s) Finished task: Retriving initial project from commit b040ef2c1b3a7c800d7a103b1c8738b8860e8210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retriving commits:   0%|          | 0/29 [00:00<?, ?it/s]\rRetriving commits:   3%|▎         | 1/29 [00:01<00:31,  1.11s/it]\rRetriving commits:   7%|▋         | 2/29 [00:02<00:30,  1.14s/it]\rRetriving commits:  10%|█         | 3/29 [00:03<00:26,  1.04s/it]\rRetriving commits:  14%|█▍        | 4/29 [00:05<00:35,  1.41s/it]\rRetriving commits:  17%|█▋        | 5/29 [00:06<00:31,  1.32s/it]\rRetriving commits:  21%|██        | 6/29 [00:08<00:37,  1.64s/it]\rRetriving commits:  24%|██▍       | 7/29 [00:09<00:30,  1.40s/it]\rRetriving commits:  28%|██▊       | 8/29 [00:10<00:23,  1.12s/it]\rRetriving commits:  31%|███       | 9/29 [00:13<00:35,  1.77s/it]\rRetriving commits:  34%|███▍      | 10/29 [00:15<00:35,  1.85s/it]\rRetriving commits:  38%|███▊      | 11/29 [00:16<00:27,  1.55s/it]\rRetriving commits:  41%|████▏     | 12/29 [00:16<00:21,  1.28s/it]\rRetriving commits:  45%|████▍     | 13/29 [00:20<00:30,  1.90s/it]\rRetriving commits:  48%|████▊     | 14/29 [00:20<00:22,  1.47s/it]\rRetriving commits:  52%|█████▏    | 15/29 [00:24<00:30,  2.20s/it]\rRetriving commits:  55%|█████▌    | 16/29 [00:27<00:30,  2.34s/it]\rRetriving commits:  59%|█████▊    | 17/29 [00:28<00:23,  1.99s/it]\rRetriving commits:  62%|██████▏   | 18/29 [00:29<00:19,  1.74s/it]\rRetriving commits:  66%|██████▌   | 19/29 [00:31<00:17,  1.75s/it]\rRetriving commits:  72%|███████▏  | 21/29 [00:31<00:08,  1.01s/it]\rRetriving commits:  76%|███████▌  | 22/29 [00:32<00:07,  1.11s/it]\rRetriving commits:  90%|████████▉ | 26/29 [00:34<00:01,  1.62it/s]\rRetriving commits:  93%|█████████▎| 27/29 [00:35<00:01,  1.36it/s]\rRetriving commits:  97%|█████████▋| 28/29 [00:36<00:00,  1.27it/s]\rRetriving commits: 100%|██████████| 29/29 [00:36<00:00,  1.55it/s]\rRetriving commits: 100%|██████████| 29/29 [00:36<00:00,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "ProjectEdit(msg='Track usages in Python decorators.', changed={'spot.static_analysis': ModuleEdit(added=[], deleted=['UsageRecorder.visit_Decorator'], modified=['ModuleAnlaysis.compute_module_usages']), 'tests.test_static_analysis': ModuleEdit(added=['test_annotation_suages'], deleted=[], modified=['test_usage_analysis'])})\n",
      "----------\n",
      "ProjectEdit(msg='Handle inner classes properly.', changed={'spot.static_analysis': ModuleEdit(added=['PythonClass.subclasses', 'PythonClass.parent_class', 'PythonClass.all_elements', 'PythonModule.all_classes', 'PythonModuleBuilder.module_base', 'PythonModuleBuilder.class_stack', 'PythonModuleBuilder.current_path'], deleted=[], modified=['PythonModuleBuilder.visit_FunctionDef', 'PythonModuleBuilder.leave_ClassDef', 'UsageAnalysis.generate_usages', 'PythonModuleBuilder.__init__', 'PythonModule.all_funcs', 'PythonModuleBuilder._record_elem', 'PythonModuleBuilder.visit_ClassDef', 'PythonModuleBuilder.visit_AnnAssign', 'UsageAnalysis.__init__', 'PythonModuleBuilder.current_class', 'PythonModuleBuilder.visit_Assign', 'ModuleAnlaysis.udpate_superclasses_', 'PythonModule.all_elements', 'PythonModule.all_vars']), 'tests.test_static_analysis': ModuleEdit(added=['test_inner_classes'], deleted=[], modified=[])})\n",
      "----------\n",
      "ProjectEdit(msg='Resolve pytest fixture usages.', changed={'spot.static_analysis': ModuleEdit(added=['get_decorator_name', 'PythonFunction.is_fixture', 'PythonFunction.is_test_func', 'PythonFunction.is_fixture_user', 'UsageAnalysis.name2fixtures'], deleted=[], modified=['PythonModuleBuilder.visit_FunctionDef', 'UsageAnalysis.generate_usages', 'ModuleAnlaysis.compute_module_usages', 'PythonClass.is_dataclass', 'UsageAnalysis.__init__']), 'tests.test_static_analysis': ModuleEdit(added=['test_fixture_usages'], deleted=[], modified=[])})\n",
      "----------\n",
      "ProjectEdit(msg='Update training and testing code.', changed={'scripts.eval_func_model': ModuleEdit(added=['evals', 'common_type_names'], deleted=['name2accs', 'acc_counts'], modified=[]), 'scripts.train_model': ModuleEdit(added=['metrics'], deleted=[], modified=[]), 'spot.data': ModuleEdit(added=[], deleted=[], modified=['get_tk_dataset_name', 'create_tokenized_srcsets']), 'spot.function_decoding': ModuleEdit(added=['DecodingOrders.RandomTwice.traverse', 'DecodingOrders.RandomTwice.traverse_length'], deleted=[], modified=[]), 'spot.train': ModuleEdit(added=[], deleted=[], modified=['TrainingConfig.get_model_name']), 'spot.type_env': ModuleEdit(added=[], deleted=[], modified=['AccuracyMetric.default_metrics']), 'spot.utils': ModuleEdit(added=['get_gpu_id'], deleted=[], modified=['show_expr'])})\n",
      "----------\n",
      "ProjectEdit(msg='Improve data format:', changed={'spot.data': ModuleEdit(added=[], deleted=[], modified=['src_to_chunks_']), 'spot.function_dataset': ModuleEdit(added=[], deleted=[], modified=['wrap_main_code', 'mk_preamble'])})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "edits = edits_from_commit_history(proj_root(), history)\n",
    "for e in edits[:5]:\n",
    "    print(\"-\" * 10)\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_add=205\n",
      "n_del=165\n",
      "n_mod=137\n"
     ]
    }
   ],
   "source": [
    "n_add = n_del = n_mod = 0\n",
    "for e in edits:\n",
    "    for c in e.all_elem_changes():\n",
    "        if isinstance(c, Added):\n",
    "            n_add += 1\n",
    "        elif isinstance(c, Deleted):\n",
    "            n_del += 1\n",
    "        elif isinstance(c, Modified):\n",
    "            n_mod += 1\n",
    "        else:\n",
    "            raise ValueError(c)\n",
    "print(\"n_commit\", len(edits))\n",
    "print(f\"n_add={n_add}\")\n",
    "print(f\"n_del={n_del}\")\n",
    "print(f\"n_mod={n_mod}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting task: Performing intial module-level analysis...\n",
      "(8.2s) Finished task: Performing intial module-level analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing edits:   0%|          | 0/29 [00:00<?, ?it/s]\rAnalyzing edits:   3%|▎         | 1/29 [00:01<00:51,  1.83s/it]\rAnalyzing edits:   7%|▋         | 2/29 [00:03<00:45,  1.68s/it]\rAnalyzing edits:  10%|█         | 3/29 [00:04<00:42,  1.63s/it]\rAnalyzing edits:  14%|█▍        | 4/29 [00:06<00:40,  1.62s/it]\rAnalyzing edits:  17%|█▋        | 5/29 [00:08<00:38,  1.61s/it]\rAnalyzing edits:  21%|██        | 6/29 [00:09<00:36,  1.60s/it]\rAnalyzing edits:  24%|██▍       | 7/29 [00:11<00:35,  1.60s/it]\rAnalyzing edits:  28%|██▊       | 8/29 [00:12<00:33,  1.59s/it]\rAnalyzing edits:  31%|███       | 9/29 [00:14<00:31,  1.59s/it]\rAnalyzing edits:  34%|███▍      | 10/29 [00:16<00:30,  1.59s/it]\rAnalyzing edits:  38%|███▊      | 11/29 [00:17<00:28,  1.59s/it]\rAnalyzing edits:  41%|████▏     | 12/29 [00:19<00:26,  1.58s/it]\rAnalyzing edits:  45%|████▍     | 13/29 [00:20<00:25,  1.58s/it]\rAnalyzing edits:  48%|████▊     | 14/29 [00:22<00:23,  1.58s/it]\rAnalyzing edits:  52%|█████▏    | 15/29 [00:24<00:22,  1.58s/it]\rAnalyzing edits:  55%|█████▌    | 16/29 [00:25<00:20,  1.58s/it]\rAnalyzing edits:  59%|█████▊    | 17/29 [00:27<00:18,  1.58s/it]\rAnalyzing edits:  62%|██████▏   | 18/29 [00:28<00:17,  1.58s/it]\rAnalyzing edits:  66%|██████▌   | 19/29 [00:30<00:15,  1.58s/it]\rAnalyzing edits:  69%|██████▉   | 20/29 [00:31<00:14,  1.58s/it]\rAnalyzing edits:  72%|███████▏  | 21/29 [00:33<00:12,  1.57s/it]\rAnalyzing edits:  76%|███████▌  | 22/29 [00:35<00:11,  1.57s/it]\rAnalyzing edits:  79%|███████▉  | 23/29 [00:36<00:09,  1.57s/it]\rAnalyzing edits:  83%|████████▎ | 24/29 [00:38<00:07,  1.57s/it]\rAnalyzing edits:  86%|████████▌ | 25/29 [00:39<00:06,  1.57s/it]\rAnalyzing edits:  90%|████████▉ | 26/29 [00:41<00:04,  1.57s/it]\rAnalyzing edits:  93%|█████████▎| 27/29 [00:42<00:03,  1.57s/it]\rAnalyzing edits:  97%|█████████▋| 28/29 [00:44<00:01,  1.57s/it]\rAnalyzing edits: 100%|██████████| 29/29 [00:46<00:00,  1.57s/it]\rAnalyzing edits: 100%|██████████| 29/29 [00:46<00:00,  1.59s/it]\n"
     ]
    }
   ],
   "source": [
    "analyzed_edits = analyze_edits(edits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interesting edits: 56\n",
      "Predictable edits: 137\n"
     ]
    }
   ],
   "source": [
    "interesting_ces = []\n",
    "all_ces = []\n",
    "for ae in analyzed_edits:\n",
    "    for ce in ae.ctx_edits:\n",
    "        all_ces.append(ce)\n",
    "        if ce.usee_changes or ce.user_changes:\n",
    "            interesting_ces.append(ce)\n",
    "print(\"Interesting edits:\", len(interesting_ces))\n",
    "print(\"Predictable edits:\", len(all_ces))\n",
    "\n",
    "out_file = Path(\"ctx_changes.txt\")\n",
    "out_file.touch()\n",
    "with open(out_file, \"w\") as f:\n",
    "    for ce in interesting_ces[:50]:\n",
    "        ce.pprint(file=f)\n",
    "        print(\"~\"*50, \"\\n\", file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of new contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"ManyTypes4Py\"\n",
    "\n",
    "result_paths = {\n",
    "    \"CodeT5\": get_eval_dir(dataset, \"\"),\n",
    "    \"TypeT5\": get_eval_dir(dataset, \"(implicit_imports, new) model-v7--TrainingConfig(drop_env_types=False, add_implicit_rel_imports=True)\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_proj = PythonProject.from_root(Path(\"/home/jiayi/Projects/type4py\"))\n",
    "analysis = UsageAnalysis(\n",
    "    ex_proj, add_implicit_rel_imports=True, add_override_usages=True\n",
    ")\n",
    "pretty_print_dict(analysis.get_stats())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot.data import (\n",
    "    create_tokenized_srcsets,\n",
    "    get_tk_dataset_name,\n",
    "    load_tokenized_srcsets,\n",
    "    TypeCheckSettings,\n",
    ")\n",
    "from spot.tokenized_src import PreprocessArgs\n",
    "\n",
    "pre_args = PreprocessArgs()\n",
    "dataset = \"InferTypes4Py\"\n",
    "sdata_name = get_tk_dataset_name(dataset, pre_args, False)\n",
    "sdata_path = get_dataroot() / \"TokenizedSrcSets\" / sdata_name\n",
    "create_tokenized_srcsets(\n",
    "    dataset,\n",
    "    sdata_path,\n",
    "    func_only=False,\n",
    "    pre_args=pre_args,\n",
    ")\n",
    "tk_dataset = load_tokenized_srcsets(sdata_path)\n",
    "tk_dataset[\"test\"].print_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot.static_analysis import ProjectPath\n",
    "\n",
    "analysis.user2used[ProjectPath.from_str(\"type4py.type_check/MypyManager._build_tc_cmd\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot.utils import *\n",
    "\n",
    "def show_subtokens(ids: list[int]):\n",
    "    tks = [f\"⦗{x}⦘\" for x in DefaultTokenizer.convert_ids_to_tokens(ids)]\n",
    "    print(\" \".join(tks))\n",
    "\n",
    "ex_code = \"\"\"\n",
    "def eval_on_dataset(\n",
    "    model: <extra_id_0>,\n",
    "    data: <extra_id_1>,\n",
    "    window_size: <extra_id_2> = None,\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "encoding = DefaultTokenizer.encode(ex_code)\n",
    "print(encoding)\n",
    "show_subtokens(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_output = \"\"\"\n",
    "<extra_id_0>ModelWrapper<extra_id_1>TokenizedSrcSet<extra_id_2>Optional[int]\n",
    "\"\"\"\n",
    "print(decoding := DefaultTokenizer.encode(ex_output))\n",
    "show_subtokens(decoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot import proj_root\n",
    "from spot.static_analysis import ProjectPath, UsageAnalysis, PythonProject\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "proj = PythonProject.from_root(proj_root())\n",
    "for caller, callees in UsageAnalysis(proj).user2used.items():\n",
    "    if caller.module == \"spot.static_analysis\":\n",
    "        print(caller)\n",
    "        for callee in callees:\n",
    "            print(\"\\t\", callee.used, \"\" if callee.is_certain else \"  (maybe)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from spot.tokenized_src import PreprocessArgs, proj_root\n",
    "from spot.function_dataset import repo_to_tk_srcs, dataset_from_repos\n",
    "\n",
    "srcs = repo_to_tk_srcs(proj_root(), PreprocessArgs(drop_env_types=True))\n",
    "\n",
    "from spot.data import TokenizedSrcSet, CtxArgs\n",
    "\n",
    "sdata = TokenizedSrcSet(proj_root(), srcs)\n",
    "ctx_args = CtxArgs(1024, 128, 256, 512)\n",
    "cdata = sdata.to_chunks(ctx_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot.utils import *\n",
    "\n",
    "print(decode_tokens(cdata.data[\"input_ids\"][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for src in [s for s in srcs if \"static_analysis/PythonFunction\" in str(s.file)][:10]:\n",
    "    print(f\"======= file: {src.file} ========\")\n",
    "    src.print_code(max_lines=250)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot.data import PreprocessArgs, repr_modified_args\n",
    "\n",
    "repr_modified_args(\n",
    "    TrainingConfig(pre_args=PreprocessArgs(imports_in_preamble=False)), flatten=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot.static_analysis import cst, PythonModule, compute_module_usages, PythonProject\n",
    "\n",
    "code1 = \"\"\"\n",
    "# root.file1\n",
    "\n",
    "# global function\n",
    "def gf(x):\n",
    "    return x * x\n",
    "\n",
    "# with inner function\n",
    "def gf_with_inner(x):\n",
    "    def inner(y):\n",
    "        return y * y\n",
    "    return inner(x)\n",
    "\n",
    "# class\n",
    "class C:\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "    \n",
    "    def foo(self, y):\n",
    "        return self.x + y\n",
    "\n",
    "    @staticmethod\n",
    "    def s_method(x):\n",
    "        return x + 1\n",
    "    \n",
    "\"\"\"\n",
    "code2 = \"\"\"\n",
    "# root.file2\n",
    "from .file1 import gf\n",
    "from root.file1 import gf_with_inner\n",
    "import root.file1\n",
    "import root.file1 as f1\n",
    "\n",
    "def usage1(x):\n",
    "    gf(x) + root.file1.C(5)\n",
    "    foo(5)\n",
    "\n",
    "def usage2(x):\n",
    "    def inner():\n",
    "        1 + gf_with_inner(x)\n",
    "    return inner()\n",
    "\n",
    "def usage_method1(x):\n",
    "    x = f1.C(5)\n",
    "    1 + x.foo(3)\n",
    "\n",
    "def usage_method2(x):\n",
    "    (1 + f1.C(5)).foo(3)\n",
    "\n",
    "def usage_local():\n",
    "    usage1(3)\n",
    "    UsageClass(4)\n",
    "\n",
    "@f1.C(1)\n",
    "def usage_dec():\n",
    "    pass\n",
    "\n",
    "class UsageClass:\n",
    "    def __init__(self, x):\n",
    "        self.x = gf_with_inner(x)\n",
    "        self.y = self.foo(5)\n",
    "\n",
    "    def foo(self, y):\n",
    "        return usage_local(f1.gf(y))\n",
    "\n",
    "    @staticmethod\n",
    "    def s_method(x):\n",
    "        return x\n",
    "\n",
    "class SubClass(UsageClass):\n",
    "    def use(self):\n",
    "        self.foo(5)\n",
    "        f1.C.s_method(5)\n",
    "\"\"\"\n",
    "\n",
    "code3 = \"\"\"\n",
    "# root.file3\n",
    "from .file1 import *\n",
    "\n",
    "def usage1(x):\n",
    "    gf(5)\n",
    "    C(5)\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "project = PythonProject.from_modules(\n",
    "    [\n",
    "        PythonModule.from_cst(cst.parse_module(code1), \"root.file1\"),\n",
    "        PythonModule.from_cst(cst.parse_module(code2), \"root.file2\"),\n",
    "        PythonModule.from_cst(cst.parse_module(code3), \"root.file3\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "for u in compute_module_usages(project.modules[\"root.file3\"]):\n",
    "    print(str(u))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot.static_analysis import build_project_namespaces\n",
    "\n",
    "build_project_namespaces(project)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot.static_analysis import compute_module_usages\n",
    "\n",
    "compute_module_usages(project.modules[\"root.file3\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot.static_analysis import UsageAnalysis, build_project_namespaces\n",
    "\n",
    "build_project_namespaces(project)\n",
    "\n",
    "analysis = UsageAnalysis(project)\n",
    "analysis.caller2callees[ProjectPath(\"root.file2\", \"SubClass.use\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import libcst as cst\n",
    "\n",
    "from spot.tokenized_src import TokenizedSrc, PreprocessArgs\n",
    "from spot.utils import Path, decode_tokens\n",
    "\n",
    "ex_code = '''# document comment 1\n",
    "  # document comment 2\n",
    "\"\"\"String document commnet\"\"\"\n",
    "import os; import spot;\n",
    "from sys import argv, exit\n",
    "# after import\n",
    "@wraps(function)\n",
    "def catch_permission_denied(function):\n",
    "    import some.inner.imports\n",
    "    \"\"\"\n",
    "    Decorator to catch :class:`psycopg2.ProgrammingError` exceptions with the\n",
    "    ``INSUFFICIENT_PRIVILEGE`` error code and rethrow them as\n",
    "    :class:`~werkzeug.exceptions.Forbidden` exceptions instead.\n",
    "    \"\"\"\n",
    "    @wraps(function)\n",
    "    def decorated(x: str, y: int) -> str:\n",
    "        try:\n",
    "            # comment 1\n",
    "            # comment 1 cont\n",
    "            return function(*args, **kwargs)\n",
    "\n",
    "        except InsufficientPrivilege as error:\n",
    "            LOG.error(\"Forbidden: %s\", error) # comment 2\n",
    "            raise Forbidden()\n",
    "\n",
    "    return decorated\n",
    "'''\n",
    "pre_args = PreprocessArgs(stub_in_preamble=True)\n",
    "ex_src = TokenizedSrc.parse(ex_code, Path(\"test_file\"), Path(\"test_repo\"), pre_args)\n",
    "print(decode_tokens(ex_src.tokenized_code))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot.data import src_to_chunks_, CtxArgs, PreprocessArgs\n",
    "from ipywidgets import interactive\n",
    "\n",
    "pre_args = PreprocessArgs(stub_in_preamble=True)\n",
    "ex_src = TokenizedSrc.parse(ex_code, Path(\"test_file\"), Path(\"test_repo\"), pre_args)\n",
    "\n",
    "\n",
    "def print_code(\n",
    "    preamble: int,\n",
    "    left: int,\n",
    "    right: int,\n",
    "    ctx_size: int,\n",
    "    max_labels: int,\n",
    "    chunk_id: int,\n",
    "    inline_prev: bool,\n",
    "):\n",
    "    chunks = []\n",
    "    args = CtxArgs(\n",
    "        ctx_size,\n",
    "        preamble,\n",
    "        left,\n",
    "        right,\n",
    "        max_labels=max_labels,\n",
    "        inline_prev_gold=inline_prev,\n",
    "    )\n",
    "    src_to_chunks_(chunks, [], ex_src, (0, len(ex_src.types)), args)\n",
    "    print(decode_tokens(chunks[chunk_id][\"input_ids\"]))\n",
    "\n",
    "\n",
    "interactive(\n",
    "    print_code,\n",
    "    preamble=(1, 100),\n",
    "    left=(1, 200),\n",
    "    right=(1, 100),\n",
    "    ctx_size=(1, 500),\n",
    "    max_labels=(1, 10),\n",
    "    chunk_id=(0, 1),\n",
    "    inline_prev=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "from spot.data import GitRepo, ModuleRemapUnpickler\n",
    "from spot.type_env import (\n",
    "    AnnotPath,\n",
    "    MypyChecker,\n",
    "    SelectAnnotations,\n",
    "    TypeInfAction,\n",
    "    TypeInfEnv,\n",
    "    TypeInfState,\n",
    "    mypy_checker,\n",
    ")\n",
    "from spot.utils import cst, proj_root, read_file, seq_flatten, tqdm, write_file\n",
    "\n",
    "os.chdir(proj_root())\n",
    "\n",
    "datadir = Path(os.getenv(\"datadir\"))\n",
    "repos_dir = datadir / \"SPOT-data/repos\"\n",
    "\n",
    "useful_repos_path = proj_root() / \"scripts\" / \"useful_repos.pkl\"\n",
    "rename_module = lambda n: \"spot.data\" if n == \"spot.data_prepare\" else n\n",
    "with useful_repos_path.open(\"rb\") as f:\n",
    "    useful_repos: list[GitRepo] = ModuleRemapUnpickler(f, rename_module).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading pre-trained model and tokenizer\n",
    "from spot.utils import get_data_dir\n",
    "\n",
    "model_dir = \"Salesforce/codet5-base\"\n",
    "# model_dir = datadir / \"checkpoints/saved/SPOT-CodeT5-no_margin/\"\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    DataCollatorForSeq2Seq,\n",
    "    RobertaTokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    ")\n",
    "from transformers.models.t5 import T5ForConditionalGeneration\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer: RobertaTokenizer = RobertaTokenizer.from_pretrained(model_dir)\n",
    "model: T5ForConditionalGeneration = T5ForConditionalGeneration.from_pretrained(\n",
    "    model_dir\n",
    ").to(device)\n",
    "max_target_length = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot.data import mask_type_annots, output_ids_as_types, tokenize_masked\n",
    "\n",
    "test_code = \"\"\"\n",
    "@dataclass\n",
    "class GitRepo:\n",
    "    author: str\n",
    "    name: str\n",
    "    url: str\n",
    "    stars: int\n",
    "    forks: int\n",
    "\n",
    "    def authorname(self):\n",
    "        return self.author + \"__\" + self.name\n",
    "\n",
    "    def repo_dir(self, repos_dir: Path) -> Path:\n",
    "        return repos_dir / \"downloaded\" / self.authorname()\n",
    "\n",
    "    def download(self, repos_dir: Path, timeout=None) -> bool:\n",
    "        pass\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def run_model(code: str, num_beams=16):\n",
    "    masked = mask_type_annots((Path(\"no_source\"), code))\n",
    "    tks = tokenize_masked(masked, tokenizer, device)\n",
    "    input_ids = tks[\"input_ids\"]\n",
    "    with torch.no_grad():\n",
    "        loss = model.forward(**tks).loss\n",
    "        dec = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_target_length,\n",
    "            num_beams=num_beams,\n",
    "            # do_sample=True,\n",
    "        )[0]\n",
    "    return {\n",
    "        \"loss\": loss,\n",
    "        \"predicted_types\": output_ids_as_types(dec, tokenizer),\n",
    "        \"labels\": output_ids_as_types(tks[\"labels\"][0], tokenizer),\n",
    "        \"generation\": tokenizer.decode(dec),\n",
    "        \"input_ids\": input_ids[0],\n",
    "        \"output_ids\": dec,\n",
    "        \"annots_info\": masked[\"annots_info\"],\n",
    "    }\n",
    "\n",
    "\n",
    "result = run_model(test_code, num_beams=10)\n",
    "result[\"loss\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot import PythonType\n",
    "from spot.type_env import apply_annotations\n",
    "\n",
    "\n",
    "def type_to_annot(ty: PythonType) -> str:\n",
    "    return cst.Annotation(cst.parse_expression(str(ty)))\n",
    "\n",
    "\n",
    "def run_aug_model(src: Path, cwd: Path):\n",
    "    result = run_model(read_file(src), num_beams=10)\n",
    "    pred_annots = {\n",
    "        info.path: type_to_annot(t)\n",
    "        for info, t in zip(result[\"annots_info\"], result[\"predicted_types\"])\n",
    "    }\n",
    "    m1 = apply_annotations(cst.parse_module(read_file(src)), pred_annots)\n",
    "    write_file(src, m1.code)\n",
    "    checker_r = MypyChecker.check_project(src, cwd)\n",
    "    pos_to_preds = {\n",
    "        info.annot_range: str(ty)\n",
    "        for info, ty in zip(result[\"annots_info\"], result[\"predicted_types\"])\n",
    "    }\n",
    "    return {\n",
    "        \"model_result\": result,\n",
    "        \"module\": m1,\n",
    "        \"checker_feedback\": checker_r,\n",
    "        \"pos_to_preds\": pos_to_preds,\n",
    "    }\n",
    "\n",
    "\n",
    "aug_r = run_aug_model(inference_dir / \"env_code_2.py\", inference_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot.utils import patch_code_with_extra\n",
    "\n",
    "print(\"---- predicted types ----\")\n",
    "print(aug_r[\"model_result\"][\"predicted_types\"])\n",
    "print(\"---- model output ----\")\n",
    "print(tokenizer.decode(aug_r[\"model_result\"][\"output_ids\"], skip_special_tokens=False))\n",
    "print(\"---- checker_feedback ----\")\n",
    "print(aug_r[\"checker_feedback\"].output_str)\n",
    "\n",
    "print(\"---- new input ----\")\n",
    "new_input = patch_code_with_extra(\n",
    "    aug_r[\"module\"].code,\n",
    "    aug_r[\"pos_to_preds\"],\n",
    "    aug_r[\"checker_feedback\"].error_dict[\"env_code_2.py\"],\n",
    ")\n",
    "print(new_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from spot.utils import Path, run_long_task, DefaultTokenizer, not_none, CountedAcc\n",
    "from spot import proj_root\n",
    "from spot.function_dataset import guess_src_root\n",
    "\n",
    "datadir = Path(not_none(os.getenv(\"datadir\")))\n",
    "repos_dir = datadir / \"SPOT-data/repos/\"\n",
    "\n",
    "repos_split_path = proj_root() / \"data/repos_split.pkl\"\n",
    "with repos_split_path.open(\"rb\") as f:\n",
    "    repos_split = pickle.load(f)\n",
    "\n",
    "root_is_src = list[bool]()\n",
    "for repo in repos_split[\"train\"]:\n",
    "    rd = repo.repo_dir(repos_dir)\n",
    "    root_is_src.append(guess_src_root(rd).name == \"src\")\n",
    "\n",
    "CountedAcc(sum(root_is_src), len(root_is_src))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_in_root = 0\n",
    "package_in_root = 0\n",
    "setup_in_root = 0\n",
    "n_proj = 0\n",
    "\n",
    "weird_repos = []\n",
    "setup_files = []\n",
    "\n",
    "for repo in repos_split[\"train\"]:\n",
    "    rd: Path = repo.repo_dir(repos_dir)\n",
    "    n_proj += 1\n",
    "    files = list(rd.iterdir())\n",
    "    if rd / \"src\" in files:\n",
    "        src_in_root += 1\n",
    "    elif rd / (pname := rd.name.split(\"__\")[-1]) in files:\n",
    "        package_in_root += 1\n",
    "    elif rd / \"setup.cfg\" in files:\n",
    "        setup_in_root += 1\n",
    "        setup_files.append(rd / \"setup.cfg\")\n",
    "    else:\n",
    "        weird_repos.append(repo)\n",
    "\n",
    "print(\"n_projects:\", n_proj)\n",
    "print(\"src_in_root:\", src_in_root)\n",
    "print(\"package_in_root:\", package_in_root)\n",
    "print(\"setup_in_root:\", setup_in_root)\n",
    "print(\"weird_repos:\", len(weird_repos))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for repo in weird_repos[:10]:\n",
    "    rd: Path = repo.repo_dir(repos_dir)\n",
    "    print(\"Repo:\", rd.relative_to(repos_dir))\n",
    "    for f in rd.iterdir():\n",
    "        print(f.relative_to(rd))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f6ffc72953da4dd16b2e00785be9c4013ef131f465a8658f3921b6634d4eeec8"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.venv': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
