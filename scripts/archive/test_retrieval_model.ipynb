{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from coeditor.common import *\n",
    "import os\n",
    "\n",
    "from coeditor.encoding import encode_basic, decode_tokens\n",
    "import torch\n",
    "from coeditor.retrieval_model import (\n",
    "    RetrievalEditorModel,\n",
    "    T5LayerSelfAttention,\n",
    "    t5_cross_attention,\n",
    "    T5Stack,\n",
    "    _encode_query_stack,\n",
    "    AttentionMode,\n",
    "    t5_attention,\n",
    "    t5_sparse_attention,\n",
    ")\n",
    "\n",
    "os.chdir(proj_root())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_parameters: 222.88M\n"
     ]
    }
   ],
   "source": [
    "model = RetrievalEditorModel.from_code_t5(\"base\")\n",
    "device = torch.device(\"cuda:2\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(f\"num_parameters: {model.num_parameters()/1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test self attention\n",
    "config = model.config\n",
    "sa = T5LayerSelfAttention(config, has_relative_attention_bias=True)\n",
    "sa = sa.to(device)\n",
    "sa.eval()\n",
    "\n",
    "hidden_states = torch.randn(2, 3, config.d_model).to(device)\n",
    "out1 = sa.forward(hidden_states)[0]\n",
    "out2 = t5_cross_attention(sa, hidden_states, key_value_states=hidden_states)[0]\n",
    "\n",
    "assert torch.all(out1 == out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = hidden_states.size(1)\n",
    "position_bias = torch.ones((1, 1, seq_len, seq_len)).to(device)\n",
    "\n",
    "out1 = t5_attention(\n",
    "    sa.SelfAttention,\n",
    "    hidden_states,\n",
    "    hidden_states,\n",
    "    position_bias,\n",
    ")\n",
    "out2 = t5_sparse_attention(\n",
    "    sa.SelfAttention,\n",
    "    hidden_states,\n",
    "    [0, hidden_states.size(1), 0],\n",
    "    lambda a, b, c: torch.ones((1, 1, a, a + b)).to(device),\n",
    ")\n",
    "out3 = sa.SelfAttention.forward(\n",
    "    hidden_states,\n",
    "    position_bias = position_bias,\n",
    ")[0]\n",
    "\n",
    "assert torch.all(out1 == out2)\n",
    "assert torch.all(out1 == out3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out1.dtype=torch.float32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:2')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack = model.encoder\n",
    "stack.eval()\n",
    "\n",
    "query_ids = torch.LongTensor([[1, 2, 5], [8, 3, 0]]).to(device)\n",
    "query_mask = query_ids.ne(0)\n",
    "ref_states = tuple(torch.randn(2, 5, config.d_model).to(device) for _ in stack.block)\n",
    "\n",
    "\n",
    "ref_mask = torch.zeros(2, 5, dtype=torch.bool).to(device)\n",
    "out1 = stack.forward(query_ids, attention_mask=query_mask)[0]\n",
    "out2 = _encode_query_stack(\n",
    "    stack,\n",
    "    query_ids,\n",
    "    ref_states,\n",
    "    ref_attention_mask=ref_mask,\n",
    ").last_hidden_state\n",
    "\n",
    "print(f\"{out1.dtype=}\")\n",
    "torch.max(torch.abs((out1 - out2) * query_mask.unsqueeze(-1))) < 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_id=0\n",
      "attention_mode=<AttentionMode.basic: 1>\n",
      "Loss with good ref: 2.984961986541748\n",
      "Loss with reversed good ref: 2.984841823577881\n",
      "Loss with bad ref: 4.679500579833984\n",
      "Loss with no ref: 5.392912864685059\n",
      "attention_mode=<AttentionMode.query2ref: 2>\n",
      "Loss with good ref: 2.831299066543579\n",
      "Loss with reversed good ref: 2.831503391265869\n",
      "Loss with bad ref: 4.534691333770752\n",
      "Loss with no ref: 5.392912864685059\n",
      "attention_mode=<AttentionMode.bidirectional: 3>\n",
      "Loss with good ref: 2.4803879261016846\n",
      "Loss with reversed good ref: 2.4807794094085693\n",
      "Loss with bad ref: 4.176642417907715\n",
      "Loss with no ref: 5.392913818359375\n"
     ]
    }
   ],
   "source": [
    "query = [\n",
    "    \"<s>assert weather == <extra_id_0>\\n</s>\",\n",
    "    \"<s>assert time == <extra_id_0> # make this longer\\n</s>\",\n",
    "    \"<s>assert name == <extra_id_0>\\n</s>\",\n",
    "]\n",
    "good_refs = [\n",
    "    \"<s>weather = 'Icey'\\n</s>\",\n",
    "    \"<s>time = '1:25AM'\\n</s>\",\n",
    "    \"<s>name = 'Tako'\\n</s>\",\n",
    "]\n",
    "bad_refs = [\n",
    "    \"<s>weather = 'Sunny'\\n</s>\",\n",
    "    \"<s>time = '5:21PM'\\n</s>\",\n",
    "    \"<s>name = 'Shmi'\\n</s>\",\n",
    "]\n",
    "answer = [\n",
    "    \"<pad><s><extra_id_0>'Icey'\",\n",
    "    \"<pad><s><extra_id_0>'1:25AM'\",\n",
    "    \"<pad><s><extra_id_0>'Tako'\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for q_id in [0]:\n",
    "    q_ids = slice(0, 2)\n",
    "    print(f\"{q_id=}\")\n",
    "    for attention_mode in [AttentionMode.basic, AttentionMode.query2ref, AttentionMode.bidirectional]:\n",
    "        print(f\"{attention_mode=}\")\n",
    "        model.attention_mode = attention_mode\n",
    "\n",
    "        out = model.forward(\n",
    "            model.encode_token_seqs(query[q_ids]),\n",
    "            references=[encode_basic(x) for x in good_refs],\n",
    "            labels=model.encode_token_seqs(answer[q_ids], -100),\n",
    "        )\n",
    "        print(\"Loss with good ref:\", out.loss.item())\n",
    "\n",
    "        with torch.autocast(\"cuda\"):\n",
    "            out = model.forward(\n",
    "                model.encode_token_seqs(query[q_ids]),\n",
    "                references=[encode_basic(x) for x in reversed(good_refs)],\n",
    "                # query_ref_list=[[1, 0], []],\n",
    "                labels=model.encode_token_seqs(answer[q_ids], -100),\n",
    "            )\n",
    "            print(\"Loss with reversed good ref:\", out.loss.item())\n",
    "\n",
    "        out = model.forward(\n",
    "            model.encode_token_seqs(query[q_ids]),\n",
    "            references=[encode_basic(x) for x in bad_refs],\n",
    "            labels=model.encode_token_seqs(answer[q_ids], -100),\n",
    "        )\n",
    "        print(\"Loss with bad ref:\", out.loss.item())\n",
    "\n",
    "        out = model.forward(\n",
    "            model.encode_token_seqs(query[q_ids]),\n",
    "            references=None,\n",
    "            labels=model.encode_token_seqs(answer[q_ids], -100),\n",
    "        )\n",
    "        print(\"Loss with no ref:\", out.loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_id=0\n",
      "attention_mode=<AttentionMode.basic: 1>\n",
      "Loss with good ref: 2.984896183013916\n",
      "Loss with reversed good ref: 2.9848129749298096\n",
      "Loss with bad ref: 4.678867816925049\n",
      "Loss with no ref: 5.39282751083374\n",
      "attention_mode=<AttentionMode.query2ref: 2>\n",
      "Loss with good ref: 2.8312265872955322\n",
      "Loss with reversed good ref: 2.8314459323883057\n",
      "Loss with bad ref: 4.534139633178711\n",
      "Loss with no ref: 5.39282751083374\n",
      "attention_mode=<AttentionMode.bidirectional: 3>\n",
      "Loss with good ref: 2.4803507328033447\n",
      "Loss with reversed good ref: 2.480753183364868\n",
      "Loss with bad ref: 4.176372528076172\n",
      "Loss with no ref: 5.39282751083374\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single input:\n",
      "<s>weather = 'Icey'\n",
      "</s><s>time = '1:25AM'\n",
      "</s><s>name = 'Tako'\n",
      "</s><s>assert weather == <extra_id_0>\n",
      "</s>\n",
      "-------\n",
      "<s>weather = 'Icey'\n",
      "</s><s>time = '1:25AM'\n",
      "</s><s>name = 'Tako'\n",
      "</s><s>assert time == <extra_id_0> # make this longer\n",
      "</s>\n",
      "-------\n",
      "<s>weather = 'Icey'\n",
      "</s><s>time = '1:25AM'\n",
      "</s><s>name = 'Tako'\n",
      "</s><s>assert name == <extra_id_0>\n",
      "</s>\n",
      "Loss of CodeT5: tensor(3.4302, device='cuda:2', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from coeditor.model import CodeT5Model\n",
    "\n",
    "single_inputs = [\"\".join([*good_refs, q]) for q in query]\n",
    "print(\"Single input:\")\n",
    "print(\"\\n-------\\n\".join(single_inputs))\n",
    "\n",
    "codet5 = cast(CodeT5Model, CodeT5Model.from_pretrained(\"Salesforce/codet5-base\"))\n",
    "codet5.to(device)\n",
    "codet5.eval()\n",
    "\n",
    "out = codet5.forward(\n",
    "    model.encode_token_seqs(single_inputs),\n",
    "    labels=model.encode_token_seqs(answer),\n",
    ")\n",
    "print(\"Loss of CodeT5:\", out.loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output 0:\n",
      "<pad><s><extra_id_0>Tako <s> public class TakoWeather {</s>\n",
      "Output 1:\n",
      "<pad><s><extra_id_0>'1:25AM' name = 'Tako'</s>\n",
      "Output 2:\n",
      "<pad><s><extra_id_0>Tako <s> public class TakoWeather</s><pad>\n",
      "Output 0:\n",
      "<pad><s><extra_id_0>'Icey' ata = 'Icey' ata=</s><pad><pad><pad><pad><pad><pad>\n",
      "Output 1:\n",
      "<pad><s><extra_id_0>'1:25AM'\n",
      " = new Tako(weather)\n",
      ".name=weather</s>\n",
      "Output 2:\n",
      "<pad><s><extra_id_0>'Tako'\n",
      " = 'Tako' unction</s><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "codet5_seq = codet5.generate(\n",
    "    model.encode_token_seqs(single_inputs),\n",
    "    max_length=50,\n",
    "    num_beams=8,\n",
    ")\n",
    "for i, y in enumerate(codet5_seq):\n",
    "    print(f\"Output {i}:\")\n",
    "    print(decode_tokens(y))\n",
    "\n",
    "model.attention_mode = AttentionMode.bidirectional\n",
    "out_seq = model.generate(\n",
    "    model.encode_token_seqs(query),\n",
    "    references=[encode_basic(x) for x in reversed(good_refs)],\n",
    "    # num_beams=8,\n",
    "    max_length=50,\n",
    ")\n",
    "for i, y in enumerate(out_seq):\n",
    "    print(f\"Output {i}:\")\n",
    "    print(decode_tokens(y))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6ffc72953da4dd16b2e00785be9c4013ef131f465a8658f3921b6634d4eeec8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
