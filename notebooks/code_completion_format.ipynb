{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from coeditor.common import *\n",
    "import os\n",
    "\n",
    "os.chdir(proj_root())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to extract code completion problems from real code changes. Let's start by picking 4 random commits from the history of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CommitInfo(hash='84cfd5206348ecc3f54d202b830f803d8a03f26f', parents=('a1e2b73ab836924d0b1f9ed88e4fd90e7a6f61e6',), msg='Add ablation: dense attention.')\n",
      "CommitInfo(hash='a1e2b73ab836924d0b1f9ed88e4fd90e7a6f61e6', parents=('ecdbdc3875e47887ff3c0320fd1367af28d0a491',), msg='Implement ablation: current_code_only.')\n",
      "CommitInfo(hash='ecdbdc3875e47887ff3c0320fd1367af28d0a491', parents=('ad918b35e2b8314f30a7f8ffc1e957c9f49956df',), msg='Exclude builtins defs in ctx by default.')\n"
     ]
    }
   ],
   "source": [
    "from coeditor.git import get_commit_history, CommitInfo\n",
    "\n",
    "repo_root = proj_root()\n",
    "\n",
    "commits = get_commit_history(\n",
    "    repo_root, 4, commit_id=\"84cfd5206348ecc3f54d202b830f803d8a03f26f\"\n",
    ")\n",
    "for c in commits[:3]:\n",
    "    print(c)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then extract `FIMProblem` instances from these commits using `edits_from_commit_history` function by specifying `C3CompletionGenerator` as the change_processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "building initial project: 100%|██████████| 34/34 [00:01<00:00, 21.39it/s]\n",
      "processing commits: 100%|██████████| 3/3 [00:07<00:00,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(fim_problems) = 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from coeditor.scoped_changes import edits_from_commit_history\n",
    "from coeditor.experiments.code_completion import FIMProblem, C3CompletionGenerator\n",
    "\n",
    "generator = C3CompletionGenerator()\n",
    "\n",
    "workdir = proj_root() / \"../temp-1\"\n",
    "fim_problems = edits_from_commit_history(\n",
    "    repo_root, commits, workdir, change_processor=generator\n",
    ")\n",
    "print(f\"{len(fim_problems) = }\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now visualize an example problem by first converting it into the input-output format used by CodeT5. Each problem instance asks the model to predict a missing line extracted that correspond to the last added line from the actual changes made to a given code region. Note that in this format, any previous changes made by the user are directly applied to the code that surrounds the missing line.\n",
    "\n",
    "Feel free to change `ex_id` to see other examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "middle:\n",
      "                seg = seg + origin_line + [Newline_id]\n",
      "--------------------------------------------------------------------------------\n",
      "left:\n",
      ",\n",
      ") -> tuple[TokenSeq, ...]:\n",
      "    \"\"\"Truncate a list of token sequences to fit within a total length limit.\n",
      "    Earlier sections have priority over later sections.\n",
      "    \"\"\"\n",
      "\n",
      "    # first, reserve equal space to each section\n",
      "    section_lens = [total_limit // len(sections) for _ in sections]\n",
      "    remaining = total_limit\n",
      "    for i, (tks, _) in enumerate(sections):\n",
      "        l = min(len(tks), section_lens[i])\n",
      "        remaining -= l\n",
      "        section_lens[i] = l\n",
      "    assert remaining >= 0\n",
      "\n",
      "    # for the unused space, assign to ealier sections when possible\n",
      "    for i, (tks, _) in enumerate(sections):\n",
      "        if remaining <= 0:\n",
      "            break\n",
      "        inc = min(remaining, len(tks) - section_lens[i])\n",
      "        section_lens[i] += inc\n",
      "        remaining -= inc\n",
      "\n",
      "    return tuple(\n",
      "        truncate_section(tks, truncate_dir, section_lens[i], add_bos, inplace=inplace)\n",
      "        for i, (tks, truncate_dir) in enumerate(sections)\n",
      "    )\n",
      "\n",
      "class TokenizedEdit(ABC):\n",
      "    input_tks: TokenSeq\n",
      "    output_tks: TokenSeq\n",
      "    main_tks: TokenSeq\n",
      "    path: ProjectPath\n",
      "    change_type: Change[None]\n",
      "\n",
      "    BAD_DELETE = encode_single_line(\"((bad delete))\")\n",
      "\n",
      "    @abstractmethod\n",
      "    def all_ctxs(self) -> dict[str, TokenSeq]:\n",
      "        pass\n",
      "\n",
      "    def meta_data_lines(self) -> list[str]:\n",
      "        return [f\"path: {str(self.path)}\"]\n",
      "\n",
      "    def stats(self) -> Mapping[str, int | float]:\n",
      "        return {\n",
      "            \"input_tks\": len(self.input_tks),\n",
      "            \"output_tks\": len(self.output_tks),\n",
      "            \"main_tks\": len(self.main_tks),\n",
      "        }\n",
      "\n",
      "    def __repr__(self) -> str:\n",
      "        return f\"{type(self).__name__}(path={str(self.path)}, type={type(self.change_type).__name__}, len(input_tks)={len(self.input_tks)}, len(output_tks)={len(self.output_tks)})\"\n",
      "\n",
      "    @classmethod\n",
      "    def show_label(cls, i: int):\n",
      "        return f\" <{i}>\" if i <= 9 else f\"<{i}>\"\n",
      "\n",
      "    @classmethod\n",
      "    def show_line(cls, tks: TokenSeq):\n",
      "        if tks and tks[0] == Add_id:\n",
      "            return \"+ \" + decode_tokens(tks[1:])\n",
      "        elif tks and tks[0] == Del_id:\n",
      "            return \"- \" + decode_tokens(tks[1:])\n",
      "        else:\n",
      "            return \"  \" + decode_tokens(tks)\n",
      "\n",
      "    @classmethod\n",
      "    def show_predictions(\n",
      "        cls, pred: TokenSeq, main_tk_lines: dict[Token, TokenSeq]\n",
      "    ) -> str:\n",
      "        id_map = {k: i for i, k in enumerate(main_tk_lines)}\n",
      "        segs = output_ids_as_seqs(pred)\n",
      "        lines = []\n",
      "        for k, seg in segs.items():\n",
      "            if not seg:\n",
      "                continue  # skip empty lines\n",
      "            if seg[-1] == Del_id:\n",
      "                # show the deleted line\n",
      "                section_lines = tk_splitlines(main_tk_lines.get(k, TokenSeq()))\n",
      "                if section_lines:\n",
      "                    origin_line = section_lines[0]\n",
      "                else:\n",
      "                    origin_line = cls.BAD_DELETE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "right:\n",
      "\n",
      "            label = cls.show_label(id_map.get(k, -1))\n",
      "            lines.append(f\"{label}:{indent(decode_tokens(seg), ' ' * 4).lstrip()}\")\n",
      "        return \"\".join(lines)\n",
      "    def show(self, pred_tks: TokenSeq | None = None, skip_ctx: bool = False) -> str:        def show_ctx(ctx_tks: TokenSeq):\n",
      "            lines = tk_splitlines(ctx_tks)\n",
      "            return \"\\n\".join(\"  \" + self.show_line(l) for l in lines)\n",
      "\n",
      "        main_segs = output_ids_as_seqs(self.main_tks)\n",
      "        id_map = {k: i for i, k in enumerate(main_segs)}\n",
      "        main_lines = list[str]()\n",
      "        for line_tks in tk_splitlines(self.main_tks):\n",
      "            if line_tks and is_extra_id(line_tks[0]):\n",
      "                prefix = self.show_label(id_map.get(line_tks[0], -1))\n",
      "                line = prefix + self.show_line(line_tks[1:])\n",
      "            else:\n",
      "                line = \"    \" + self.show_line(line_tks)\n",
      "            main_lines.append(line)\n",
      "\n",
      "        pred_lines = (\n",
      "            [\n",
      "                \"========Prediction========\",\n",
      "                f\"{self.show_predictions(pred_tks, main_segs)}\",\n",
      "            ]\n",
      "            if pred_tks\n",
      "            else []\n",
      "        )\n",
      "        outputs = [\n",
      "            \"-\" * 80,\n",
      "            *self.meta_data_lines(),\n",
      "            \"========Ground Truth========\",\n",
      "            self.show_predictions(self.output_tks, main_segs),\n",
      "            *pred_lines,\n",
      "            \"========Main Code========\",\n",
      "            \"\\n\".join(main_lines),\n",
      "        ]\n",
      "        if not skip_ctx:\n",
      "            outputs.extend(\n",
      "                f\"==========={name}===========\\n\" + show_ctx(tks)\n",
      "                for name, tks in self.all_ctxs().items()\n",
      "            )\n",
      "        return \"\\n\".join(outputs)\n",
      "    # turn off redundant BLEU warnings\n",
      "    warnings.simplefilter(\n",
      "        \"ignore\",\n",
      "        category=UserWarning,\n",
      "        lineno=552,\n",
      "    )\n",
      "    def is_repetitive_edit(self, blue_threshold=0.8) -> bool:        \"\"\"Check if all additions in the output_tokens can be matched to\n",
      "        an addition in the input_tokens with a BLEU score above the threshold.\"\"\"\n",
      "\n",
      "        def get_changes(tks, key_tk: Token):\n",
      "            if tks and tks[0] == key_tk:\n",
      "                s = decode_tokens(tks[1:])\n",
      "                s.strip()\n",
      "                return encode_single_line(s)\n",
      "            else:\n",
      "                return []\n",
      "\n",
      "        ctx_lines = tk_splitlines(self.input_tks)\n",
      "        main_lines = output_ids_as_seqs(self.input_tks)\n",
      "        ctx_addtions = [tks for l in ctx_lines if (tks := get_changes(l, Add_id))]\n",
      "        ctx_deletions = [tks for l in ctx_lines if (tks := get_changes(l, Del_id))]\n",
      "\n",
      "        def has_match(line, line_key: Token):\n",
      "            if line:\n",
      "                if line[0] == Add_id:\n",
      "                    added = line[1:]\n",
      "                    return any(\n",
      "                        as_any(sentence_bleu([ref], added)) > blue_threshold\n",
      "                        for ref in ctx_addtions\n",
      "                    )\n",
      "                elif line == [Del_id]:\n",
      "                    if line_key not in main_lines\n"
     ]
    }
   ],
   "source": [
    "from coeditor.encoding import decode_tokens, _Tokenizer\n",
    "\n",
    "\n",
    "ex_id = 0\n",
    "left, right = fim_problems[ex_id].get_contexts(_Tokenizer)\n",
    "middle = fim_problems[ex_id].middle\n",
    "\n",
    "# we use decode_tokens to convert the token sequences into strings\n",
    "print_sections(\n",
    "    (\"middle\", middle),\n",
    "    (\"left\", left),\n",
    "    (\"right\", right),\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the CodeT5 and InCoder model and see how they perform on these problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coeditor.experiments.code_completion import CodeT5Wrapper\n",
    "codet5 = CodeT5Wrapper.from_pretrained(\"Salesforce/codet5-large\")\n",
    "# codet5 = CodeT5Wrapper.from_pretrained()\n",
    "codet5.model.half().to(\"cuda\")\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "predicted:\n",
      "                return\n",
      "--------------------------------------------------------------------------------\n",
      "label:\n",
      "                seg = seg + origin_line + [Newline_id]\n"
     ]
    }
   ],
   "source": [
    "print_sections(\n",
    "    (\"predicted\", codet5.infill(left, right)),\n",
    "    (\"label\", middle),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coeditor.experiments.in_coder import InCoderWrapper\n",
    "\n",
    "incoder = InCoderWrapper.from_pretrained(\"facebook/incoder-1B\", half_precision=True)\n",
    "incoder.model.to(\"cuda\")\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3943 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "predicted:\n",
      "                indent = \" \" * 4\n",
      "                label = cls.show_label(id_map.get(k, -1))\n",
      "                lines.append(f\"{label}:{indent(origin_line, ' ' * 4).lstrip()}\")\n",
      "            else:\n",
      "                # show the added line\n",
      "                section_lines = tk_splitlines(main_tk_lines.get(k, TokenSeq()))\n",
      "                if section_lines:\n",
      "                    origin_line = section_lines[-1]\n",
      "                else:\n",
      "                    origin_line = cls.BAD_DELETE\n",
      "                indent = \" \" * 4\n",
      "--------------------------------------------------------------------------------\n",
      "label:\n",
      "                seg = seg + origin_line + [Newline_id]\n"
     ]
    }
   ],
   "source": [
    "left, right = fim_problems[ex_id].get_contexts(incoder.tokenizer)\n",
    "print_sections(\n",
    "    (\"predicted\", incoder.infill(left, right)),\n",
    "    (\"label\", middle),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare this with a different format that models this problem as a special case of code editing. To do that, we will run `edits_from_commit_history` again but with `C3ProblemGenerator` as the `change_processor`. This will give us `C3Problem` instances, which correspond to general contextual code change prediction problem. We can then convert them into instances that are similar to  similar to the `FIMProblem` problems above using `C3ToCodeCompletion`.  will then convert into  we then convert them into "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "building initial project: 100%|██████████| 34/34 [00:00<00:00, 107.66it/s]\n",
      "processing commits: 100%|██████████| 3/3 [00:17<00:00,  5.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(c3_problems) = 6\n",
      "len(comp_probs) = 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from coeditor.c3problem import C3ProblemGenerator, C3ToCodeCompletion\n",
    "\n",
    "c3_problems = edits_from_commit_history(\n",
    "    repo_root, commits, workdir, change_processor=C3ProblemGenerator()\n",
    ")\n",
    "print(f\"{len(c3_problems) = }\")\n",
    "\n",
    "transform = C3ToCodeCompletion()\n",
    "comp_probs = join_list([transform.transform(p) for p in c3_problems])\n",
    "print(f\"{len(comp_probs) = }\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "path: coeditor.encoding/TokenizedEdit.show_predictions\n",
      "n_references: 1\n",
      "total_reference_tks: 500\n",
      "project: temp-1\n",
      "commit: CommitInfo(hash='a1e2b73ab836924d0b1f9ed88e4fd90e7a6f61e6', parents=('ecdbdc3875e47887ff3c0320fd1367af28d0a491',), msg='Implement ablation: current_code_only.')\n",
      "========Ground Truth========\n",
      " <0>:<add>                 seg = seg + origin_line + [Newline_id]\n",
      "\n",
      "========Main Code========\n",
      "      # module: coeditor.encoding\n",
      "      class TokenizedEdit(ABC):\n",
      "          @classmethod\n",
      "          def show_predictions(\n",
      "              cls, pred: TokenSeq, main_tk_lines: dict[Token, TokenSeq]\n",
      "          ) -> str:\n",
      "              id_map = {k: i for i, k in enumerate(main_tk_lines)}\n",
      "              segs = output_ids_as_seqs(pred)\n",
      "              lines = []\n",
      "              for k, seg in segs.items():\n",
      "                  if not seg:\n",
      "                      continue  # skip empty lines\n",
      "                  if seg[-1] == Del_id:\n",
      "                      # show the deleted line\n",
      "                      section_lines = tk_splitlines(main_tk_lines.get(k, TokenSeq()))\n",
      "                      if section_lines:\n",
      "                          origin_line = section_lines[0]\n",
      "                      else:\n",
      "                          origin_line = cls.BAD_DELETE\n",
      "    -                 origin_line.append(Newline_id)\n",
      "    -                 seg = seg + origin_line\n",
      " <0>              label = cls.show_label(id_map.get(k, -1))\n",
      "                  lines.append(f\"{label}:{indent(decode_tokens(seg), ' ' * 4).lstrip()}\")\n",
      "              return \"\".join(lines)\n",
      "      \n",
      "      \n",
      "===========unchanged ref 0===========\n",
      "    at: coeditor.common\n",
      "        Token = int\n",
      "    \n",
      "        TokenSeq = list[Token]\n",
      "    \n",
      "    at: coeditor.encoding\n",
      "        Del_id = get_tk_id(Del)\n",
      "    \n",
      "        Newline_id = get_tk_id(\"\\n\")\n",
      "    \n",
      "        tk_splitlines(tks: TokenSeq)\n",
      "    \n",
      "        decode_tokens(tokens: TokenSeq, prettify: bool=False) -> str\n",
      "    \n",
      "        output_ids_as_seqs(output_ids: Iterable[Token]) -> dict[Token, TokenSeq]\n",
      "    \n",
      "        id_map = {k: i for i, k in enumerate(main_tk_lines)}\n",
      "    \n",
      "        id_map = {k: i for i, k in enumerate(main_tk_lines)}\n",
      "    \n",
      "    at: coeditor.encoding.TokenizedEdit\n",
      "        input_tks: TokenSeq\n",
      "    \n",
      "        output_tks: TokenSeq\n",
      "    \n",
      "        main_tks: TokenSeq\n",
      "    \n",
      "        path: ProjectPath\n",
      "    \n",
      "        change_type: Change[None]\n",
      "    \n",
      "        BAD_DELETE = encode_single_line(\"((bad delete))\")\n",
      "    \n",
      "        show_label(i: int)\n",
      "    \n",
      "    at: coeditor.encoding.TokenizedEdit.show_predictions\n",
      "        id_map = {k: i for i, k in enumerate(main_tk_lines)}\n",
      "    \n",
      "        segs = output_ids_as_seqs(pred)\n",
      "    \n",
      "        lines = []\n",
      "    \n",
      "        seg = seg + origin_line + [Newline_id]\n",
      "    \n",
      "        section_lines = tk_splitlines(main_tk_lines.get(k, TokenSeq()))\n",
      "    \n",
      "        origin_line = cls.BAD_DELETE\n",
      "        origin_line = section_lines[0]\n",
      "    \n",
      "        label = cls.show_label(id_map.get(k, -1))\n",
      "    \n",
      "    at: textwrap\n",
      "        indent(text: str, prefix: str, predicate: Optional[Callable[[str], bool]]=...) -> str\n",
      "    \n",
      "    at: typing.Mapping\n",
      "        get(key: _KT) -> Optional[_VT_co]\n",
      "        get(key: _KT, default: Union[_VT_co, _T]) -> Union[_VT_co, _T]\n",
      "    \n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from coeditor.c3problem import C3ProblemTokenizer\n",
    "\n",
    "\n",
    "tknizer = C3ProblemTokenizer(max_ref_tks_sum=2000)\n",
    "tk_prob = tknizer.tokenize_problem(comp_probs[ex_id])\n",
    "print(tk_prob.show())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coeditor.model import RetrievalEditorModel\n",
    "from coeditor.experiments.code_completion import infill_with_coeditor\n",
    "\n",
    "coeditor = RetrievalEditorModel.load(\"MrVPlusOne/coeditor-perm2k-base-v1.7.3\")\n",
    "coeditor.to(\"cuda\")\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "predicted:\n",
      "<pad><s><extra_id_0> <add>                 seg = seg + origin_line + [Newline_id]\n",
      "</s>\n",
      "--------------------------------------------------------------------------------\n",
      "label:\n",
      "                seg = seg + origin_line + [Newline_id]\n"
     ]
    }
   ],
   "source": [
    "print_sections(\n",
    "    (\"predicted\", decode_tokens(infill_with_coeditor(coeditor, tk_prob))),\n",
    "    (\"label\", middle),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# module: coeditor.encoding\n",
      "class TokenizedEdit(ABC):\n",
      "    @classmethod\n",
      "    def show_predictions(\n",
      "        cls, pred: TokenSeq, main_tk_lines: dict[Token, TokenSeq]\n",
      "    ) -> str:\n",
      "        id_map = {k: i for i, k in enumerate(main_tk_lines)}\n",
      "        segs = output_ids_as_seqs(pred)\n",
      "        lines = []\n",
      "        for k, seg in segs.items():\n",
      "            if not seg:\n",
      "                continue  # skip empty lines\n",
      "            if seg[-1] == Del_id:\n",
      "                # show the deleted line\n",
      "                section_lines = tk_splitlines(main_tk_lines.get(k, TokenSeq()))\n",
      "                if section_lines:\n",
      "                    origin_line = section_lines[0]\n",
      "                else:\n",
      "                    origin_line = cls.BAD_DELETE\n",
      " <del>                 origin_line.append(Newline_id)\n",
      " <del>                 seg = seg + origin_line\n",
      " <add>                 seg = seg + origin_line + [Newline_id]\n",
      "            label = cls.show_label(id_map.get(k, -1))\n",
      "            lines.append(f\"{label}:{indent(decode_tokens(seg), ' ' * 4).lstrip()}\")\n",
      "        return \"\".join(lines)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from coeditor.encoding import inline_output_tokens\n",
    "\n",
    "\n",
    "inlined_tks = inline_output_tokens(tk_prob.main_tks, infill_with_coeditor(coeditor, tk_prob))\n",
    "print(decode_tokens(inlined_tks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "                seg = seg + origin_line + [Newline_id]\n",
      "--------------------------------------------------------------------------------\n",
      "                all_refs.append((f\"unchanged ref {i}\", chunk))\n",
      "--------------------------------------------------------------------------------\n",
      "            save_steps=max(500, min(10000, epoch_steps // 5)),\n",
      "--------------------------------------------------------------------------------\n",
      "    eval_tkn.max_query_tks = 1024\n",
      "--------------------------------------------------------------------------------\n",
      "                hidden_state_mask=tks_mask,\n",
      "====================================================================================================\n",
      "--------------------------------------------------------------------------------\n",
      "TkDelta(\n",
      "  15: ('<add>                 seg = seg + origin_line + [Newline_id]',)\n",
      ")\n",
      "--------------------------------------------------------------------------------\n",
      "TkDelta(\n",
      "  107: ('<add>                 all_refs.append((f\"unchanged ref {i}\", chunk))',)\n",
      ")\n",
      "--------------------------------------------------------------------------------\n",
      "TkDelta(\n",
      "  42: ('<add>             save_steps=max(500, min(10000, epoch_steps // 5)),',)\n",
      ")\n",
      "--------------------------------------------------------------------------------\n",
      "TkDelta(\n",
      "  69: ('<add>     eval_tkn.max_query_tks = 1024',)\n",
      ")\n",
      "--------------------------------------------------------------------------------\n",
      "TkDelta(\n",
      "  54: ('<add>                 hidden_state_mask=tks_mask,',)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for p in fim_problems:\n",
    "    print(SEP)\n",
    "    print(p.middle)\n",
    "\n",
    "print(\"=\" * 100)\n",
    "for p in comp_probs:\n",
    "    print(SEP)\n",
    "    print(p.span.delta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
